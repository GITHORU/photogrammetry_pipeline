import os
import numpy as np
import logging
import xml.etree.ElementTree as ET
from multiprocessing import Pool, cpu_count

# PATCH RASTERIO CIBL√â - Seulement les modules vraiment n√©cessaires
def patch_rasterio_essentials():
    """Patch cibl√© pour les modules rasterio essentiels"""
    import types
    
    # Modules vraiment utilis√©s dans le code
    essential_modules = [
        'rasterio.sample',    # Utilis√© dans process_single_cloud_orthoimage
        'rasterio.vrt',       # Utilis√© dans process_single_cloud_orthoimage  
        'rasterio._features', # Erreur actuelle
        'rasterio.coords',    # Utilis√© pour BoundingBox
    ]
    
    for module_name in essential_modules:
        try:
            __import__(module_name)
        except ImportError:
            # Cr√©er un module minimal avec seulement ce qui est n√©cessaire
            module = types.ModuleType(module_name)
            
            # Cas sp√©ciaux pour certains modules
            if module_name == 'rasterio.coords':
                class BoundingBox:
                    def __init__(self, left, bottom, right, top):
                        self.left = left
                        self.bottom = bottom
                        self.right = right
                        self.top = top
                module.BoundingBox = BoundingBox
                logging.getLogger(__name__).warning(f"PATCH: {module_name}.BoundingBox cr√©√©")
            
            # Injecter le module dans rasterio
            module_parts = module_name.split('.')
            if len(module_parts) == 2:
                parent_name, child_name = module_parts
                if parent_name in globals():
                    parent = globals()[parent_name]
                    setattr(parent, child_name, module)
                    logging.getLogger(__name__).warning(f"PATCH: {module_name} cr√©√© (module minimal)")

# Appliquer le patch au d√©marrage
patch_rasterio_essentials()

def process_single_cloud_add_offset(args):
    """Fonction de traitement d'un seul nuage pour l'ajout d'offset (pour multiprocessing)"""
    ply_file, output_dir, coord_file, extra_params = args
    
    # Cr√©ation d'un logger pour ce processus
    logger = logging.getLogger(f"AddOffset_{os.getpid()}")
    logger.setLevel(logging.INFO)
    
    try:
        # Import d'open3d
        import open3d as o3d
        
        # Lecture du nuage
        cloud = o3d.io.read_point_cloud(ply_file)
        if not cloud.has_points():
            return False, f"Nuage vide dans {os.path.basename(ply_file)}"
        
        points = np.asarray(cloud.points)
        
        # Lecture de l'offset depuis le fichier de coordonn√©es
        offset = None
        if coord_file and os.path.exists(coord_file):
            with open(coord_file, 'r') as f:
                for line in f:
                    if line.startswith('#Offset to add :'):
                        offset_text = line.replace('#Offset to add :', '').strip()
                        offset = [float(x) for x in offset_text.split()]
                        break
        
        if not offset:
            return False, f"Offset non trouv√© dans {coord_file}"
        
        # Application de l'offset
        offset_array = np.array(offset)
        deformed_points = points + offset_array
        
        # Cr√©ation du nouveau nuage
        new_cloud = o3d.geometry.PointCloud()
        new_cloud.points = o3d.utility.Vector3dVector(deformed_points)
        
        # Copie des couleurs et normales
        if cloud.has_colors():
            new_cloud.colors = cloud.colors
        if cloud.has_normals():
            new_cloud.normals = cloud.normals
        
        # Sauvegarde
        output_file = os.path.join(output_dir, os.path.basename(ply_file))
        success = o3d.io.write_point_cloud(output_file, new_cloud)
        
        if success:
            return True, f"Trait√© : {os.path.basename(ply_file)} ({len(points)} points)"
        else:
            return False, f"Erreur de sauvegarde : {os.path.basename(ply_file)}"
            
    except Exception as e:
        return False, f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}"

def process_single_cloud_itrf_to_enu(args):
    """Fonction de traitement d'un seul nuage pour la conversion ITRF‚ÜíENU (pour multiprocessing)"""
    if len(args) == 5:
        # Ancien format : compatibilit√©
        ply_file, output_dir, coord_file, extra_params, ref_point_name = args
        global_ref_point = None
        force_global_ref = False
    else:
        # Nouveau format avec point global
        ply_file, output_dir, coord_file, extra_params, ref_point_name, global_ref_point, force_global_ref = args
    
    # Cr√©ation d'un logger pour ce processus
    logger = logging.getLogger(f"ITRFtoENU_{os.getpid()}")
    logger.setLevel(logging.INFO)
    
    try:
        import open3d as o3d
        import pyproj
        
        # Lecture du nuage
        cloud = o3d.io.read_point_cloud(ply_file)
        if not cloud.has_points():
            return False, f"Nuage vide dans {os.path.basename(ply_file)}"
        
        points = np.asarray(cloud.points)
        logger.info(f"  {len(points)} points charg√©s")
        
        # √âTAPE 1 : Priorisation du point de r√©f√©rence global si forc√©
        if force_global_ref and global_ref_point is not None:
            logger.info("üéØ UTILISATION DU POINT DE R√âF√âRENCE GLOBAL FORC√â (ITRF‚ÜíENU)")
            logger.info(f"Point global : ({global_ref_point[0]:.6f}, {global_ref_point[1]:.6f}, {global_ref_point[2]:.6f})")
            tr_center = global_ref_point
            logger.info("Le point global remplace le point local pour la transformation ITRF‚ÜíENU")
            logger.info("‚ö†Ô∏è  L'offset ne s'applique PAS au point global (coordonn√©es absolues pr√©serv√©es)")
        else:
            logger.info("üìç UTILISATION DU POINT DE R√âF√âRENCE LOCAL (ITRF‚ÜíENU)")
            
            # Lecture du point de r√©f√©rence depuis le fichier de coordonn√©es
            ref_point = None
            offset = None
            
            if coord_file and os.path.exists(coord_file):
                with open(coord_file, 'r') as f:
                    lines = f.readlines()
                
                # Recherche du point de r√©f√©rence
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        parts = line.split()
                        if len(parts) >= 4:  # Format: NOM X Y Z
                            try:
                                point_name = parts[0]
                                x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                                if ref_point_name is None or point_name == ref_point_name:
                                    ref_point = [x, y, z]
                                    break
                            except ValueError:
                                continue
                
                # Lecture de l'offset
                for line in lines:
                    line = line.strip()
                    if line.startswith('#Offset to add :'):
                        offset_text = line.replace('#Offset to add :', '').strip()
                        offset = [float(x) for x in offset_text.split()]
                        break
            
            if ref_point is None:
                return False, f"Point de r√©f√©rence non trouv√© dans {coord_file}"
            
            if offset is None:
                return False, f"Offset non trouv√© dans {coord_file}"
            
            # Application de l'offset au point local UNIQUEMENT
            ref_point_with_offset = [ref_point[0] + offset[0], ref_point[1] + offset[1], ref_point[2] + offset[2]]
            tr_center = ref_point_with_offset
            logger.info(f"Point local avec offset appliqu√© : ({tr_center[0]:.6f}, {tr_center[1]:.6f}, {tr_center[2]:.6f})")
        tr_ellps = "GRS80"
        
        pipeline = "+proj=topocentric +X_0={0} +Y_0={1} +Z_0={2} +ellps={3}".format(
            tr_center[0], tr_center[1], tr_center[2], tr_ellps
        )
        
        transformer = pyproj.Transformer.from_pipeline(pipeline)
        
        # Application de la transformation topocentrique (optimis√©e)
        # Traitement par chunks pour √©viter surcharge m√©moire avec gros nuages
        chunk_size = 100000  # 100k points par chunk
        total_points = len(points)
        arr_pts_ENU = np.zeros_like(points)
        
        for i in range(0, total_points, chunk_size):
            end_idx = min(i + chunk_size, total_points)
            chunk_points = points[i:end_idx]
            
            # Extraction directe sans conversion list inutile
            arr_x = chunk_points[:, 0]
            arr_y = chunk_points[:, 1] 
            arr_z = chunk_points[:, 2]
            
            # Transformation du chunk
            chunk_enu = np.array(transformer.transform(arr_x, arr_y, arr_z)).T
            arr_pts_ENU[i:end_idx] = chunk_enu
            
            # DEBUG : V√©rification des coordonn√©es de transformation (premier chunk seulement)
            if i == 0:
                logger.info(f"üîç DEBUG - Transformation ITRF‚ÜíENU (premier chunk):")
                logger.info(f"   Premier point ITRF: ({arr_x[0]:.6f}, {arr_y[0]:.6f}, {arr_z[0]:.6f})")
                logger.info(f"   Premier point ENU: ({chunk_enu[0, 0]:.6f}, {chunk_enu[0, 1]:.6f}, {chunk_enu[0, 2]:.6f})")
                logger.info(f"   Dernier point ITRF: ({arr_x[-1]:.6f}, {arr_y[-1]:.6f}, {arr_z[-1]:.6f})")
                logger.info(f"   Dernier point ENU: ({chunk_enu[-1, 0]:.6f}, {chunk_enu[-1, 1]:.6f}, {chunk_enu[-1, 2]:.6f})")
                
                # DEBUG : V√©rification du centre de transformation
                logger.info(f"üîç DEBUG - Centre de transformation utilis√©:")
                logger.info(f"   Centre ITRF: ({tr_center[0]:.6f}, {tr_center[1]:.6f}, {tr_center[2]:.6f})")
                logger.info(f"   Centre ENU: (0.000000, 0.000000, 0.000000)")
                
                # DEBUG : Calcul manuel pour v√©rifier
                test_point_itrf = np.array([arr_x[0], arr_y[0], arr_z[0]])
                test_point_enu = transformer.transform(test_point_itrf[0], test_point_itrf[1], test_point_itrf[2])
                logger.info(f"üîç DEBUG - V√©rification manuelle:")
                logger.info(f"   Point test ITRF: ({test_point_itrf[0]:.6f}, {test_point_itrf[1]:.6f}, {test_point_itrf[2]:.6f})")
                logger.info(f"   Point test ENU: ({test_point_enu[0]:.6f}, {test_point_enu[1]:.6f}, {test_point_enu[2]:.6f})")
                logger.info(f"   Diff√©rence ITRF - Centre: ({test_point_itrf[0] - tr_center[0]:.6f}, {test_point_itrf[1] - tr_center[1]:.6f}, {test_point_itrf[2] - tr_center[2]:.6f})")
        
        # Cr√©ation du nouveau nuage
        new_cloud = o3d.geometry.PointCloud()
        new_cloud.points = o3d.utility.Vector3dVector(arr_pts_ENU)
        
        # Copie des couleurs et normales
        if cloud.has_colors():
            new_cloud.colors = cloud.colors
        if cloud.has_normals():
            new_cloud.normals = cloud.normals
        
        # Sauvegarde
        output_file = os.path.join(output_dir, os.path.basename(ply_file))
        success = o3d.io.write_point_cloud(output_file, new_cloud)
        
        if success:
            return True, f"Conversion ITRF‚ÜíENU : {os.path.basename(ply_file)} ({len(points)} points)"
        else:
            return False, f"Erreur de sauvegarde : {os.path.basename(ply_file)}"
        
    except Exception as e:
        return False, f"Erreur lors de la conversion ITRF‚ÜíENU de {os.path.basename(ply_file)} : {e}"

def process_single_cloud_deform(args):
    """Fonction de traitement d'un seul nuage pour la d√©formation (pour multiprocessing)"""
    ply_file, output_dir, residues_enu, gcp_positions, deformation_type = args
    
    # Cr√©ation d'un logger pour ce processus
    logger = logging.getLogger(f"Deform_{os.getpid()}")
    logger.setLevel(logging.INFO)
    
    try:
        import open3d as o3d
        from scipy.spatial.distance import cdist
        from scipy.linalg import solve
        
        # Lecture du nuage
        cloud = o3d.io.read_point_cloud(ply_file)
        if not cloud.has_points():
            return False, f"Nuage vide dans {os.path.basename(ply_file)}"
        
        points = np.asarray(cloud.points)
        logger.info(f"  {len(points)} points charg√©s")
        
        # Pr√©paration des donn√©es pour l'interpolation TPS
        control_points = []
        control_values = []
        
        for gcp_name, residue_data in residues_enu.items():
            if gcp_name in gcp_positions:
                control_points.append(gcp_positions[gcp_name])
                control_values.append(residue_data['offset'])
                logger.info(f"    Point de contr√¥le {gcp_name}: position={gcp_positions[gcp_name]}, correction={residue_data['offset']}")
        
        if len(control_points) < 3:
            return False, f"Trop peu de points de contr√¥le ({len(control_points)}) pour {os.path.basename(ply_file)}"
        
        # Conversion en arrays numpy
        control_points = np.array(control_points)
        control_values = np.array(control_values)
        
        # Application de la d√©formation selon le type
        if deformation_type == "tps":
            logger.info(f"  Interpolation TPS avec {len(control_points)} points de contr√¥le...")
            
            # Interpolation TPS
            def thin_plate_spline_interpolation(points, control_points, control_values):
                """Interpolation par Thin Plate Splines"""
                try:
                    from scipy.spatial.distance import cdist
                    from scipy.linalg import solve
                    
                    M = len(control_points)
                    N = len(points)
                    
                    # Calcul des distances entre points de contr√¥le
                    K = cdist(control_points, control_points, metric='euclidean')
                    # Fonction de base radiale (RBF)
                    K = K * np.log(K + 1e-10)  # √âviter log(0)
                    
                    # Construction du syst√®me lin√©aire
                    # [K  P] [w] = [v]
                    # [P' 0] [a]   [0]
                    # o√π P = [1, x, y, z] pour chaque point de contr√¥le
                    
                    P = np.column_stack([np.ones(M), control_points])
                    A = np.block([[K, P], [P.T, np.zeros((4, 4))]])
                    
                    # R√©solution pour chaque composante (x, y, z)
                    interpolated_values = np.zeros((N, 3))
                    
                    for dim in range(3):
                        b = np.concatenate([control_values[:, dim], np.zeros(4)])
                        solution = solve(A, b)
                        w = solution[:M]
                        a = solution[M:]
                        
                        # Calcul des distances entre points d'interpolation et points de contr√¥le
                        K_interp = cdist(points, control_points, metric='euclidean')
                        K_interp = K_interp * np.log(K_interp + 1e-10)
                        
                        # Interpolation
                        P_interp = np.column_stack([np.ones(N), points])
                        interpolated_values[:, dim] = K_interp @ w + P_interp @ a
                    
                    return interpolated_values
                    
                except ImportError:
                    # Fallback si scipy n'est pas disponible
                    logger.warning("scipy non disponible, utilisation de l'interpolation lin√©aire")
                    return linear_interpolation(points, control_points, control_values)
            
            def linear_interpolation(points, control_points, control_values):
                """Interpolation lin√©aire simple comme fallback"""
                # Calcul de la moyenne des corrections
                mean_correction = np.mean(control_values, axis=0)
                return np.tile(mean_correction, (len(points), 1))
            
            # Application de l'interpolation TPS
            deformations = thin_plate_spline_interpolation(points, control_points, control_values)
            
        elif deformation_type == "lineaire":
            logger.info(f"  Interpolation lin√©aire avec {len(control_points)} points de contr√¥le...")
            
            def linear_interpolation(points, control_points, control_values):
                """Interpolation lin√©aire par distance inverse pond√©r√©e"""
                from scipy.spatial.distance import cdist
                
                # Calcul des distances
                distances = cdist(points, control_points, metric='euclidean')
                
                # Pond√©ration par distance inverse
                weights = 1.0 / (distances + 1e-10)
                weights = weights / np.sum(weights, axis=1, keepdims=True)
                
                # Interpolation
                interpolated_values = weights @ control_values
                
                return interpolated_values
            
            deformations = linear_interpolation(points, control_points, control_values)
            
        else:  # uniforme
            logger.info(f"  D√©formation uniforme avec {len(control_points)} points de contr√¥le...")
            
            # Calcul de la moyenne des corrections
            mean_correction = np.mean(control_values, axis=0)
            deformations = np.tile(mean_correction, (len(points), 1))
        
        # Application des d√©formations
        deformed_points = points + deformations
        
        # Calcul des statistiques de d√©formation
        deformation_magnitudes = np.linalg.norm(deformations, axis=1)
        min_deform = np.min(deformation_magnitudes)
        max_deform = np.max(deformation_magnitudes)
        mean_deform = np.mean(deformation_magnitudes)
        
        logger.info(f"  D√©formation TPS appliqu√©e - min: {min_deform:.6f}, max: {max_deform:.6f}, moy: {mean_deform:.6f}")
        
        # Cr√©ation du nouveau nuage
        new_cloud = o3d.geometry.PointCloud()
        new_cloud.points = o3d.utility.Vector3dVector(deformed_points)
        
        # Copie des couleurs et normales
        if cloud.has_colors():
            new_cloud.colors = cloud.colors
            logger.info("  Couleurs copi√©es")
        
        if cloud.has_normals():
            new_cloud.normals = cloud.normals
            logger.info("  Normales copi√©es")
        
        # Sauvegarde
        output_file = os.path.join(output_dir, os.path.basename(ply_file))
        success = o3d.io.write_point_cloud(output_file, new_cloud)
        
        if success:
            return True, f"D√©formation TPS : {os.path.basename(ply_file)} ({len(points)} points)"
        else:
            return False, f"Erreur de sauvegarde : {os.path.basename(ply_file)}"
        
    except Exception as e:
        return False, f"Erreur lors de la d√©formation de {os.path.basename(ply_file)} : {e}"

def add_offset_to_clouds(input_dir, logger, coord_file=None, extra_params="", max_workers=None):
    """Ajoute l'offset aux nuages de points .ply dans le dossier fourni (et ses sous-dossiers √©ventuels)"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"Ajout de l'offset aux nuages dans {abs_input_dir} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    if not coord_file:
        logger.error("Aucun fichier de coordonn√©es fourni pour l'ajout d'offset.")
        raise RuntimeError("Aucun fichier de coordonn√©es fourni pour l'ajout d'offset.")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    output_dir = os.path.join(os.path.dirname(abs_input_dir), "offset_step")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # Lecture du fichier de coordonn√©es pour extraire l'offset
    offset = None
    try:
        with open(coord_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line.startswith('#Offset to add :'):
                    # Format: #Offset to add : X Y Z
                    parts = line.split(':')[1].strip().split()
                    if len(parts) == 3:
                        offset = [float(parts[0]), float(parts[1]), float(parts[2])]
                        break
        
        if offset is None:
            logger.error("Offset non trouv√© dans le fichier de coordonn√©es.")
            raise RuntimeError("Offset non trouv√© dans le fichier de coordonn√©es.")
        
        logger.info(f"Offset extrait : {offset}")
        
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        logger.info("Open3D import√© avec succ√®s")
    except ImportError:
        logger.error("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
        raise RuntimeError("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
    
    total_files_processed = 0
    
    # Recherche des fichiers .ply dans le dossier fourni (et sous-dossiers)
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))  # Maximum 10 processus par d√©faut
    else:
        # Limiter par CPU disponibles ET fichiers pour √©viter surcharge cluster
        max_workers = min(max_workers, cpu_count(), len(ply_files))
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, output_dir, coord_file, extra_params))
    
    # Traitement parall√®le
    total_files_processed = 0
    failed_files = []
    
    try:
        with Pool(processes=max_workers) as pool:
            # Lancement du traitement parall√®le
            results = pool.map(process_single_cloud_add_offset, process_args)
            
            # Analyse des r√©sultats
            for i, (success, message) in enumerate(results):
                if success:
                    logger.info(f"‚úÖ {message}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {message}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel en cas d'erreur
        logger.info("Tentative de traitement s√©quentiel...")
        total_files_processed = 0
        for ply_file in ply_files:
            try:
                result = process_single_cloud_add_offset((ply_file, output_dir, coord_file, extra_params))
                if result[0]:
                    logger.info(f"‚úÖ {result[1]}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    # R√©sum√© final
    if failed_files:
        logger.warning(f"‚ö†Ô∏è {len(failed_files)} fichiers n'ont pas pu √™tre trait√©s")
    logger.info(f"Ajout d'offset termin√©. {total_files_processed} fichiers trait√©s dans {output_dir}.")

def convert_itrf_to_enu(input_dir, logger, coord_file=None, extra_params="", ref_point_name=None, max_workers=None, global_ref_point=None, force_global_ref=False):
    # DEBUG : V√©rification des param√®tres re√ßus
    logger.info(f"üîç DEBUG - Param√®tres re√ßus dans convert_itrf_to_enu:")
    logger.info(f"   global_ref_point: {global_ref_point}")
    logger.info(f"   force_global_ref: {force_global_ref}")
    logger.info(f"   ref_point_name: {ref_point_name}")
    """Convertit les nuages de points d'ITRF vers ENU"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"Conversion ITRF vers ENU dans {abs_input_dir} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    if not coord_file:
        logger.error("Aucun fichier de coordonn√©es fourni pour la conversion ITRF‚ÜíENU.")
        raise RuntimeError("Aucun fichier de coordonn√©es fourni pour la conversion ITRF‚ÜíENU.")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    output_dir = os.path.join(os.path.dirname(abs_input_dir), "itrf_to_enu_step")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # √âTAPE 1 : Lecture du fichier de coordonn√©es pour obtenir le point de r√©f√©rence
    logger.info(f"Lecture du fichier de coordonn√©es : {coord_file}")
    try:
        with open(coord_file, 'r') as f:
            lines = f.readlines()
        
        # Recherche du point de r√©f√©rence
        ref_point = None
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split()
                if len(parts) >= 4:  # Format: NOM X Y Z
                    try:
                        point_name = parts[0]
                        x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                        
                        # Si un nom de point de r√©f√©rence est sp√©cifi√©, on cherche ce point
                        if ref_point_name and point_name == ref_point_name:
                            ref_point = np.array([x, y, z])
                            logger.info(f"Point de r√©f√©rence sp√©cifi√© trouv√© : {point_name} ({x:.6f}, {y:.6f}, {z:.6f})")
                            break
                        # Sinon, on prend le premier point valide
                        elif ref_point is None:
                            ref_point = np.array([x, y, z])
                            logger.info(f"Point de r√©f√©rence trouv√© : {point_name} ({x:.6f}, {y:.6f}, {z:.6f})")
                            break
                    except ValueError:
                        continue
        
        if ref_point is None:
            if ref_point_name:
                raise RuntimeError(f"Point de r√©f√©rence '{ref_point_name}' non trouv√© dans le fichier de coordonn√©es")
            else:
                raise RuntimeError("Aucun point de r√©f√©rence valide trouv√© dans le fichier de coordonn√©es")
        
        # √âTAPE 1.5 : Priorisation du point de r√©f√©rence global si forc√©
        if force_global_ref and global_ref_point is not None:
            logger.info("üéØ UTILISATION DU POINT DE R√âF√âRENCE GLOBAL FORC√â")
            logger.info(f"Point global : ({global_ref_point[0]:.6f}, {global_ref_point[1]:.6f}, {global_ref_point[2]:.6f})")
            ref_point = np.array(global_ref_point)
            logger.info("Le point global remplace le point local pour unifier le rep√®re ENU")
            logger.info("‚ö†Ô∏è  L'offset ne s'applique PAS au point global (coordonn√©es absolues pr√©serv√©es)")
        else:
            logger.info("üìç UTILISATION DU POINT DE R√âF√âRENCE LOCAL")
            logger.info(f"Point local : ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
            
            # √âTAPE 1.6 : Lecture de l'offset et application au point local UNIQUEMENT
            logger.info("Lecture de l'offset depuis le fichier de coordonn√©es...")
            offset = None
            try:
                with open(coord_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line.startswith('#Offset to add :'):
                            # Format: #Offset to add : X Y Z
                            parts = line.split(':')[1].strip().split()
                            if len(parts) == 3:
                                offset = [float(parts[0]), float(parts[1]), float(parts[2])]
                                break
                
                if offset is None:
                    logger.warning("Offset non trouv√© dans le fichier de coordonn√©es. Utilisation du point de r√©f√©rence sans offset.")
                else:
                    logger.info(f"Offset trouv√© : {offset}")
                    # Application de l'offset au point local UNIQUEMENT
                    ref_point = ref_point + np.array(offset)
                    logger.info(f"Point local avec offset appliqu√© : ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
                    
            except Exception as e:
                logger.warning(f"Erreur lors de la lecture de l'offset : {e}. Utilisation du point de r√©f√©rence sans offset.")
            
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
    
    # √âTAPE 2 : Configuration de la transformation topocentrique
    logger.info("Configuration de la transformation topocentrique ITRF2020‚ÜíENU...")
    
    try:
        import pyproj
        
        # Le point de r√©f√©rence (ref_point) est le centre de la transformation topocentrique
        # Il d√©finit l'origine du syst√®me ENU local
        tr_center = ref_point  # [X, Y, Z] en ITRF2020
        tr_ellps = "GRS80"     # Ellipso√Øde de r√©f√©rence (standard pour ITRF)
        
        logger.info(f"Centre de transformation topocentrique : ({tr_center[0]:.3f}, {tr_center[1]:.3f}, {tr_center[2]:.3f})")
        logger.info(f"Ellipso√Øde de r√©f√©rence : {tr_ellps}")
        
        # Cr√©ation du pipeline de transformation topocentrique
        # +proj=topocentric : projection topocentrique
        # +X_0, +Y_0, +Z_0 : coordonn√©es du centre de transformation en ITRF
        # +ellps : ellipso√Øde de r√©f√©rence
        pipeline = "+proj=topocentric +X_0={0} +Y_0={1} +Z_0={2} +ellps={3}".format(
            tr_center[0], tr_center[1], tr_center[2], tr_ellps
        )
        
        transformer = pyproj.Transformer.from_pipeline(pipeline)
        logger.info(f"Pipeline de transformation cr√©√© : {pipeline}")
        
    except ImportError:
        logger.error("pyproj n'est pas install√©. La transformation topocentrique n√©cessite pyproj.")
        raise RuntimeError("pyproj n'est pas install√©. Veuillez l'installer avec: pip install pyproj")
    
    logger.info("Transformation topocentrique configur√©e avec succ√®s")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        logger.info("Open3D import√© avec succ√®s")
    except ImportError:
        logger.error("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
        raise RuntimeError("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
    
    # √âTAPE 4 : Traitement des nuages de points
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.lower().endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))  # Maximum 10 processus par d√©faut
    else:
        # Limiter par CPU disponibles ET fichiers pour √©viter surcharge cluster
        max_workers = min(max_workers, cpu_count(), len(ply_files))
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, output_dir, coord_file, extra_params, ref_point_name))
    
    # Traitement parall√®le
    total_files_processed = 0
    failed_files = []
    
    try:
        # DEBUG : V√©rification avant l'appel parall√®le
        logger.info(f"üîç DEBUG - Pr√©paration de l'appel √† process_single_cloud_itrf_to_enu:")
        logger.info(f"   Nombre de fichiers PLY: {len(ply_files)}")
        logger.info(f"   Max workers: {max_workers}")
        logger.info(f"   Centre de transformation: ({tr_center[0]:.6f}, {tr_center[1]:.6f}, {tr_center[2]:.6f})")
        logger.info(f"   Premier fichier: {os.path.basename(ply_files[0]) if ply_files else 'Aucun'}")
        
        # Traitement parall√®le avec Pool
        with Pool(processes=max_workers) as pool:
            # Lancement du traitement parall√®le
            # Ajout des param√®tres du point global aux arguments
            extended_process_args = [args + (global_ref_point, force_global_ref) for args in process_args]
            results = pool.map(process_single_cloud_itrf_to_enu, extended_process_args)
            
            # Analyse des r√©sultats
            for i, (success, message) in enumerate(results):
                if success:
                    logger.info(f"‚úÖ {message}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {message}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel en cas d'erreur
        logger.info("Tentative de traitement s√©quentiel...")
        total_files_processed = 0
        for ply_file in ply_files:
            try:
                result = process_single_cloud_itrf_to_enu((ply_file, output_dir, coord_file, extra_params, ref_point_name))
                if result[0]:
                    logger.info(f"‚úÖ {result[1]}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    # R√©sum√© final
    if failed_files:
        logger.warning(f"‚ö†Ô∏è {len(failed_files)} fichiers n'ont pas pu √™tre trait√©s")
    logger.info(f"Conversion ITRF vers ENU termin√©e. {total_files_processed} fichiers trait√©s dans {output_dir}.")

def deform_clouds(input_dir, logger, deformation_type="lineaire", deformation_params="", extra_params="", bascule_xml_file=None, coord_file=None, max_workers=None, global_ref_point=None, force_global_ref=False):
    """Applique une d√©formation aux nuages de points bas√©e sur les r√©sidus GCPBascule"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"D√©formation des nuages dans {abs_input_dir} avec le type {deformation_type} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        logger.info("Open3D import√© avec succ√®s")
    except ImportError:
        logger.error("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
        raise RuntimeError("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    output_dir = os.path.join(os.path.dirname(abs_input_dir), f"deform_{deformation_type}_step")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # √âTAPE 1 : Lecture des r√©sidus depuis le fichier XML GCPBascule
    logger.info("Lecture des r√©sidus GCPBascule...")
    
    if not bascule_xml_file:
        logger.error("Aucun fichier XML GCPBascule sp√©cifi√©")
        raise RuntimeError("Aucun fichier XML GCPBascule sp√©cifi√©. Utilisez le param√®tre bascule_xml_file.")
    
    if not os.path.exists(bascule_xml_file):
        logger.error(f"Fichier XML GCPBascule introuvable : {bascule_xml_file}")
        raise RuntimeError(f"Fichier XML GCPBascule introuvable : {bascule_xml_file}")
    
    xml_file = bascule_xml_file
    logger.info(f"Fichier XML GCPBascule : {xml_file}")
    
    # Lecture et parsing du XML
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        # Extraction des r√©sidus
        residues = {}
        for residue in root.findall('.//Residus'):
            name = residue.find('Name').text
            offset_elem = residue.find('Offset')
            offset_text = offset_elem.text.strip().split()
            offset = [float(x) for x in offset_text]
            dist = float(residue.find('Dist').text)
            
            residues[name] = {
                'offset': np.array(offset),  # R√©sidu en ITRF
                'distance': dist
            }
            logger.info(f"R√©sidu {name}: offset={offset}, distance={dist:.3f}m")
        
        if not residues:
            logger.error("Aucun r√©sidu trouv√© dans le fichier XML")
            raise RuntimeError("Aucun r√©sidu trouv√© dans le fichier XML GCPBascule")
            
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier XML : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier XML : {e}")
    
    logger.info(f"Lecture termin√©e : {len(residues)} r√©sidus trouv√©s")
    
    # √âTAPE 1.5 : Conversion des r√©sidus ITRF vers ENU
    logger.info("Conversion des r√©sidus ITRF vers ENU...")
    
    # Nous avons besoin du point de r√©f√©rence pour la transformation topocentrique
    # Nous devons le lire depuis le fichier de coordonn√©es
    if not coord_file:
        logger.error("Fichier de coordonn√©es requis pour la conversion des r√©sidus ITRF‚ÜíENU")
        raise RuntimeError("Fichier de coordonn√©es requis pour la conversion des r√©sidus ITRF‚ÜíENU")
    
    if not os.path.exists(coord_file):
        logger.error(f"Fichier de coordonn√©es introuvable : {coord_file}")
        raise RuntimeError(f"Fichier de coordonn√©es introuvable : {coord_file}")
    
    # Lecture du point de r√©f√©rence depuis le fichier de coordonn√©es
    try:
        with open(coord_file, 'r') as f:
            lines = f.readlines()
        
        # √âTAPE 1.5 : Priorisation du point de r√©f√©rence global si forc√©
        if force_global_ref and global_ref_point is not None:
            logger.info("üéØ UTILISATION DU POINT DE R√âF√âRENCE GLOBAL FORC√â (d√©formation)")
            logger.info(f"Point global : ({global_ref_point[0]:.6f}, {global_ref_point[1]:.6f}, {global_ref_point[2]:.6f})")
            ref_point = np.array(global_ref_point)
            logger.info("Le point global remplace le point local pour la d√©formation")
            logger.info("‚ö†Ô∏è  L'offset ne s'applique PAS au point global (coordonn√©es absolues pr√©serv√©es)")
        else:
            logger.info("üìç UTILISATION DU POINT DE R√âF√âRENCE LOCAL (d√©formation)")
            
            # Recherche du point de r√©f√©rence (premier point par d√©faut)
            ref_point = None
            offset = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('#F=') or line.startswith('#'):
                    continue
                
                # Format attendu : NOM X Y Z
                parts = line.split()
                if len(parts) >= 4:
                    try:
                        x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                        ref_point = np.array([x, y, z])
                        logger.info(f"Point local trouv√© : {parts[0]} ({x:.6f}, {y:.6f}, {z:.6f})")
                        break
                    except ValueError:
                        continue
            
            if ref_point is None:
                logger.error("Aucun point de r√©f√©rence valide trouv√© dans le fichier de coordonn√©es")
                raise RuntimeError("Aucun point de r√©f√©rence valide trouv√© dans le fichier de coordonn√©es")
            
            # Lecture de l'offset et application au point local UNIQUEMENT
            try:
                for line in lines:
                    if line.startswith('#Offset to add :'):
                        offset_text = line.replace('#Offset to add :', '').strip()
                        offset = [float(x) for x in offset_text.split()]
                        logger.info(f"Offset trouv√© : {offset}")
                        break
            except Exception as e:
                logger.warning(f"Erreur lors de la lecture de l'offset : {e}. Utilisation du point de r√©f√©rence sans offset.")
                offset = [0.0, 0.0, 0.0]
            
            # Application de l'offset au point local UNIQUEMENT
            if offset:
                ref_point = ref_point + np.array(offset)
                logger.info(f"Point local avec offset appliqu√© : ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
        
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
    
    # Configuration de la transformation topocentrique pour les r√©sidus
    try:
        import pyproj
        
        # Le point de r√©f√©rence (ref_point) est le centre de la transformation topocentrique
        tr_center = ref_point  # [X, Y, Z] en ITRF2020
        tr_ellps = "GRS80"     # Ellipso√Øde de r√©f√©rence (standard pour ITRF)
        
        logger.info(f"Centre de transformation pour r√©sidus : ({tr_center[0]:.3f}, {tr_center[1]:.3f}, {tr_center[2]:.3f})")
        
        # Cr√©ation du pipeline de transformation topocentrique
        pipeline = "+proj=topocentric +X_0={0} +Y_0={1} +Z_0={2} +ellps={3}".format(
            tr_center[0], tr_center[1], tr_center[2], tr_ellps
        )
        
        transformer = pyproj.Transformer.from_pipeline(pipeline)
        logger.info(f"Pipeline de transformation pour r√©sidus cr√©√© : {pipeline}")
        
    except ImportError:
        logger.error("pyproj n'est pas install√©. Veuillez l'installer avec: pip install pyproj")
        raise RuntimeError("pyproj n'est pas install√©. Veuillez l'installer avec: pip install pyproj")
    except Exception as e:
        logger.error(f"Erreur lors de la configuration de la transformation : {e}")
        raise RuntimeError(f"Erreur lors de la configuration de la transformation : {e}")
    
    # Conversion des r√©sidus ITRF vers ENU
    residues_enu = {}
    for name, residue_data in residues.items():
        # Le r√©sidu est un vecteur de d√©placement en ITRF
        itrf_offset = residue_data['offset']
        
        # Conversion du vecteur de d√©placement ITRF vers ENU
        # Pour un vecteur de d√©placement, nous utilisons la matrice de rotation
        # qui est la d√©riv√©e de la transformation topocentrique
        
        # M√©thode 1 : Utilisation de la matrice de rotation
        # La transformation topocentrique a une matrice de rotation R
        # Pour un vecteur de d√©placement : ENU_vector = R * ITRF_vector
        
        # Calcul de la matrice de rotation (approximation)
        # Pour un point proche du centre de transformation, la rotation est approximativement constante
        
        # M√©thode 2 : Transformation directe (plus pr√©cise)
        # Nous transformons le point de r√©f√©rence + le vecteur de d√©placement
        point_with_offset = ref_point + itrf_offset
        enu_point = transformer.transform(point_with_offset[0], point_with_offset[1], point_with_offset[2])
        enu_ref = transformer.transform(ref_point[0], ref_point[1], ref_point[2])
        
        # Le vecteur de d√©placement ENU est la diff√©rence
        enu_offset = np.array([enu_point[0] - enu_ref[0], 
                              enu_point[1] - enu_ref[1], 
                              enu_point[2] - enu_ref[2]])
        
        # DEBUG : V√©rification des coordonn√©es de transformation
        logger.info(f"üîç DEBUG - Transformation r√©sidu {name}:")
        logger.info(f"   Point de r√©f√©rence ITRF: ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
        logger.info(f"   Point avec offset ITRF: ({point_with_offset[0]:.6f}, {point_with_offset[1]:.6f}, {point_with_offset[2]:.6f})")
        logger.info(f"   Point de r√©f√©rence ENU: ({enu_ref[0]:.6f}, {enu_ref[1]:.6f}, {enu_ref[2]:.6f})")
        logger.info(f"   Point avec offset ENU: ({enu_point[0]:.6f}, {enu_point[1]:.6f}, {enu_point[2]:.6f})")
        logger.info(f"   Vecteur d√©placement ENU: ({enu_offset[0]:.6f}, {enu_offset[1]:.6f}, {enu_offset[2]:.6f})")
        
        residues_enu[name] = {
            'offset': enu_offset,
            'distance': residue_data['distance']
        }
        
        logger.info(f"R√©sidu {name} ENU: offset={enu_offset.tolist()}, distance={residue_data['distance']:.3f}m")
    
    logger.info(f"Conversion termin√©e : {len(residues_enu)} r√©sidus convertis en ENU")
    
    # √âTAPE 1.6 : Pr√©paration des donn√©es pour l'interpolation TPS
    logger.info("Pr√©paration des donn√©es pour l'interpolation TPS...")
    
    # Nous avons besoin des positions des GCPs dans les nuages pour l'interpolation
    # Pour l'instant, nous utilisons une approximation bas√©e sur les coordonn√©es nominales
    # TODO: Impl√©menter la d√©tection automatique des GCPs dans les nuages
    
    # Extraction des positions des GCPs depuis le fichier de coordonn√©es
    gcp_positions = {}
    try:
        with open(coord_file, 'r') as f:
            lines = f.readlines()
        
        # IMPORTANT : Relecture de l'offset du fichier de coordonn√©es pour les GCPs
        coord_offset = None
        try:
            for line in lines:
                if line.startswith('#Offset to add :'):
                    offset_text = line.replace('#Offset to add :', '').strip()
                    coord_offset = [float(x) for x in offset_text.split()]
                    logger.info(f"Offset du fichier de coordonn√©es pour GCPs : {coord_offset}")
                    break
        except Exception as e:
            logger.warning(f"Erreur lors de la lecture de l'offset pour GCPs : {e}")
            coord_offset = [0.0, 0.0, 0.0]
        
        for line in lines:
            line = line.strip()
            if line.startswith('#F=') or line.startswith('#'):
                continue
            
            parts = line.split()
            if len(parts) >= 4:
                try:
                    name = parts[0]
                    x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                    
                    # IMPORTANT : L'offset doit TOUJOURS √™tre appliqu√© pour obtenir les vraies coordonn√©es ITRF
                    # Le point global sert seulement de centre de transformation ENU
                    if coord_offset is not None:
                        x += coord_offset[0]
                        y += coord_offset[1]
                        z += coord_offset[2]
                        logger.info(f"GCP {name} : offset appliqu√© ({coord_offset[0]:.6f}, {coord_offset[1]:.6f}, {coord_offset[2]:.6f})")
                    else:
                        logger.warning(f"GCP {name} : pas d'offset disponible, coordonn√©es relatives utilis√©es")
                    
                    point_itrf = np.array([x, y, z])
                    
                    if force_global_ref and global_ref_point is not None:
                        logger.info(f"GCP {name} : transformation ENU avec le point global comme centre")
                    else:
                        logger.info(f"GCP {name} : transformation ENU avec le point local comme centre")
                    
                    # Conversion en ENU avec le M√äME transformer
                    enu_pos = transformer.transform(point_itrf[0], point_itrf[1], point_itrf[2])
                    gcp_positions[name] = np.array([enu_pos[0], enu_pos[1], enu_pos[2]])
                    
                    logger.info(f"GCP {name}: ITRF({point_itrf[0]:.6f}, {point_itrf[1]:.6f}, {point_itrf[2]:.6f}) ‚Üí ENU({enu_pos[0]:.6f}, {enu_pos[1]:.6f}, {enu_pos[2]:.6f})")
                    
                except ValueError:
                    continue
        
        logger.info(f"Positions des GCPs pr√©par√©es : {len(gcp_positions)} points")
        
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©paration des positions GCPs : {e}")
        raise RuntimeError(f"Erreur lors de la pr√©paration des positions GCPs : {e}")
    
    # √âTAPE 2 : Fonctions d'interpolation TPS
    def thin_plate_spline_interpolation(points, control_points, control_values):
        """
        Interpolation par Thin Plate Splines
        
        Args:
            points: Points √† interpoler (N, 3)
            control_points: Points de contr√¥le (M, 3)
            control_values: Valeurs aux points de contr√¥le (M, 3)
        
        Returns:
            Valeurs interpol√©es aux points (N, 3)
        """
        try:
            from scipy.spatial.distance import cdist
            from scipy.linalg import solve
            
            M = len(control_points)
            N = len(points)
            
            # Calcul des distances entre points de contr√¥le
            K = cdist(control_points, control_points, metric='euclidean')
            # Fonction de base radiale (RBF)
            K = K * np.log(K + 1e-10)  # √âviter log(0)
            
            # Construction du syst√®me lin√©aire
            # [K  P] [w] = [v]
            # [P' 0] [a]   [0]
            # o√π P = [1, x, y, z] pour chaque point de contr√¥le
            
            P = np.column_stack([np.ones(M), control_points])
            A = np.block([[K, P], [P.T, np.zeros((4, 4))]])
            
            # R√©solution pour chaque composante (x, y, z)
            interpolated_values = np.zeros((N, 3))
            
            for dim in range(3):
                b = np.concatenate([control_values[:, dim], np.zeros(4)])
                solution = solve(A, b)
                w = solution[:M]
                a = solution[M:]
                
                # Calcul des distances entre points d'interpolation et points de contr√¥le
                K_interp = cdist(points, control_points, metric='euclidean')
                K_interp = K_interp * np.log(K_interp + 1e-10)
                
                # Interpolation
                P_interp = np.column_stack([np.ones(N), points])
                interpolated_values[:, dim] = K_interp @ w + P_interp @ a
            
            return interpolated_values
            
        except ImportError:
            logger.warning("scipy non disponible, utilisation de l'interpolation lin√©aire")
            return linear_interpolation(points, control_points, control_values)
    
    def linear_interpolation(points, control_points, control_values):
        """
        Interpolation lin√©aire simple par distance inverse pond√©r√©e
        """
        from scipy.spatial.distance import cdist
        
        # Calcul des distances
        distances = cdist(points, control_points, metric='euclidean')
        
        # Pond√©ration par distance inverse
        weights = 1.0 / (distances + 1e-10)
        weights = weights / np.sum(weights, axis=1, keepdims=True)
        
        # Interpolation
        interpolated_values = weights @ control_values
        
        return interpolated_values
    
    # √âTAPE 3 : Traitement des nuages de points
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.lower().endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))  # Maximum 10 processus par d√©faut
    else:
        # Limiter par CPU disponibles ET fichiers pour √©viter surcharge cluster
        max_workers = min(max_workers, cpu_count(), len(ply_files))
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, output_dir, residues_enu, gcp_positions, deformation_type))
    
    # Traitement parall√®le
    total_files_processed = 0
    failed_files = []
    
    try:
        with Pool(processes=max_workers) as pool:
            # Lancement du traitement parall√®le
            results = pool.map(process_single_cloud_deform, process_args)
            
            # Analyse des r√©sultats
            for i, (success, message) in enumerate(results):
                if success:
                    logger.info(f"‚úÖ {message}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {message}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel en cas d'erreur
        logger.info("Tentative de traitement s√©quentiel...")
        total_files_processed = 0
        for ply_file in ply_files:
            try:
                result = process_single_cloud_deform((ply_file, output_dir, residues_enu, gcp_positions, deformation_type))
                if result[0]:
                    logger.info(f"‚úÖ {result[1]}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    # R√©sum√© final
    if failed_files:
        logger.warning(f"‚ö†Ô∏è {len(failed_files)} fichiers n'ont pas pu √™tre trait√©s")
    logger.info(f"D√©formation {deformation_type} termin√©e. {total_files_processed} fichiers trait√©s dans {output_dir}.")

def create_orthoimage_from_pointcloud(input_dir, logger, output_dir=None, resolution=0.1, height_field="z", color_field="rgb", max_workers=None):
    """Cr√©e une orthoimage √† partir des nuages de points .ply dans le dossier fourni"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"Cr√©ation d'orthoimage √† partir des nuages dans {abs_input_dir} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(abs_input_dir), "orthoimage_step")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        logger.info("Open3D import√© avec succ√®s")
    except ImportError:
        logger.error("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
        raise RuntimeError("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
    
    # Recherche des fichiers .ply dans le dossier fourni (et sous-dossiers)
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))  # Maximum 10 processus par d√©faut
    else:
        max_workers = min(max_workers, len(ply_files))  # Respecter la limite demand√©e
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, output_dir, resolution, height_field, color_field))
    
    # Traitement parall√®le
    total_files_processed = 0
    failed_files = []
    
    try:
        with Pool(processes=max_workers) as pool:
            # Lancement du traitement parall√®le
            results = pool.map(process_single_cloud_orthoimage, process_args)
            
            # Analyse des r√©sultats
            for i, (success, message) in enumerate(results):
                if success:
                    logger.info(f"‚úÖ {message}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {message}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel en cas d'erreur
        logger.info("Tentative de traitement s√©quentiel...")
        total_files_processed = 0
        for ply_file in ply_files:
            try:
                result = process_single_cloud_orthoimage((ply_file, output_dir, resolution, height_field, color_field))
                if result[0]:
                    logger.info(f"‚úÖ {result[1]}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    # R√©sum√© final
    if failed_files:
        logger.warning(f"‚ö†Ô∏è {len(failed_files)} fichiers n'ont pas pu √™tre trait√©s")
    logger.info(f"Cr√©ation d'orthoimage termin√©e. {total_files_processed} fichiers trait√©s dans {output_dir}.")



def create_unified_orthoimage_and_dtm(input_dir, logger, output_dir=None, resolution=0.1, max_workers=None):
    """Cr√©e une orthoimage et un MNT unifi√©s √† partir de tous les nuages de points .ply dans le dossier fourni"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"Cr√©ation d'orthoimage et MNT unifi√©s √† partir des nuages dans {abs_input_dir} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(abs_input_dir), "ortho_mnt_unified")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        import numpy as np
        import rasterio
        from rasterio.transform import from_origin
        
        # PATCH: Modules rasterio g√©r√©s globalement par patch_rasterio_essentials()
        
        logger.info("Open3D et Rasterio import√©s avec succ√®s")
    except ImportError as e:
        logger.error(f"Importation √©chou√©e : {e}")
        raise RuntimeError(f"Importation √©chou√©e : {e}")
    
    # Recherche des fichiers .ply dans le dossier fourni (et sous-dossiers)
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # √âTAPE 1 : Calculer l'√©tendue globale de tous les nuages
    logger.info("Calcul de l'√©tendue globale de tous les nuages...")
    global_min_coords = None
    global_max_coords = None
    
    for ply_file in ply_files:
        try:
            cloud = o3d.io.read_point_cloud(ply_file)
            if not cloud.has_points():
                continue
            
            points = np.asarray(cloud.points)
            min_coords = np.min(points, axis=0)
            max_coords = np.max(points, axis=0)
            
            if global_min_coords is None:
                global_min_coords = min_coords
                global_max_coords = max_coords
            else:
                global_min_coords = np.minimum(global_min_coords, min_coords)
                global_max_coords = np.maximum(global_max_coords, max_coords)
                
        except Exception as e:
            logger.warning(f"Erreur lors de la lecture de {os.path.basename(ply_file)} : {e}")
            continue
    
    if global_min_coords is None:
        logger.error("Aucun nuage valide trouv√©")
        return
    
    logger.info(f"√âtendue globale : X[{global_min_coords[0]:.3f}, {global_max_coords[0]:.3f}], Y[{global_min_coords[1]:.3f}, {global_max_coords[1]:.3f}]")
    
    # Cr√©ation de la grille globale
    x_range = np.arange(global_min_coords[0], global_max_coords[0] + resolution, resolution)
    y_range = np.arange(global_min_coords[1], global_max_coords[1] + resolution, resolution)
    
    logger.info(f"Grille globale cr√©√©e : {len(x_range)} x {len(y_range)} pixels")
    logger.info(f"R√©solution : {resolution}m par pixel")
    
    # √âTAPE 2 : Traitement des nuages individuels
    logger.info("Traitement des nuages individuels...")
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))
    else:
        max_workers = min(max_workers, len(ply_files))
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, global_min_coords, global_max_coords, resolution, len(x_range), len(y_range)))
    
    # Traitement parall√®le
    individual_rasters = []
    failed_files = []
    
    try:
        with Pool(processes=max_workers) as pool:
            results = pool.map(process_single_cloud_for_unified, process_args)
            
            for i, result in enumerate(results):
                if result[0]:  # Succ√®s
                    individual_rasters.append(result[1])
                    logger.info(f"‚úÖ {os.path.basename(ply_files[i])} trait√©")
                else:
                    logger.error(f"‚ùå {result[1]}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel
        logger.info("Tentative de traitement s√©quentiel...")
        for ply_file in ply_files:
            try:
                result = process_single_cloud_for_unified((ply_file, global_min_coords, global_max_coords, resolution, len(x_range), len(y_range)))
                if result[0]:
                    individual_rasters.append(result[1])
                    logger.info(f"‚úÖ {os.path.basename(ply_file)} trait√©")
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    if not individual_rasters:
        logger.error("Aucun raster individuel cr√©√©")
        return
    
    logger.info(f"Rasters individuels cr√©√©s : {len(individual_rasters)}")
    
    # √âTAPE 3 : Fusion des rasters
    logger.info("Fusion des rasters individuels...")
    
    # Initialisation des rasters unifi√©s
    unified_height = np.full((len(y_range), len(x_range)), np.nan)
    unified_color = np.zeros((len(y_range), len(x_range), 3), dtype=np.uint8)
    unified_count = np.zeros((len(y_range), len(x_range)), dtype=np.uint8)
    
    # Fusion des rasters
    for height_raster, color_raster in individual_rasters:
        # Pour la hauteur : prendre le maximum (√©viter les occlusions)
        mask = ~np.isnan(height_raster)
        unified_height[mask] = np.maximum(unified_height[mask], height_raster[mask])
        
        # Pour la couleur : moyenne pond√©r√©e
        for c in range(3):  # RGB
            color_channel = color_raster[:, :, c]
            valid_mask = (color_channel > 0) & ~np.isnan(height_raster)
            if np.any(valid_mask):
                # Moyenne pond√©r√©e par la hauteur
                unified_color[valid_mask, c] = (
                    (unified_color[valid_mask, c] * unified_count[valid_mask] + 
                     color_channel[valid_mask] * height_raster[valid_mask]) / 
                    (unified_count[valid_mask] + height_raster[valid_mask])
                ).astype(np.uint8)
        
        # Compteur pour la moyenne
        unified_count[mask] += 1
    
    # Normalisation finale de la couleur
    valid_color_mask = unified_count > 0
    for c in range(3):
        unified_color[valid_color_mask, c] = (unified_color[valid_color_mask, c] / unified_count[valid_color_mask]).astype(np.uint8)
    
    logger.info(f"Fusion termin√©e. Pixels valides : {np.sum(~np.isnan(unified_height))}")
    
    # √âTAPE 4 : Sauvegarde des r√©sultats unifi√©s
    logger.info("Sauvegarde des r√©sultats unifi√©s...")
    
    # Calcul du g√©or√©f√©rencement
    origin_x = global_min_coords[0]
    origin_y = global_max_coords[1]  # Y invers√© pour l'image
    transform = from_origin(origin_x, origin_y, resolution, resolution)
    
    # M√©tadonn√©es
    metadata = {
        'Software': 'PhotoGeoAlign Unified Orthoimage/DTM Generator',
        'Resolution': f'{resolution}m per pixel',
        'Origin_X': f'{origin_x:.6f}',
        'Origin_Y': f'{origin_y:.6f}',
        'Extent_X': f'{float(global_max_coords[0] - global_min_coords[0]):.3f}m',
'Extent_Y': f'{float(global_max_coords[1] - global_min_coords[1]):.3f}m',
        'Source_Files': str(len(ply_files)),
        'Valid_Pixels': str(int(np.sum(~np.isnan(unified_height)))),
        'Height_Range': f'{np.nanmin(unified_height):.3f}m to {np.nanmax(unified_height):.3f}m' if np.sum(~np.isnan(unified_height)) > 0 else 'No valid pixels'
    }
    
    # Sauvegarde de l'orthoimage unifi√©e
    orthoimage_filename = "unified_orthoimage.tif"
    orthoimage_path = os.path.join(output_dir, orthoimage_filename)
    
    with rasterio.open(
        orthoimage_path,
        'w',
        driver='GTiff',
        height=unified_color.shape[0],
        width=unified_color.shape[1],
        count=3,
        dtype=unified_color.dtype,
        crs='+proj=utm +zone=30 +datum=WGS84',
        transform=transform,
        photometric='rgb'
    ) as dst:
        dst.write(unified_color[:,:,0], 1)  # Rouge
        dst.write(unified_color[:,:,1], 2)  # Vert
        dst.write(unified_color[:,:,2], 3)  # Bleu
        dst.update_tags(**metadata)
    
    # Sauvegarde du MNT unifi√©
    dtm_filename = "unified_dtm.tif"
    dtm_path = os.path.join(output_dir, dtm_filename)
    
    # Conversion des NaN en nodata
    dtm_data = unified_height.copy()
    dtm_data = np.where(np.isnan(dtm_data), 0, dtm_data)
    
    with rasterio.open(
        dtm_path,
        'w',
        driver='GTiff',
        height=dtm_data.shape[0],
        width=dtm_data.shape[1],
        count=1,
        dtype=dtm_data.dtype,
        crs='+proj=utm +zone=30 +datum=WGS84',
        transform=transform,
        nodata=0
    ) as dst:
        dst.write(dtm_data, 1)
        dst.update_tags(**metadata)
    
    logger.info(f"Orthoimage unifi√©e sauvegard√©e : {orthoimage_filename}")
    logger.info(f"MNT unifi√© sauvegard√© : {dtm_filename}")
    logger.info(f"Cr√©ation d'orthoimage et MNT unifi√©s termin√©e dans {output_dir}")

def process_single_cloud_for_unified(args):
    """Fonction de traitement d'un seul nuage pour la cr√©ation d'orthoimage/MNT unifi√©s (pour multiprocessing)"""
    ply_file, global_min_coords, global_max_coords, resolution, grid_width, grid_height = args
    
    # Cr√©ation d'un logger pour ce processus
    logger = logging.getLogger(f"Unified_{os.getpid()}")
    logger.setLevel(logging.INFO)
    
    try:
        import open3d as o3d
        import numpy as np
        
        # Lecture du nuage
        cloud = o3d.io.read_point_cloud(ply_file)
        if not cloud.has_points():
            return False, f"Nuage vide dans {os.path.basename(ply_file)}"
        
        points = np.asarray(cloud.points)
        logger.info(f"  {len(points)} points charg√©s")
        
        # Cr√©ation des matrices pour ce nuage
        height_raster = np.full((grid_height, grid_width), np.nan)
        color_raster = np.zeros((grid_height, grid_width, 3), dtype=np.uint8)
        
        # Rasterisation des points
        points_processed = 0
        for i, point in enumerate(points):
            # Conversion des coordonn√©es en indices de grille globale
            x_idx = int((point[0] - global_min_coords[0]) / resolution)
            y_idx = int((point[1] - global_min_coords[1]) / resolution)
            
            # V√©rification des limites
            if 0 <= x_idx < grid_width and 0 <= y_idx < grid_height:
                points_processed += 1
                # Mise √† jour de la hauteur (prendre la plus haute si plusieurs points)
                if np.isnan(height_raster[y_idx, x_idx]) or point[2] > height_raster[y_idx, x_idx]:
                    height_raster[y_idx, x_idx] = point[2]
                
                # Mise √† jour de la couleur
                if cloud.has_colors():
                    colors = np.asarray(cloud.colors)
                    color = colors[i]
                    # Conversion de [0,1] vers [0,255]
                    color_255 = (color * 255).astype(np.uint8)
                    color_raster[y_idx, x_idx] = color_255
        
        logger.info(f"  Points trait√©s : {points_processed}/{len(points)}")
        
        return True, (height_raster, color_raster)
        
    except Exception as e:
        return False, f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}"

def merge_orthoimages_and_dtm(input_dir, logger, output_dir=None, target_resolution=None, max_workers=None, color_fusion_method="average"):
    """Fusionne les orthoimages et MNT individuels d√©j√† g√©n√©r√©s en orthoimage et MNT unifi√©s"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"Fusion des orthoimages et MNT dans {abs_input_dir} ...")
    logger.info(f"M√©thode de fusion des couleurs : {color_fusion_method}")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(abs_input_dir), "ortho_mnt_unified")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # Import des biblioth√®ques n√©cessaires
    try:
        import numpy as np
        import rasterio
        from rasterio.warp import reproject, Resampling
        from rasterio.transform import from_origin
        
        # PATCH: Modules rasterio g√©r√©s globalement par patch_rasterio_essentials()
        
        logger.info("Rasterio import√© avec succ√®s")
    except ImportError as e:
        logger.error(f"Importation √©chou√©e : {e}")
        raise RuntimeError(f"Importation √©chou√©e : {e}")
    
    # Recherche des fichiers .tif dans le dossier fourni
    height_files = []
    color_files = []
    
    for file in os.listdir(abs_input_dir):
        if file.endswith('_height.tif'):
            height_files.append(os.path.join(abs_input_dir, file))
        elif file.endswith('_color.tif'):
            color_files.append(os.path.join(abs_input_dir, file))
    
    logger.info(f"Trouv√© {len(height_files)} fichiers de hauteur et {len(color_files)} fichiers couleur")
    
    if len(height_files) == 0:
        logger.warning(f"Aucun fichier de hauteur trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # √âTAPE 1 : Calculer l'√©tendue globale et la r√©solution
    logger.info("Calcul de l'√©tendue globale et de la r√©solution...")
    
    # Lire le premier fichier pour obtenir les m√©tadonn√©es de r√©f√©rence
    with rasterio.open(height_files[0]) as src:
        reference_transform = src.transform
        reference_crs = src.crs
        reference_resolution = src.res[0]  # R√©solution en m√®tres
    
    logger.info(f"Fichier de r√©f√©rence : {os.path.basename(height_files[0])}")
    logger.info(f"CRS utilis√© : {reference_crs}")
    logger.info(f"Transform de r√©f√©rence : {reference_transform}")
    logger.info(f"R√©solution de r√©f√©rence : {reference_resolution}m")
    
    # V√©rification que nous sommes bien dans un rep√®re local ENU
    if reference_crs and ('geocent' in str(reference_crs) or 'cart' in str(reference_crs)):
        logger.info("‚úÖ Rep√®re local ENU d√©tect√© (CRS cart√©sien local)")
    elif reference_crs and 'tmerc' in str(reference_crs):
        logger.info("‚úÖ Rep√®re local ENU d√©tect√© (Transverse Mercator local)")
    else:
        logger.warning("‚ö†Ô∏è CRS non standard d√©tect√© - v√©rifiez que c'est bien un rep√®re local ENU")
    
    # Utiliser la r√©solution cible si elle est fournie, sinon utiliser la r√©solution de r√©f√©rence
    if target_resolution is not None:
        final_resolution = target_resolution
        logger.info(f"R√©solution cible sp√©cifi√©e : {final_resolution}m")
        logger.info(f"R√©solution finale utilis√©e : {final_resolution}m")
    else:
        final_resolution = reference_resolution
        logger.info(f"R√©solution de r√©f√©rence utilis√©e : {final_resolution}m")
    
    # V√©rifier la coh√©rence des CRS entre tous les fichiers
    logger.info("V√©rification de la coh√©rence des CRS...")
    crs_issues = []
    for i, height_file in enumerate(height_files):
        with rasterio.open(height_file) as src:
            if src.crs != reference_crs:
                crs_issues.append(f"  - {os.path.basename(height_file)}: {src.crs} (attendu: {reference_crs})")
    
    if crs_issues:
        logger.warning("ATTENTION: Incoh√©rences de CRS d√©tect√©es:")
        for issue in crs_issues:
            logger.warning(issue)
        logger.warning("La fusion peut √©chouer ou donner des r√©sultats incorrects!")
    else:
        logger.info("‚úÖ Tous les fichiers ont le m√™me CRS")
    
    # Calculer l'√©tendue globale en lisant tous les fichiers
    global_bounds = None
    
    for height_file in height_files:
        with rasterio.open(height_file) as src:
            bounds = src.bounds
            if global_bounds is None:
                global_bounds = bounds
            else:
                global_bounds = rasterio.coords.BoundingBox(
                    min(global_bounds.left, bounds.left),
                    min(global_bounds.bottom, bounds.bottom),
                    max(global_bounds.right, bounds.right),
                    max(global_bounds.top, bounds.top)
                )
    
    logger.info(f"√âtendue globale : {global_bounds}")
    
    # Calculer les dimensions de la grille unifi√©e avec la r√©solution finale
    width = int((global_bounds.right - global_bounds.left) / final_resolution)
    height = int((global_bounds.top - global_bounds.bottom) / final_resolution)
    
    logger.info(f"Grille unifi√©e : {width} x {height} pixels")
    logger.info(f"Dimensions physiques : {float(global_bounds.right - global_bounds.left):.3f}m x {float(global_bounds.top - global_bounds.bottom):.3f}m")
    
    # Calcul du g√©or√©f√©rencement unifi√© avec la r√©solution finale
    transform = from_origin(global_bounds.left, global_bounds.top, final_resolution, final_resolution)
    
    # √âTAPE 2 : Fusion des rasters
    logger.info("Fusion des rasters...")
    
    # Initialisation des rasters unifi√©s
    unified_height = np.full((height, width), np.nan)
    unified_color = np.zeros((height, width, 3), dtype=np.uint8)
    unified_count = np.zeros((height, width), dtype=np.uint8)
    
    # Traitement de chaque fichier
    for i, height_file in enumerate(height_files):
        logger.info(f"Traitement de {os.path.basename(height_file)} ({i+1}/{len(height_files)})")
        
        try:
            # Lecture du raster de hauteur
            with rasterio.open(height_file) as src:
                height_data = src.read(1)
                height_transform = src.transform
                
                # Cr√©er un raster temporaire pour la transformation
                # CORRECTION: Initialiser avec une valeur nodata diff√©rente pour d√©tecter les conversions
                temp_height = np.full((height, width), -9999.0)  # Valeur nodata distinctive
                
                # Transformation de la hauteur
                logger.info(f"  Reprojection de {os.path.basename(height_file)}")
                logger.info(f"    Source: {height_data.shape}, transform: {height_transform}")
                logger.info(f"    Destination: {temp_height.shape}, transform: {transform}")
                
                # CORRECTION: Debug des donn√©es source avant reprojection
                source_valid = np.sum(~np.isnan(height_data))
                source_nan = np.sum(np.isnan(height_data))
                source_zero = np.sum(height_data == 0)
                source_negative = np.sum(height_data < 0)
                logger.info(f"    Source: {source_valid} pixels valides, {source_nan} pixels NaN")
                logger.info(f"    Source: {source_zero} pixels √† 0, {source_negative} pixels n√©gatifs")
                if source_valid > 0:
                    logger.info(f"    Source hauteur min/max: {np.nanmin(height_data):.3f}m / {np.nanmax(height_data):.3f}m")
                
                # CORRECTION: V√©rification des valeurs 0 dans la source
                if source_zero > 0:
                    logger.warning(f"    ‚ö†Ô∏è ATTENTION: {source_zero} pixels avec valeur 0 dans la source!")
                    logger.warning(f"    Ces valeurs 0 vont √™tre propag√©es dans la fusion")
                
                try:
                    # CORRECTION: Reprojection avec gestion explicite des nodata
                    # Utiliser src_nodata et dst_nodata pour pr√©server les zones sans donn√©es
                    
                    # D√©terminer la valeur nodata source
                    if np.any(np.isnan(height_data)):
                        src_nodata = np.nan
                    else:
                        # Si pas de NaN, chercher une valeur nodata appropri√©e
                        src_nodata = height_data.min() - 1.0
                    
                    reproject(
                        height_data,
                        temp_height,
                        src_transform=height_transform,
                        dst_transform=transform,
                        src_crs=reference_crs,
                        dst_crs=reference_crs,
                        src_nodata=src_nodata,
                        dst_nodata=-9999.0,  # Valeur nodata de destination
                        resampling=Resampling.nearest
                    )
                    logger.info(f"  ‚úÖ Reprojection r√©ussie avec gestion des nodata")
                    
                    # CORRECTION: Debug des donn√©es apr√®s reprojection
                    dest_valid = np.sum(temp_height != -9999.0)
                    dest_nan = np.sum(np.isnan(temp_height))
                    dest_zero = np.sum(temp_height == 0)
                    dest_negative = np.sum(temp_height < 0)
                    dest_nodata = np.sum(temp_height == -9999.0)
                    logger.info(f"    Apr√®s reprojection: {dest_valid} pixels valides, {dest_nan} pixels NaN")
                    logger.info(f"    Apr√®s reprojection: {dest_zero} pixels √† 0, {dest_negative} pixels n√©gatifs, {dest_nodata} pixels nodata")
                    if dest_valid > 0:
                        logger.info(f"    Dest hauteur min/max: {np.nanmin(temp_height[temp_height != -9999.0]):.3f}m / {np.nanmax(temp_height[temp_height != -9999.0]):.3f}m")
                    
                    # CORRECTION: V√©rification de la gestion des nodata
                    if dest_nodata > 0:
                        logger.info(f"    ‚úÖ Gestion des nodata r√©ussie: {dest_nodata} pixels nodata pr√©serv√©s")
                    else:
                        logger.warning(f"    ‚ö†Ô∏è ATTENTION: Aucun pixel nodata d√©tect√© apr√®s reprojection")
                    
                    # CORRECTION: V√©rification des valeurs 0 (maintenant suspectes)
                    if dest_zero > 0:
                        logger.warning(f"    ‚ö†Ô∏è ATTENTION: {dest_zero} pixels avec valeur 0 apr√®s reprojection!")
                        logger.warning(f"    Ces valeurs 0 peuvent √™tre des vraies hauteurs ou des erreurs")
                    
                    # CORRECTION: V√©rification de la perte de donn√©es
                    if source_valid > 0 and dest_valid == 0:
                        logger.error(f"  ‚ùå PROBL√àME: Toutes les donn√©es perdues lors de la reprojection!")
                        logger.error(f"    Source: {source_valid} pixels ‚Üí Destination: {dest_valid} pixels")
                        logger.error(f"    V√©rifiez l'alignement des transformations g√©om√©triques")
                    elif dest_valid < source_valid * 0.5:  # Perte de plus de 50%
                        logger.warning(f"  ‚ö†Ô∏è Perte importante de donn√©es: {source_valid} ‚Üí {dest_valid} pixels")
                except Exception as e:
                    logger.error(f"  ‚ùå Erreur de reprojection: {e}")
                    continue
                
                # Mise √† jour de la hauteur (prendre le maximum)
                # CORRECTION: Traiter TOUS les pixels, pas seulement les valides
                # Cela permet de combler les trous entre les orthoimages
                # CORRECTION: Adapter le masque pour les nouvelles valeurs nodata
                # Maintenant que la reprojection g√®re correctement les nodata
                # Exclure les pixels nodata (-9999.0) ET les pixels NaN
                valid_mask = (temp_height != -9999.0) & ~np.isnan(temp_height)
                valid_pixels_count = np.sum(valid_mask)
                logger.info(f"  Pixels valides dans ce raster: {valid_pixels_count}")
                
                # CORRECTION: Logique de fusion des hauteurs am√©lior√©e
                pixels_updated = 0
                pixels_new = 0
                pixels_max_updated = 0
                
                for r in range(height):
                    for c_idx in range(width):
                        temp_height_val = temp_height[r, c_idx]
                        
                        # CORRECTION: Ignorer les pixels nodata (-9999.0) pendant la fusion
                        if not np.isnan(temp_height_val) and temp_height_val != -9999.0:
                            # Pixel avec donn√©es valides (pas nodata)
                            if np.isnan(unified_height[r, c_idx]):
                                # Premier pixel valide √† cette position
                                unified_height[r, c_idx] = temp_height_val
                                unified_count[r, c_idx] = 1
                                pixels_new += 1
                            else:
                                # CORRECTION: Prendre le maximum des hauteurs, mais pr√©server les vraies valeurs n√©gatives
                                old_height = unified_height[r, c_idx]
                                
                                # V√©rifier si l'ancienne valeur est une vraie hauteur ou une valeur par d√©faut
                                # CORRECTION: D√©tecter les vraies valeurs par d√©faut (0 ou valeurs tr√®s proches de 0)
                                if (abs(old_height) < 0.001 or old_height == 0) and temp_height_val < 0:
                                    # Si l'ancienne valeur est proche de 0 (par d√©faut) et la nouvelle est n√©gative,
                                    # prendre la valeur n√©gative (c'est une vraie hauteur)
                                    unified_height[r, c_idx] = temp_height_val
                                    pixels_max_updated += 1
                                    logger.debug(f"    Pixel ({r}, {c_idx}): remplacement de {old_height:.3f}m par {temp_height_val:.3f}m (valeur n√©gative)")
                                else:
                                    # Sinon, prendre le maximum normal
                                    unified_height[r, c_idx] = max(unified_height[r, c_idx], temp_height_val)
                                    if unified_height[r, c_idx] > old_height:
                                        pixels_max_updated += 1
                                
                                unified_count[r, c_idx] += 1
                                pixels_updated += 1
                        # Si temp_height_val est NaN, on ne fait rien (pas de donn√©es √† cette position)
                
                # CORRECTION: Logs d√©plac√©s en dehors de la boucle des pixels
                logger.info(f"  Hauteur min/max de ce raster: {np.nanmin(temp_height):.3f}m / {np.nanmax(temp_height):.3f}m")
                logger.info(f"  Pixels mis √† jour: {pixels_updated} (dont {pixels_max_updated} avec hauteur max)")
                logger.info(f"  Nouveaux pixels: {pixels_new}")
                
                # CORRECTION: Debug des valeurs n√©gatives dans ce raster
                negative_pixels_in_raster = np.sum((temp_height < 0) & ~np.isnan(temp_height))
                if negative_pixels_in_raster > 0:
                    logger.info(f"  Pixels avec hauteurs n√©gatives dans ce raster: {negative_pixels_in_raster}")
                    logger.info(f"  Hauteur minimale n√©gative: {np.nanmin(temp_height[temp_height < 0]):.3f}m")
                else:
                    logger.info(f"  Aucune hauteur n√©gative dans ce raster")
                
                # CORRECTION: V√©rification de s√©curit√© des hauteurs
                if np.nanmax(temp_height) > 1000:  # Plus de 1km = suspect
                    logger.warning(f"  ‚ö†Ô∏è Hauteur maximale suspecte: {np.nanmax(temp_height):.3f}m")
                    logger.warning(f"  V√©rifiez les unit√©s et la g√©om√©trie du nuage de points")
                
                if np.nanmin(temp_height) < -1000:  # Moins de -1km = suspect
                    logger.warning(f"  ‚ö†Ô∏è Hauteur minimale suspecte: {np.nanmin(temp_height):.3f}m")
                    logger.warning(f"  V√©rifiez les unit√©s et la g√©om√©trie du nuage de points")
                
                if valid_pixels_count == 0:
                    logger.warning(f"  Aucun pixel valide dans le raster de hauteur")
                
                # CORRECTION: Debug du traitement des couleurs
                logger.info(f"  Traitement des couleurs avec le m√™me masque que les hauteurs")
                logger.info(f"  Masque de validit√©: {np.sum(valid_mask)} pixels valides")
                
                # CORRECTION: Debug de l'√©tat de unified_height apr√®s ce raster
                current_negative = np.sum((unified_height < 0) & ~np.isnan(unified_height))
                current_positive = np.sum((unified_height > 0) & ~np.isnan(unified_height))
                current_zero = np.sum((unified_height == 0) & ~np.isnan(unified_height))
                current_near_zero = np.sum((abs(unified_height) < 0.001) & ~np.isnan(unified_height))
                logger.info(f"  √âtat unified_height apr√®s ce raster:")
                logger.info(f"    Pixels n√©gatifs: {current_negative}, Pixels positifs: {current_positive}, Pixels √† z√©ro: {current_zero}, Pixels proches de z√©ro: {current_near_zero}")
                if current_negative > 0:
                    logger.info(f"    Plage hauteurs n√©gatives: {np.nanmin(unified_height[unified_height < 0]):.3f}m √† {np.nanmax(unified_height[unified_height < 0]):.3f}m")
                if current_zero > 0:
                    logger.warning(f"    ‚ö†Ô∏è ATTENTION: {current_zero} pixels avec valeur exactement 0 (suspect!)")
                    logger.warning(f"    V√©rifiez la source de ces valeurs 0")
                
                # Lecture du raster couleur correspondant
                color_file = height_file.replace('_height.tif', '_color.tif')
                if os.path.exists(color_file):
                    with rasterio.open(color_file) as src_color:
                        color_data = src_color.read([1, 2, 3])  # RGB
                        
                        # Transformation de la couleur
                        temp_color = np.zeros((height, width, 3), dtype=np.uint8)
                        for c in range(3):  # RGB
                            reproject(
                                color_data[c],
                                temp_color[:, :, c],
                                src_transform=height_transform,
                                dst_transform=transform,
                                src_crs=reference_crs,
                                dst_crs=reference_crs,
                                resampling=Resampling.average
                            )
                        
                        # Traitement des couleurs selon la m√©thode s√©lectionn√©e
                        if color_fusion_method == "median":
                            # M√©thode m√©diane : plus robuste aux valeurs aberrantes
                            logger.info(f"  Application de la m√©diane pour les couleurs")
                            
                            # Pour la m√©diane, on utilise une approche diff√©rente :
                            # On garde le pixel avec la valeur la plus proche de la moyenne locale
                            # Cela √©vite les valeurs aberrantes tout en pr√©servant les d√©tails
                            
                            for c in range(3):  # RGB
                                temp_color_float = temp_color[:, :, c].astype(np.float64)
                                unified_color_float = unified_color[:, :, c].astype(np.float64)
                                
                                for r in range(height):
                                    for c_idx in range(width):
                                        if valid_mask[r, c_idx]:
                                            if unified_count[r, c_idx] == 0:
                                                # Premier pixel √† cette position
                                                unified_color_float[r, c_idx] = temp_color_float[r, c_idx]
                                            else:
                                                # Comparer avec le pixel existant
                                                # On garde celui qui est le plus "repr√©sentatif"
                                                current_pixel = temp_color_float[r, c_idx]
                                                existing_pixel = unified_color_float[r, c_idx]
                                                
                                                # Calculer la "qualit√©" bas√©e sur la diff√©rence avec la moyenne
                                                # Plus la diff√©rence est faible, plus le pixel est repr√©sentatif
                                                local_mean = (current_pixel + existing_pixel) / 2.0
                                                current_quality = abs(current_pixel - local_mean)
                                                existing_quality = abs(existing_pixel - local_mean)
                                                
                                                if current_quality < existing_quality:
                                                    # Nouveau pixel a une meilleure qualit√©
                                                    unified_color_float[r, c_idx] = current_pixel
                                
                                # Conversion finale en uint8
                                unified_color[:, :, c] = np.clip(unified_color_float, 0, 255).astype(np.uint8)
                                
                                logger.debug(f"    Canal {['R', 'G', 'B'][c]}: m√©diane appliqu√©e")
                        
                        else:
                            # M√©thode moyenne (par d√©faut)
                            logger.info(f"  Application de la moyenne pond√©r√©e pour les couleurs")
                            
                            for c in range(3):  # RGB
                                # Utiliser le m√™me masque que pour les hauteurs (coh√©rence)
                                valid_color_mask = valid_mask  # Pas de condition sur couleur > 0
                                
                                if np.any(valid_color_mask):
                                    # Utiliser des calculs en float64 pour √©viter les overflows
                                    temp_color_float = temp_color[:, :, c].astype(np.float64)
                                    unified_color_float = unified_color[:, :, c].astype(np.float64)
                                    
                                    # Mise √† jour de la couleur avec moyenne pond√©r√©e
                                    for r in range(height):
                                        for c_idx in range(width):
                                            if valid_color_mask[r, c_idx]:
                                                if unified_count[r, c_idx] > 0:
                                                    # Moyenne pond√©r√©e par le nombre de contributions
                                                    unified_color_float[r, c_idx] = (
                                                        (unified_color_float[r, c_idx] * (unified_count[r, c_idx] - 1) + temp_color_float[r, c_idx]) / 
                                                        unified_count[r, c_idx]
                                                    )
                                                else:
                                                    unified_color_float[r, c_idx] = temp_color_float[r, c_idx]
                                    
                                    # Conversion finale en uint8 avec clipping
                                    unified_color[:, :, c] = np.clip(unified_color_float, 0, 255).astype(np.uint8)
                                    
                                    logger.debug(f"    Canal {['R', 'G', 'B'][c]}: {np.sum(valid_color_mask)} pixels trait√©s")
                                else:
                                    logger.warning(f"    Canal {['R', 'G', 'B'][c]}: Aucun pixel valide √† traiter")
                
        except Exception as e:
            logger.warning(f"Erreur lors du traitement de {os.path.basename(height_file)} : {e}")
            continue
    
    # Pas de normalisation finale n√©cessaire car la couleur est d√©j√† correctement moyenn√©e
    # Le unified_color contient d√©j√† les valeurs finales
    
    # CORRECTION: Pas d'interpolation - on garde les NaN pour les pixels sans donn√©es
    # L'objectif est de conserver la valeur la plus haute de chaque pixel
    holes_count = np.sum(np.isnan(unified_height))
    total_pixels = height * width
    logger.info(f"Statistiques finales:")
    logger.info(f"  Pixels totaux: {total_pixels}")
    logger.info(f"  Pixels avec donn√©es: {total_pixels - holes_count}")
    logger.info(f"  Pixels sans donn√©es (NaN): {holes_count}")
    logger.info(f"  Couverture: {((total_pixels - holes_count) / total_pixels * 100):.1f}%")
    
    # Debug: afficher des informations sur les donn√©es unifi√©es
    valid_pixels = np.sum(~np.isnan(unified_height))
    logger.info(f"Fusion termin√©e. Pixels valides : {valid_pixels}")
    
    if valid_pixels == 0:
        logger.warning("ATTENTION: Aucun pixel valide trouv√© dans l'orthoimage unifi√©e!")
        logger.warning("V√©rifiez que les fichiers d'entr√©e contiennent des donn√©es valides")
        logger.warning(f"Nombre de fichiers trait√©s: {len(height_files)}")
        logger.warning(f"Dimensions de la grille unifi√©e: {width} x {height}")
        logger.warning(f"√âtendue globale: {global_bounds}")
    else:
        logger.info(f"Hauteur min/max: {np.nanmin(unified_height):.3f}m / {np.nanmax(unified_height):.3f}m")
        logger.info(f"Couleur min/max (R): {np.min(unified_color[:,:,0])} / {np.max(unified_color[:,:,0])}")
        logger.info(f"Couleur min/max (G): {np.min(unified_color[:,:,1])} / {np.max(unified_color[:,:,1])}")
        logger.info(f"Couleur min/max (B): {np.min(unified_color[:,:,2])} / {np.max(unified_color[:,:,2])}")
        
        # CORRECTION: Affichage des statistiques de fusion
        logger.info(f"Statistiques de fusion:")
        logger.info(f"  Pixels avec 1 contribution: {np.sum(unified_count == 1)}")
        logger.info(f"  Pixels avec 2+ contributions: {np.sum(unified_count > 1)}")
        logger.info(f"  Hauteur moyenne des pixels multiples: {np.nanmean(unified_height[unified_count > 1]):.3f}m")
        
        # CORRECTION: Debug des valeurs n√©gatives
        negative_pixels = np.sum((unified_height < 0) & ~np.isnan(unified_height))
        if negative_pixels > 0:
            logger.info(f"  Pixels avec hauteurs n√©gatives: {negative_pixels}")
            logger.info(f"  Hauteur minimale (avec n√©gatifs): {np.nanmin(unified_height):.3f}m")
            logger.info(f"  Hauteur maximale: {np.nanmax(unified_height):.3f}m")
        else:
            logger.info(f"  Aucune hauteur n√©gative d√©tect√©e")
        
        # CORRECTION: Statistiques de couverture pour identifier les trous
        total_pixels = height * width
        coverage_percentage = (valid_pixels / total_pixels) * 100
        logger.info(f"Statistiques de couverture:")
        logger.info(f"  Pixels totaux: {total_pixels}")
        logger.info(f"  Pixels couverts: {valid_pixels}")
        logger.info(f"  Couverture: {coverage_percentage:.1f}%")
        
        if coverage_percentage < 80:
            logger.warning(f"‚ö†Ô∏è Couverture faible ({coverage_percentage:.1f}%) - V√©rifiez l'alignement des orthoimages")
        elif coverage_percentage < 95:
            logger.info(f"‚ÑπÔ∏è Couverture correcte ({coverage_percentage:.1f}%) - Quelques trous √† combler")
        else:
            logger.info(f"‚úÖ Couverture excellente ({coverage_percentage:.1f}%)")
    
    # √âTAPE 3 : Sauvegarde des r√©sultats unifi√©s
    logger.info("Sauvegarde des r√©sultats unifi√©s...")
    
    # M√©tadonn√©es
    metadata = {
        'Software': 'PhotoGeoAlign Unified Orthoimage/DTM Merger',
        'Resolution': f'{reference_resolution}m per pixel',
        'Origin_X': f'{global_bounds.left:.6f}',
        'Origin_Y': f'{global_bounds.top:.6f}',
        'Extent_X': f'{float(global_bounds.right - global_bounds.left):.3f}m',
'Extent_Y': f'{float(global_bounds.top - global_bounds.bottom):.3f}m',
        'Source_Files': str(len(height_files)),
        'Valid_Pixels': str(int(np.sum(~np.isnan(unified_height)))),
        'Height_Range': f'{np.nanmin(unified_height):.3f}m to {np.nanmax(unified_height):.3f}m' if valid_pixels > 0 else 'No valid pixels'
    }
    
    # Sauvegarde de l'orthoimage unifi√©e
    orthoimage_filename = "unified_orthoimage.tif"
    orthoimage_path = os.path.join(output_dir, orthoimage_filename)
    
    logger.info(f"Sauvegarde de l'orthoimage unifi√©e avec le CRS : {reference_crs}")
    
    with rasterio.open(
        orthoimage_path,
        'w',
        driver='GTiff',
        height=height,
        width=width,
        count=3,
        dtype=unified_color.dtype,
        crs=reference_crs,  # Utiliser le CRS du fichier de r√©f√©rence
        transform=transform,
        photometric='rgb'
    ) as dst:
        dst.write(unified_color[:,:,0], 1)  # Rouge
        dst.write(unified_color[:,:,1], 2)  # Vert
        dst.write(unified_color[:,:,2], 3)  # Bleu
        dst.update_tags(**metadata)
    
    # Sauvegarde du MNT unifi√©
    dtm_filename = "unified_dtm.tif"
    dtm_path = os.path.join(output_dir, dtm_filename)
    
    # CORRECTION: Pr√©server les valeurs n√©gatives et utiliser une valeur nodata appropri√©e
    dtm_data = unified_height.copy()
    
    # D√©terminer une valeur nodata appropri√©e (en dehors de la plage des hauteurs)
    height_min = np.nanmin(dtm_data)
    height_max = np.nanmax(dtm_data)
    height_range = height_max - height_min
    
    # Utiliser une valeur nodata en dehors de la plage des hauteurs
    if height_min < 0:
        # Si on a des hauteurs n√©gatives, utiliser une valeur tr√®s n√©gative
        nodata_value = height_min - height_range - 1.0
        logger.info(f"  Hauteurs n√©gatives d√©tect√©es: min={height_min:.3f}m, nodata={nodata_value:.3f}m")
    else:
        # Si toutes les hauteurs sont positives, utiliser -9999
        nodata_value = -9999.0
        logger.info(f"  Toutes hauteurs positives: min={height_min:.3f}m, nodata={nodata_value:.3f}m")
    
    # Remplacer les NaN par la valeur nodata temporairement pour la fusion
    dtm_data = np.where(np.isnan(dtm_data), nodata_value, dtm_data)
    
    # CORRECTION: Remplacer les -9999.0 par np.nan juste avant la sauvegarde
    # Pour avoir le m√™me comportement que les MNT unitaires
    # Utiliser une comparaison plus robuste pour les floats
    dtm_data = np.where(np.isclose(dtm_data, nodata_value, atol=0.1), np.nan, dtm_data)
    
    # Debug: v√©rifier que la conversion a fonctionn√©
    remaining_nodata = np.sum(np.isclose(dtm_data, nodata_value, atol=0.1))
    if remaining_nodata > 0:
        logger.warning(f"  ‚ö†Ô∏è ATTENTION: {remaining_nodata} pixels nodata n'ont pas √©t√© convertis!")
    else:
        logger.info(f"  ‚úÖ Conversion nodata ‚Üí NaN r√©ussie")
    
    # Debug d√©taill√© des valeurs
    unique_values = np.unique(dtm_data)
    logger.info(f"  Valeurs uniques dans dtm_data apr√®s conversion: {unique_values}")
    
    # V√©rifier s'il y a encore des valeurs proches de -9999
    near_9999 = np.sum(np.abs(dtm_data + 9999) < 1.0)
    if near_9999 > 0:
        logger.warning(f"  ‚ö†Ô∏è ATTENTION: {near_9999} pixels avec des valeurs proches de -9999!")
    
    # V√©rifier le type de donn√©es et le convertir si n√©cessaire
    if dtm_data.dtype != np.float32 and dtm_data.dtype != np.float64:
        dtm_data = dtm_data.astype(np.float32)
        logger.info(f"  Type de donn√©es converti en float32 pour pr√©server les valeurs n√©gatives")
    
    # Statistiques finales avec np.nan
    valid_pixels = np.sum(~np.isnan(dtm_data))
    nodata_pixels = np.sum(np.isnan(dtm_data))
    logger.info(f"  MNT final: {valid_pixels} pixels valides, {nodata_pixels} pixels nodata (NaN)")
    if valid_pixels > 0:
        logger.info(f"  Plage hauteurs: {np.nanmin(dtm_data):.3f}m √† {np.nanmax(dtm_data):.3f}m")
    
    with rasterio.open(
        dtm_path,
        'w',
        driver='GTiff',
        height=height,
        width=width,
        count=1,
        dtype=dtm_data.dtype,
        crs=reference_crs,
        transform=transform,
        nodata=np.nan  # Utiliser NaN comme les MNT unitaires
    ) as dst:
        dst.write(dtm_data, 1)
        dst.update_tags(**metadata)
    
    logger.info(f"Orthoimage unifi√©e sauvegard√©e : {orthoimage_filename}")
    logger.info(f"MNT unifi√© sauvegard√© : {dtm_filename}")
    logger.info(f"Fusion des orthoimages et MNT termin√©e dans {output_dir}")

def process_zone_with_orthos(zone_data):
    """
    Fonction globale ex√©cut√©e par chaque processus pour traiter une zone
    UNIQUEMENT avec l'√©galisation par m√©diane superpos√©e
    """
    import os
    import numpy as np
    import rasterio
    from rasterio import warp, transform, enums
    
    zone_id = zone_data['zone_id']
    assigned_files = zone_data['assigned_orthos']
    
    # S√âPARER LES ORTHOS ET LES MNT
    ortho_color_files = [f for f in assigned_files if f.endswith('_color.tif')]
    ortho_height_files = [f for f in assigned_files if f.endswith('_height.tif')]
    
    print(f"üîÑ PROCESSUS {os.getpid()}: Traitement Zone {zone_id}")
    print(f"   üé® Orthos couleur ({len(ortho_color_files)}): {ortho_color_files}")
    print(f"   üìè MNT hauteur ({len(ortho_height_files)}): {ortho_height_files}")
    
    # TRAITEMENT R√âEL DES ORTHOS ET MNT
    zone_bounds = zone_data['bounds']
    zone_size_meters = zone_data['zone_size_meters']
    final_resolution = zone_data['final_resolution']
    
    # Calculer les dimensions de la zone en pixels
    zone_width_px = max(1, round(zone_size_meters / final_resolution))
    zone_height_px = max(1, round(zone_size_meters / final_resolution))
    
    results = {}
    
    # 1. FUSION DES ORTHOS COULEUR (UNIQUEMENT √âGALISATION PAR M√âDIANE SUPERPOS√âE)
    if ortho_color_files:
        print(f"   üé® Fusion de {len(ortho_color_files)} orthos couleur...")
        try:
            # Lire et reprojeter chaque ortho couleur
            ortho_arrays = []
            nodata_values = []
            for ortho_file in ortho_color_files:
                full_path = os.path.join(zone_data['input_dir'], ortho_file)
                with rasterio.open(full_path) as src:
                    # LECTURE DES NODATA
                    nodata_value = src.nodata
                    nodata_values.append(nodata_value)
                    
                    # Reprojection vers la zone (SANS interpolation pour √©viter les contours sombres)
                    ortho_reprojected, _ = warp.reproject(
                        source=rasterio.band(src, [1, 2, 3]),  # 3 bandes RGB
                        destination=np.zeros((3, zone_height_px, zone_width_px), dtype=np.float32),
                        src_transform=src.transform,
                        src_crs=src.crs,
                        dst_transform=transform.from_bounds(*zone_bounds, zone_width_px, zone_height_px),
                        dst_crs=src.crs,
                        resampling=enums.Resampling.nearest,  # ‚úÖ PLUS PROCHE VOISIN (pas d'interpolation)
                        nodata=nodata_value  # Pr√©server la valeur nodata originale
                    )
                    
                    ortho_arrays.append(ortho_reprojected)
                    print(f"         ‚úÖ {ortho_file}: reprojet√© avec nodata g√©r√©")
            
            # √âGALISATION UNIQUEMENT PAR M√âDIANE SUPERPOS√âE
            if ortho_arrays:
                print(f"         üé® √âgalisation par m√©diane superpos√©e...")
                
                # 1. CR√âER UN MASQUE COMMUN DES PIXELS VALIDES
                print(f"            üîç Cr√©ation du masque commun des pixels valides...")
                common_valid_mask = np.zeros((zone_height_px, zone_width_px), dtype=bool)
                
                for ortho_array in ortho_arrays:
                    # Un pixel est valide s'il n'est pas (0,0,0) dans au moins une bande
                    ortho_valid = ~np.all(ortho_array == 0.0, axis=0)
                    common_valid_mask |= ortho_valid
                
                valid_pixels_count = np.sum(common_valid_mask)
                print(f"            üìä Masque commun : {valid_pixels_count}/{zone_height_px * zone_width_px} pixels valides")
                
                if valid_pixels_count > 0:
                    # 2. HARMONISATION DIRECTE DES COULEURS PAR ANALYSE DES DIFF√âRENCES
                    print(f"            üé® Harmonisation directe des couleurs par analyse des diff√©rences...")
                    
                    if len(ortho_arrays) > 1:
                        # 1. S√âLECTION DE L'ORTHO DE R√âF√âRENCE (plus de pixels valides)
                        ortho_pixel_counts = []
                        for i, ortho in enumerate(ortho_arrays):
                            valid_pixels = np.sum(np.any(ortho > 0, axis=0))
                            ortho_pixel_counts.append(valid_pixels)
                            print(f"               üìä Ortho {i+1} - Pixels valides: {valid_pixels}")
                        
                        ref_index = np.argmax(ortho_pixel_counts)
                        print(f"            üèÜ Ortho de r√©f√©rence: Ortho {ref_index + 1} ({ortho_pixel_counts[ref_index]} pixels valides)")
                        
                        # 2. CALCUL DES DIFF√âRENCES ET FACTEURS CORRECTEURS
                        harmonized_orthos = []
                        
                        for i, ortho in enumerate(ortho_arrays):
                            if i == ref_index:
                                # L'ortho de r√©f√©rence reste inchang√©e
                                harmonized_orthos.append(np.copy(ortho))
                                print(f"               ‚úÖ Ortho {i+1} (r√©f√©rence) - Aucune modification")
                            else:
                                # Calculer les diff√©rences avec l'ortho de r√©f√©rence
                                print(f"               üîÑ Harmonisation Ortho {i+1} vers la r√©f√©rence...")
                                
                                # Cr√©er le masque des pixels de chevauchement
                                ref_valid = np.any(ortho_arrays[ref_index] > 0, axis=0)
                                ortho_valid = np.any(ortho > 0, axis=0)
                                overlap_mask = ref_valid & ortho_valid
                                
                                if np.sum(overlap_mask) > 0:
                                    # Calculer les diff√©rences par bande (avec signe)
                                    diff_R = ortho_arrays[ref_index][0, overlap_mask] - ortho[0, overlap_mask]
                                    diff_G = ortho_arrays[ref_index][1, overlap_mask] - ortho[1, overlap_mask]
                                    diff_B = ortho_arrays[ref_index][2, overlap_mask] - ortho[2, overlap_mask]
                                    
                                    # üÜï HARMONISATION MULTI-QUANTILES : Q25, Q50 (m√©diane), Q75
                                    print(f"                  üìä Analyse multi-quantiles des diff√©rences...")
                                    
                                    # Calculer les quantiles des diff√©rences pour chaque bande
                                    diff_Q25_R = np.percentile(diff_R, 25)
                                    diff_Q50_R = np.percentile(diff_R, 50)  # M√©diane
                                    diff_Q75_R = np.percentile(diff_R, 75)
                                    
                                    diff_Q25_G = np.percentile(diff_G, 25)
                                    diff_Q50_G = np.percentile(diff_G, 50)  # M√©diane
                                    diff_Q75_G = np.percentile(diff_G, 75)
                                    
                                    diff_Q25_B = np.percentile(diff_B, 25)
                                    diff_Q50_B = np.percentile(diff_B, 50)  # M√©diane
                                    diff_Q75_B = np.percentile(diff_B, 75)
                                    
                                    print(f"                  üìà Bande R - Q25={diff_Q25_R:.1f}, Q50={diff_Q50_R:.1f}, Q75={diff_Q75_R:.1f}")
                                    print(f"                  üìà Bande G - Q25={diff_Q25_G:.1f}, Q50={diff_Q50_G:.1f}, Q75={diff_Q75_G:.1f}")
                                    print(f"                  üìà Bande B - Q25={diff_Q25_B:.1f}, Q50={diff_Q50_B:.1f}, Q75={diff_Q75_B:.1f}")
                                    
                                    # Calculer les facteurs correcteurs multi-quantiles
                                    # Facteur = 1 + (Q50 / ref_mean) + (Q75 - Q25) / (2 * ref_mean)
                                    # Q50 : correction centrale, (Q75-Q25) : correction de la dispersion
                                    ref_mean_R = np.mean(ortho_arrays[ref_index][0, overlap_mask])
                                    ref_mean_G = np.mean(ortho_arrays[ref_index][1, overlap_mask])
                                    ref_mean_B = np.mean(ortho_arrays[ref_index][2, overlap_mask])
                                    
                                    if ref_mean_R != 0:
                                        # Correction centrale + correction de dispersion
                                        facteur_R = 1 + (diff_Q50_R / ref_mean_R) + (diff_Q75_R - diff_Q25_R) / (2 * ref_mean_R)
                                    else:
                                        facteur_R = 1.0
                                        
                                    if ref_mean_G != 0:
                                        facteur_G = 1 + (diff_Q50_G / ref_mean_G) + (diff_Q75_G - diff_Q25_G) / (2 * ref_mean_G)
                                    else:
                                        facteur_G = 1.0
                                        
                                    if ref_mean_B != 0:
                                        facteur_B = 1 + (diff_Q50_B / ref_mean_B) + (diff_Q75_B - diff_Q25_B) / (2 * ref_mean_B)
                                    else:
                                        facteur_B = 1.0
                                    
                                    print(f"                  üéØ Facteurs multi-quantiles: R={facteur_R:.3f}, G={facteur_G:.3f}, B={facteur_B:.3f}")
                                    
                                    # Appliquer les facteurs correcteurs √† toute l'ortho
                                    harmonized_ortho = np.copy(ortho)
                                    harmonized_ortho[0] *= facteur_R  # Bande R
                                    harmonized_ortho[1] *= facteur_G  # Bande G
                                    harmonized_ortho[2] *= facteur_B  # Bande B
                                    
                                    # Clipper pour √©viter les valeurs > 255
                                    harmonized_ortho = np.clip(harmonized_ortho, 0, 255)
                                    
                                    harmonized_orthos.append(harmonized_ortho)
                                    print(f"               ‚úÖ Ortho {i+1} harmonis√©e")
                                else:
                                    # Pas de chevauchement, pas d'harmonisation possible
                                    harmonized_orthos.append(np.copy(ortho))
                                    print(f"               ‚ö†Ô∏è Ortho {i+1} - Pas de chevauchement avec la r√©f√©rence")
                        
                        # Remplacer les orthos par les versions harmonis√©es
                        equalized_orthos = harmonized_orthos
                        print(f"            ‚úÖ Harmonisation des couleurs termin√©e")
                    else:
                        # Une seule ortho, pas d'harmonisation possible
                        equalized_orthos = ortho_arrays
                        print(f"            ‚ö†Ô∏è Pas d'harmonisation possible - une seule ortho")
                    
                    # 3. FUSION PAR MOYENNE DES ORTHOS HARMONIS√âES
                    print(f"            üé® Fusion par moyenne des orthos harmonis√©es...")
                    fused_color = np.zeros((3, zone_height_px, zone_width_px), dtype=np.float32)
                    
                    # Pour chaque pixel, calculer la moyenne des orthos harmonis√©es
                    for y in range(zone_height_px):
                        for x in range(zone_width_px):
                            for band in range(3):  # RGB
                                # Collecter les valeurs valides pour ce pixel et cette bande
                                valid_values = []
                                
                                for equalized_ortho in equalized_orthos:
                                    pixel_value = equalized_ortho[band, y, x]
                                    # Un pixel est valide s'il n'est pas 0 (apr√®s harmonisation)
                                    if pixel_value > 0:
                                        valid_values.append(pixel_value)
                                
                                # Si on a des valeurs valides, calculer la moyenne
                                if valid_values:
                                    fused_color[band, y, x] = np.mean(valid_values)
                                # Sinon, le pixel reste √† 0 (pas de donn√©es)
                    
                    # Clip et convertir en uint8
                    fused_color = np.clip(fused_color, 0, 255).astype(np.uint8)
                    
                    # Sauvegarder l'ortho fusionn√©e finale
                    color_filename = f"zone_{zone_id}_fused_color_median_harmonized.tif"
                    color_path = os.path.join(zone_data['output_dir'], color_filename)
                    
                    with rasterio.open(
                        color_path, 'w',
                        driver='GTiff',
                        height=zone_height_px,
                        width=zone_width_px,
                        count=3,
                        dtype=np.uint8,
                        crs=src.crs,
                        transform=transform.from_bounds(*zone_bounds, zone_width_px, zone_height_px),
                        nodata=None
                    ) as dst:
                        dst.write(fused_color, [1, 2, 3])
                    
                    print(f"            ‚úÖ Ortho fusionn√©e finale sauvegard√©e : {color_filename}")
                    results['color_fused_final'] = color_path
                    
                else:
                    print(f"            ‚ö†Ô∏è Aucun pixel valide trouv√©, pas de fusion possible")
                
        except Exception as e:
            print(f"   ‚ùå Erreur fusion orthos couleur : {e}")
            results['color_error'] = str(e)
    
    # 2. FUSION DES MNT HAUTEUR (MAXIMUM)
    if ortho_height_files:
        print(f"   üìè Fusion de {len(ortho_height_files)} MNT hauteur...")
        try:
            # Lire et reprojeter chaque MNT hauteur
            height_arrays = []
            nodata_values = []
            
            for height_file in ortho_height_files:
                full_path = os.path.join(zone_data['input_dir'], height_file)
                with rasterio.open(full_path) as src:
                    # LECTURE DES NODATA
                    nodata_value = src.nodata
                    nodata_values.append(nodata_value)
                    
                    # REPROJECTION COH√âRENTE VERS LA ZONE (COMME POUR LES ORTHOS COULEUR)
                    dst_transform = transform.from_bounds(*zone_bounds, zone_width_px, zone_height_px)
                    
                    # Reprojection avec plus proche voisin pour pr√©server les valeurs exactes
                    height_reprojected, _ = warp.reproject(
                        source=rasterio.band(src, 1),  # Bande 1 pour la hauteur
                        destination=np.zeros((zone_height_px, zone_width_px), dtype=np.float32),
                        src_transform=src.transform,
                        src_crs=src.crs,
                        dst_transform=dst_transform,
                        dst_crs=src.crs,
                        resampling=enums.Resampling.nearest,  # Plus proche voisin pour pr√©server les valeurs exactes
                        nodata=nodata_value  # Pr√©server la valeur nodata originale
                    )
                    
                    # POST-TRAITEMENT : Remplacer les pixels hors limites par NaN
                    if nodata_value is not None:
                        valid_mask = height_reprojected != nodata_value
                        height_reprojected[(height_reprojected == 0.0) & valid_mask] = np.nan
                    else:
                        height_reprojected[height_reprojected == 0.0] = np.nan
                    
                    height_arrays.append(height_reprojected)
                    print(f"         ‚úÖ {height_file}: reprojet√© vers {height_reprojected.shape} avec transform coh√©rent et nodata g√©r√©")
            
            # Fusion par maximum des hauteurs (IGNORANT LES VALEURS NODATA)
            if height_arrays:
                # Initialiser avec NaN (pas de valeur initiale)
                fused_height = np.full((zone_height_px, zone_width_px), np.nan, dtype=np.float32)
                
                # Pour chaque pixel de la zone
                for y in range(zone_height_px):
                    for x in range(zone_width_px):
                        # Collecter toutes les valeurs valides pour ce pixel
                        valid_values = []
                        
                        for height_array in height_arrays:
                            pixel_value = height_array[y, x]
                            
                            # V√©rifier si la valeur est valide (pas nodata, pas NaN, pas hors limites)
                            if not np.isnan(pixel_value):  # Ignorer les NaN (pixels hors limites)
                                valid_values.append(pixel_value)
                        
                        # Si on a des valeurs valides, prendre le maximum
                        if valid_values:
                            fused_height[y, x] = max(valid_values)
                        # Sinon, le pixel reste NaN (pas de donn√©es)
                
                # Compter les pixels avec des valeurs valides
                valid_pixels = ~np.isnan(fused_height)
                print(f"         üéØ Fusion termin√©e : {np.sum(valid_pixels)}/{fused_height.size} pixels avec valeurs valides")
                
                # Sauvegarder le MNT fusionn√©
                height_filename = f"zone_{zone_id}_fused_height.tif"
                height_path = os.path.join(zone_data['output_dir'], height_filename)
                
                with rasterio.open(
                    height_path, 'w',
                    driver='GTiff',
                    height=zone_height_px,
                    width=zone_width_px,
                    count=1,
                    dtype=np.float32,
                    crs=src.crs,
                    transform=transform.from_bounds(*zone_bounds, zone_width_px, zone_height_px),
                    nodata=None
                ) as dst:
                    dst.write(fused_height, 1)
                
                print(f"   ‚úÖ MNT hauteur fusionn√© sauvegard√© : {height_filename}")
                results['height_fused'] = height_path
                
        except Exception as e:
            print(f"   ‚ùå Erreur fusion MNT hauteur : {e}")
            results['height_error'] = str(e)
    
    return {
        'zone_id': zone_id,
        'status': 'success',
        'orthos_color_processed': len(ortho_color_files),
        'orthos_height_processed': len(ortho_height_files),
        'message': f"Zone {zone_id}: {len(ortho_color_files)} orthos couleur + {len(ortho_height_files)} MNT hauteur"
    }

def unified_ortho_mnt_fusion(input_dir, logger, output_dir, final_resolution=None, zone_size_meters=5.0, max_workers=None):
    """
    üéØ FUSION FINALE : Assemblage des orthoimages et MNT unifi√©s
    Objectif : Fusionner les orthoimages unitaires en une orthoimage finale unifi√©e
    
    Args:
        input_dir: R√©pertoire d'entr√©e contenant les orthoimages unitaires
        logger: Logger pour les messages
        output_dir: R√©pertoire de sortie
        final_resolution: R√©solution finale en m√®tres (d√©faut: 0.003m = 3mm)
        zone_size_meters: Taille de chaque zone en m√®tres (d√©faut: 5.0m)
        max_workers: Nombre maximum de processus parall√®les
    """
    import os
    import numpy as np
    import rasterio
    from rasterio.coords import BoundingBox
    from rasterio.transform import from_origin
    from multiprocessing import Pool
    
    logger.info("üéØ FUSION FINALE : Assemblage des orthoimages et MNT unifi√©s")
    
    # Cr√©er le dossier de sortie
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # √âTAPE 1.1 : Analyser les orthoimages unitaires pour d√©terminer l'√©tendue globale
    logger.info("Analyse des orthoimages unitaires...")
    
    # Chercher tous les fichiers .tif dans le dossier d'entr√©e
    ortho_files = []
    for file in os.listdir(input_dir):
        if file.lower().endswith('.tif') or file.lower().endswith('.tiff'):
            ortho_files.append(os.path.join(input_dir, file))
    
    if not ortho_files:
        logger.warning("‚ö†Ô∏è Aucune orthoimage .tif trouv√©e, impossible de continuer sans donn√©es")
        raise ValueError("Aucune orthoimage .tif trouv√©e dans le dossier d'entr√©e")
    else:
        logger.info(f"Trouv√© {len(ortho_files)} orthoimage(s) unitaire(s)")
        
        # Analyser la premi√®re orthoimage pour obtenir la r√©solution et les dimensions
        first_ortho = ortho_files[0]
        logger.info(f"Analyse de l'orthoimage de r√©f√©rence : {os.path.basename(first_ortho)}")
        
        try:
            with rasterio.open(first_ortho) as src:
                bounds = src.bounds
                transform = src.transform
                width = src.width
                height = src.height
                crs = src.crs
                
                logger.info(f"  CRS : {crs}")
                logger.info(f"  Dimensions : {width} √ó {height} pixels")
                logger.info(f"  Transform : {transform}")
                logger.info(f"  Bounds : {bounds}")
                
                # Calculer la r√©solution r√©elle
                pixel_size_x = abs(transform.a)
                pixel_size_y = abs(transform.e)
                logger.info(f"  R√©solution pixel : {pixel_size_x:.6f}m √ó {pixel_size_y:.6f}m")
                
                # Si la r√©solution finale n'est pas sp√©cifi√©e, utiliser celle de l'ortho
                if final_resolution is None:
                    final_resolution = min(pixel_size_x, pixel_size_y)
                    logger.info(f"  R√©solution finale automatique : {final_resolution:.6f}m")
                
                # S'assurer que la r√©solution finale est valide
                if final_resolution <= 0:
                    logger.warning(f"  ‚ö†Ô∏è R√©solution invalide d√©tect√©e : {final_resolution}, utilisation de 0.1m")
                    final_resolution = 0.1
                
                # Calculer l'√©tendue globale en analysant toutes les orthos
                all_bounds = [bounds]
                for ortho_file in ortho_files[1:]:
                    try:
                        with rasterio.open(ortho_file) as src2:
                            all_bounds.append(src2.bounds)
                    except Exception as e:
                        logger.warning(f"  ‚ö†Ô∏è Impossible de lire {os.path.basename(ortho_file)} : {e}")
                
                # Calculer l'√©tendue globale
                global_left = min(b.left for b in all_bounds)
                global_bottom = min(b.bottom for b in all_bounds)
                global_right = max(b.right for b in all_bounds)
                global_top = max(b.top for b in all_bounds)
                
                # V√©rifier que les bounds sont valides
                if global_left >= global_right or global_bottom >= global_top:
                    logger.error(f"‚ùå Bounds invalides d√©tect√©s : left={global_left}, right={global_right}, bottom={global_bottom}, top={global_top}")
                    raise ValueError("Bounds invalides - l'√©tendue des orthoimages est incorrecte")
                
                logger.info(f"  √âtendue des orthos : {float(global_left):.2f}m √† {float(global_right):.2f}m (X), {float(global_bottom):.2f}m √† {float(global_top):.2f}m (Y)")
                
                # Si la taille de grille n'est pas sp√©cifi√©e, l'utiliser pour contraindre
                # üéØ GRID AUTOMATIQUE : Utiliser l'√©tendue r√©elle des orthos
                global_bounds = BoundingBox(
                    left=global_left, bottom=global_bottom,
                    right=global_right, top=global_top
                )
                
                logger.info(f"√âtendue globale calcul√©e : {global_bounds}")
                logger.info(f"  Largeur : {float(global_bounds.right - global_bounds.left):.2f}m")
                logger.info(f"  Hauteur : {float(global_bounds.top - global_bounds.bottom):.2f}m")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse de l'orthoimage : {e}")
            raise RuntimeError(f"Impossible d'analyser les orthoimages : {e}")
    
    # S'assurer que la r√©solution finale est valide
    if final_resolution is None or final_resolution <= 0:
        logger.warning(f"‚ö†Ô∏è R√©solution finale invalide : {final_resolution}, utilisation de 0.1m")
        final_resolution = 0.1
    
    # S'assurer que la taille des zones est valide
    if zone_size_meters <= 0:
        logger.warning(f"‚ö†Ô∏è Taille des zones invalide : {zone_size_meters}, utilisation de 5.0m")
        zone_size_meters = 5.0
    
    logger.info(f"R√©solution finale : {final_resolution}m")
    logger.info(f"Taille des zones : {zone_size_meters}m √ó {zone_size_meters}m")
    
    # üîß CORRECTION CRITIQUE : Adapter la taille des zones √† la r√©solution exacte des pixels
    # Calculer combien de pixels correspondent exactement √† la taille demand√©e
    pixels_per_zone = zone_size_meters / final_resolution
    logger.info(f"üîç Calcul pixels par zone : {zone_size_meters}m √∑ {final_resolution}m = {pixels_per_zone:.6f} pixels")
    
    # Adapter la taille des zones pour qu'elles correspondent √† un nombre entier de pixels
    # OPTION 1 : Troncature (zones l√©g√®rement plus petites)
    zone_size_adjusted_down = int(pixels_per_zone) * final_resolution
    # OPTION 2 : Arrondi (zones l√©g√®rement plus grandes)
    zone_size_adjusted_up = round(pixels_per_zone) * final_resolution
    
    logger.info(f"üîç Option 1 (troncature) : {int(pixels_per_zone)} pixels √ó {final_resolution}m = {zone_size_adjusted_down:.6f}m")
    logger.info(f"üîç Option 2 (arrondi) : {round(pixels_per_zone)} pixels √ó {final_resolution}m = {zone_size_adjusted_up:.6f}m")
    
    # Choisir l'option qui minimise le d√©calage avec la taille demand√©e
    if abs(zone_size_adjusted_down - zone_size_meters) <= abs(zone_size_adjusted_up - zone_size_meters):
        zone_size = zone_size_adjusted_down
        logger.info(f"‚úÖ Choix : Option 1 (troncature) - Taille finale des zones : {zone_size:.6f}m √ó {zone_size:.6f}m")
        logger.info(f"   ‚Üí D√©calage : {zone_size - zone_size_meters:.6f}m ({((zone_size - zone_size_meters) * 1000):.3f}mm)")
    else:
        zone_size = zone_size_adjusted_up
        logger.info(f"‚úÖ Choix : Option 2 (arrondi) - Taille finale des zones : {zone_size:.6f}m √ó {zone_size:.6f}m")
        logger.info(f"   ‚Üí D√©calage : {zone_size - zone_size_meters:.6f}m ({((zone_size - zone_size_meters) * 1000):.3f}mm)")
    
    # √âTAPE 1.2 : Cr√©er des zones param√©trables parfaitement align√©es
    zones = []
    
    # OPTION 2 : Aligner la grille directement sur les coordonn√©es des orthos
    # Sauvegarder les bounds originaux des orthos AVANT modification
    original_ortho_bounds = BoundingBox(
        left=global_bounds.left,
        bottom=global_bounds.bottom,
        right=global_bounds.right,
        top=global_bounds.top
    )
    
    aligned_left = global_bounds.left
    aligned_bottom = global_bounds.bottom
    
    # √âtendre l√©g√®rement la grille en bas et √† droite pour couvrir compl√®tement
    # CORRECTION : Utiliser zone_size (ajust√©e) au lieu de zone_size_meters
    aligned_right = global_bounds.right + (zone_size - (global_bounds.right - global_bounds.left) % zone_size) % zone_size
    aligned_top = global_bounds.top + (zone_size - (global_bounds.top - global_bounds.bottom) % zone_size) % zone_size
    
    # S'assurer que l'extension ne d√©passe pas une zone compl√®te
    if aligned_right > global_bounds.right + zone_size:
        aligned_right = global_bounds.right + zone_size
    if aligned_top > global_bounds.top + zone_size:
        aligned_top = global_bounds.top + zone_size
    
    logger.info(f"Coordonn√©es des orthos (sans arrondi) : left={aligned_left:.6f}, bottom={aligned_bottom:.6f}, right={aligned_right:.6f}, top={aligned_top:.6f}")
    logger.info(f"  ‚Üí Grille align√©e directement sur les orthos (pas de d√©calage de r√©solution)")
    logger.info(f"  ‚Üí Extension en bas/droite pour couvrir compl√®tement les zones de {zone_size_meters}m")
    
    # Cr√©er les zones
    zone_id = 0
    x = aligned_left
    while x < aligned_right:
        y = aligned_bottom
        while y < aligned_top:
            zone = {
                'id': zone_id,
                'bounds': (x, y, x + zone_size, y + zone_size),
                'center': (x + zone_size/2, y + zone_size/2),
                'color': zone_id % 8  # 8 couleurs diff√©rentes pour identifier les zones
            }
            zones.append(zone)
            zone_id += 1
            y += zone_size
        x += zone_size
    
    logger.info(f"Cr√©√© {len(zones)} zones de {zone_size}m √ó {zone_size}m")
    
    # √âTAPE 1.2.5 : ASSIGNER LES ORTHOS AUX ZONES QUI LES CHEVAUCHENT
    logger.info("Assignation des orthos aux zones qui les chevauchent...")
    
    def zones_overlap(zone_bounds, ortho_bounds):
        """
        V√©rifie si une zone et une ortho se chevauchent
        zone_bounds: (x1, y1, x2, y2) - coordonn√©es de la zone
        ortho_bounds: (left, bottom, right, top) - bounds de l'ortho
        """
        x1, y1, x2, y2 = zone_bounds
        left, bottom, right, top = ortho_bounds
        
        # Pas de chevauchement si une zone est compl√®tement √† gauche, droite, haut ou bas
        return not (x2 < left or x1 > right or y2 < bottom or y1 > top)
    
    # Assigner les orthos aux zones
    zone_assignments = {}
    for zone in zones:
        zone_id = zone['id']
        zone_bounds = zone['bounds']
        overlapping_orthos = []
        
        for ortho_file in ortho_files:
            try:
                with rasterio.open(ortho_file) as src:
                    ortho_bounds = src.bounds
                    if zones_overlap(zone_bounds, ortho_bounds):
                        overlapping_orthos.append(os.path.basename(ortho_file))
                        logger.debug(f"Zone {zone_id} ‚Üê {os.path.basename(ortho_file)} (chevauchement)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de lire {os.path.basename(ortho_file)} : {e}")
        
        zone_assignments[zone_id] = overlapping_orthos
        logger.info(f"Zone {zone_id}: {len(overlapping_orthos)} ortho(s) assign√©e(s) : {overlapping_orthos}")
    
    # Ajouter les assignations aux zones
    for zone in zones:
        zone['assigned_orthos'] = zone_assignments[zone['id']]
    
    logger.info(f"‚úÖ Assignation termin√©e : {len(zone_assignments)} zones avec orthos assign√©es")
    
    # R√©sum√© des assignations
    total_orthos_assigned = sum(len(orthos) for orthos in zone_assignments.values())
    zones_with_ortho = sum(1 for orthos in zone_assignments.values() if len(orthos) > 0)
    logger.info(f"üìä R√©sum√© : {total_orthos_assigned} assignations totales, {zones_with_ortho}/{len(zones)} zones avec orthos")
    
    # üÜï CR√âATION DE L'IMAGE DE DEBUG AVEC CONTOURS DES ZONES (AVANT traitement parall√®le)
    # D√âSACTIV√âE - Probl√®mes d'affichage et non critique pour le fonctionnement
    # logger.info("üé® Cr√©ation de l'image de debug avec contours des zones...")
    
    # try:
    #     # Cr√©er une image de debug avec les contours des zones
    #     debug_image_path = os.path.join(output_dir, "debug_zones_contours.png")
    #     
    #     # Calculer les dimensions de l'image de debug
    #     # IMPORTANT : L'image doit couvrir TOUTES les zones, pas seulement les orthos !
    #     # Les zones s'√©tendent souvent au-del√† des bounds des orthos pour l'alignement
    #     debug_bounds = BoundingBox(
    #         left=aligned_left,      # Coordonn√©e la plus √† gauche des zones
    #         bottom=aligned_bottom,  # Coordonn√©e la plus basse des zones  
    #         right=aligned_right,    # Coordonn√©e la plus √† droite des zones
    #         top=aligned_top         # Coordonn√©e la plus haute des zones
    #     )
    #     
    #     # Ajouter des logs pour debug
    #     logger.info(f"üîç Debug - Bounds originaux des orthos : {original_ortho_bounds}")
    #     logger.info(f"üîç Debug - Bounds √©tendus pour toutes les zones : {debug_bounds}")
    #     logger.info(f"üîç Debug - R√©solution finale : {final_resolution}m")
    #     
    #     # Cr√©er une image de debug avec le M√äME g√©or√©f√©rencement que les orthos finales
    #     debug_resolution = final_resolution  # 0.003m/pixel (m√™me que les orthos)
    #     
    #     debug_width = int((debug_bounds.right - debug_bounds.left) / debug_resolution)
    #     debug_height = int((debug_bounds.top - debug_bounds.bottom) / debug_resolution)
    #     
    #     logger.info(f"üîç Debug - R√©solution debug : {debug_resolution}m/pixel (m√™me que les orthos)")
    #     logger.info(f"üîç Debug - Dimensions image : {debug_width} √ó {debug_height} pixels")
    #     logger.info(f"üîç Debug - Dimensions physiques : {(debug_bounds.right - debug_bounds.left):.3f}m √ó {(debug_bounds.top - debug_bounds.bottom):.3f}m")
    #     
    #     # Cr√©er une image vide (noire)
    #     debug_image = np.zeros((debug_height, debug_width, 3), dtype=np.uint8)
    #     
    #     # Dessiner les contours de chaque zone
    #     for zone in zones:
    #         zone_bounds = zone['bounds']  # (x, y, x + zone_size, y + zone_size)
    #         
    #         # Extraire les coordonn√©es du tuple
    #         zone_left = zone_bounds[0]
    #         zone_bottom = zone_bounds[1]
    #         zone_right = zone_bounds[2]
    #         zone_top = zone_bounds[3]
    #         
    #         # Convertir les coordonn√©es g√©ographiques en pixels (avec r√©solution debug)
    #         # Utiliser round() au lieu de int() pour √©viter les troncatures
    #         zone_left_px = round((zone_left - debug_bounds.left) / debug_resolution)
    #         zone_right_px = round((zone_right - debug_bounds.left) / debug_resolution)
    #         # CORRECTION : Pas d'inversion Y, utiliser directement les coordonn√©es
    #         zone_bottom_px = round((zone_bottom - debug_bounds.bottom) / debug_resolution)
    #         zone_top_px = round((zone_top - debug_bounds.bottom) / debug_resolution)
    #         
    #         # Log de debug pour cette zone
    #         logger.info(f"üîç Zone {zone['id']} - Coord: ({zone_left:.3f}, {zone_bottom:.3f}) √† ({zone_right:.3f}, {zone_top:.3f})")
    #         logger.info(f"üîç Zone {zone['id']} - Pixels: ({zone_left_px}, {zone_top_px}) √† ({zone_right_px}, {zone_bottom_px})")
    #         
    #         # Couleur unique pour chaque zone (cycle de couleurs)
    #         colors = [
    #             (255, 0, 0),    # Rouge
    #             (0, 255, 0),    # Vert
    #             (0, 0, 255),    # Bleu
    #             (255, 255, 0),  # Jaune
    #             (255, 0, 255),  # Magenta
    #             (0, 255, 255),  # Cyan
    #             (255, 128, 0),  # Orange
    #             (128, 0, 255),  # Violet
    #             (0, 128, 255),  # Bleu clair
    #             (255, 0, 128),  # Rose
    #             (128, 255, 0),  # Vert clair
    #             (255, 128, 128) # Rose clair
    #         ]
    #         color = colors[zone['id'] % len(colors)]
    #         
    #         # Dessiner le contour de la zone (2 pixels d'√©paisseur)
    #         thickness = 2
    #         
    #         # Lignes horizontales
    #         for y in range(max(0, zone_top_px), min(debug_height, zone_bottom_px + 1)):
    #             if 0 <= zone_left_px < debug_width:
    #                 debug_image[y, zone_left_px:min(zone_left_px + thickness, debug_width)] = color
    #             if 0 <= zone_right_px < debug_width:
    #                 debug_image[y, max(0, zone_right_px - thickness):zone_right_px] = color
    #                 
    #         # Lignes verticales
    #         for x in range(max(0, zone_left_px), min(debug_width, zone_right_px + 1)):
    #             if 0 <= zone_top_px < debug_height:
    #                 debug_image[max(0, zone_top_px):min(zone_top_px + thickness, debug_height), x] = color
    #             if 0 <= zone_bottom_px < debug_height:
    #                 debug_image[max(0, zone_bottom_px - thickness):zone_bottom_px, x] = color
    #                 
    #         # Ajouter le num√©ro de zone au centre
    #         center_x = (zone_left_px + zone_right_px) // 2
    #         center_y = (zone_top_px + zone_bottom_px) // 2
    #         
    #         if 0 <= center_x < debug_width and 0 <= center_y < debug_height:
    #             # Cr√©er un petit carr√© blanc avec le num√©ro
    #             label_size = 20
    #             label_left = max(0, center_x - label_size // 2)
    #             label_right = min(debug_width, center_x + label_size // 2)
    #             label_top = max(0, center_y - label_size // 2)
    #             label_bottom = min(debug_height, center_y + label_size // 2)
    #             
    #             debug_image[label_top:label_bottom, label_left:label_right] = [255, 255, 255]  # Blanc
    #         
    #     # Sauvegarder l'image de debug en GeoTIFF avec le bon g√©or√©f√©rencement
    #     import rasterio
    #     from rasterio.transform import from_origin
    #     
    #     # Cr√©er le transform pour le g√©or√©f√©rencement (m√™me que les orthos)
    #     transform = from_origin(debug_bounds.left, debug_bounds.top, debug_resolution, debug_resolution)
    #     
    #     # R√©cup√©rer le CRS des orthos unitaires (m√™me que les orthos finales)
    #     ortho_crs = None
    #     try:
    #         with rasterio.open(ortho_files[0]) as src:
    #             ortho_crs = src.crs
    #             logger.info(f"üîç Debug - CRS des orthos : {ortho_crs}")
    #     except Exception as e:
    #         logger.warning(f"‚ö†Ô∏è Impossible de lire le CRS des orthos : {e}")
    #         ortho_crs = 'EPSG:4326'  # Fallback
    #     
    #     # Sauvegarder en GeoTIFF avec g√©or√©f√©rencement
    #     with rasterio.open(
    #         debug_image_path.replace('.png', '.tif'),
    #         'w',
    #         driver='GTiff',
    #         height=debug_height,
    #         width=debug_width,
    #         count=3,
    #         dtype=debug_image.dtype,
    #         crs=ortho_crs,  # M√™me CRS que les orthos
    #         transform=transform
    #     ) as dst:
    #         dst.write(debug_image.transpose(2, 0, 1))  # PIL attend (channels, height, width)
    #     
    #     logger.info(f"‚úÖ GeoTIFF de debug cr√©√© : {debug_image_path.replace('.png', '.tif')}")
    #     logger.info(f"   üìè Dimensions : {debug_width} √ó {debug_height} pixels")
    #     logger.info(f"   üé® {len(zones)} zones avec contours color√©s")
    #     logger.info(f"   üó∫Ô∏è G√©or√©f√©rencement : {transform}")
    #     logger.info(f"   üó∫Ô∏è CRS : {ortho_crs}")
    #     
    # except Exception as e:
    #     logger.error(f"‚ö†Ô∏è Erreur lors de la cr√©ation de l'image de debug : {e}")
    
    logger.info("üé® Image de debug d√©sactiv√©e - Passage direct √† la parall√©lisation")
    
    # √âTAPE 1.3 : PARALL√âLISATION - TRAITEMENT DES ZONES AVEC ORTHOS R√âELLES
    logger.info("üöÄ D√âMARRAGE DE LA PARALL√âLISATION - Traitement des zones avec orthos r√©elles...")
    
    # Pr√©parer les donn√©es pour chaque processus
    process_data = []
    for zone in zones:
        process_data.append({
            'zone_id': zone['id'],
            'assigned_orthos': zone['assigned_orthos'],
            'bounds': zone['bounds'],
            'zone_size_meters': zone_size_meters,
            'final_resolution': final_resolution,
            'input_dir': input_dir,  # Chemin des orthos d'entr√©e
            'output_dir': output_dir  # Dossier de sortie pour les r√©sultats
        })
    
    logger.info(f"üìã Donn√©es pr√©par√©es pour {len(process_data)} processus")
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(4, len(process_data))  # Maximum 4 processus par d√©faut
    else:
        max_workers = min(max_workers, len(process_data))  # Respecter la limite demand√©e
    logger.info(f"üîß Lancement de {max_workers} processus parall√®les...")
    
    try:
        with Pool(processes=max_workers) as pool:
            logger.info(f"üöÄ Pool de processus cr√©√© avec {max_workers} workers")
            
            # Lancer le traitement parall√®le
            results = pool.map(process_zone_with_orthos, process_data)
            
            # Analyser les r√©sultats
            logger.info("üìä R√©sultats du traitement parall√®le :")
            total_color = sum(result['orthos_color_processed'] for result in results)
            total_height = sum(result['orthos_height_processed'] for result in results)
            
            for result in results:
                logger.info(f"  ‚úÖ {result['message']}")
            
            logger.info(f"üéâ PARALL√âLISATION TERMIN√âE : {len(results)} zones trait√©es")
            logger.info(f"üìä TOTAL : {total_color} orthos couleur + {total_height} MNT hauteur trait√©s")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la parall√©lisation : {e}")
        logger.warning("‚ö†Ô∏è Fallback vers le traitement s√©quentiel...")
        
        # Fallback s√©quentiel
        results = []
        for zone_data in process_data:
            try:
                result = process_zone_with_orthos(zone_data)
                results.append(result)
            except Exception as e:
                logger.error(f"‚ùå Erreur zone {zone_data['zone_id']} : {e}")
        
        logger.info(f"üîÑ Traitement s√©quentiel termin√© : {len(results)} zones trait√©es")
    
    logger.info(f"‚úÖ FUSION FINALE TERMIN√âE : Assemblage des orthoimages et MNT unifi√©s termin√©")
    logger.info(f"R√©sultat attendu : {len(results)} zones trait√©es avec orthos fusionn√©es")
    
    # üÜï √âTAPE 2 : √âGALISATION D√âSACTIV√âE POUR LE MOMENT
    logger.info("‚è∏Ô∏è √âGALISATION COLORIM√âTRIQUE D√âSACTIV√âE - Utilisation des zones originales")
    
    # Utiliser directement les zones fusionn√©es sans √©galisation
    equalized_zones = []
    for file in os.listdir(output_dir):
        if file.endswith('_fused_color_median_harmonized.tif'):
            equalized_zones.append(os.path.join(output_dir, file))
    
    logger.info(f"üìÅ Zones utilis√©es (sans √©galisation) : {len(equalized_zones)}")
    
    # üÜï √âTAPE 3 : ASSEMBLAGE AUTOMATIQUE DES ORTHOS UNIFI√âES
    logger.info("üöÄ LANCEMENT AUTOMATIQUE DE L'ASSEMBLAGE DES ORTHOS...")
    
    try:
        # R√©cup√©rer la r√©solution finale depuis les param√®tres
        final_resolution = None  # D√©tection automatique
        
        # Lancer l'assemblage automatique des orthos ET des MNT
        logger.info("üöÄ LANCEMENT DE L'ASSEMBLAGE AUTOMATIQUE : Orthos + MNT...")
        
        # Assemblage des orthos
        unified_ortho_path = simple_ortho_assembly(
            zones_output_dir=output_dir,
            logger=logger,
            final_resolution=final_resolution
        )
        
        # Assemblage des MNT
        unified_mnt_path = simple_mnt_assembly(
            zones_output_dir=output_dir,
            logger=logger,
            final_resolution=final_resolution
        )
        
        if unified_ortho_path and unified_mnt_path:
            logger.info(f"üéâ ASSEMBLAGE AUTOMATIQUE COMPLET R√âUSSI !")
            logger.info(f"üìÅ Ortho unifi√©e cr√©√©e : {unified_ortho_path}")
            logger.info(f"üìÅ MNT unifi√© cr√©√© : {unified_mnt_path}")
            logger.info(f"‚úÖ PIPELINE COMPLET TERMIN√â : Zones + Orthos + MNT unifi√©s")
        elif unified_ortho_path:
            logger.warning(f"‚ö†Ô∏è Ortho cr√©√©e mais MNT √©chou√© : {unified_ortho_path}")
        elif unified_mnt_path:
            logger.warning(f"‚ö†Ô∏è MNT cr√©√© mais ortho √©chou√©e : {unified_mnt_path}")
        else:
            logger.warning(f"‚ö†Ô∏è Assemblage automatique √©chou√©, mais zones cr√©√©es avec succ√®s")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'assemblage automatique : {e}")
        logger.warning(f"‚ö†Ô∏è Les zones ont √©t√© cr√©√©es, mais l'assemblage a √©chou√©")
    
    return output_dir

def simple_ortho_assembly(zones_output_dir, logger, final_resolution=None):
    """
    Simple assemblage des orthos de zones (pas de fusion)
    
    Args:
        zones_output_dir: R√©pertoire contenant les orthos fusionn√©es par zones
        logger: Logger pour les messages
        final_resolution: R√©solution finale en m√®tres (si None, utilise la r√©solution des orthos unitaires)
    
    Returns:
        str: Chemin vers l'ortho unifi√©e cr√©√©e
    """
    import os
    import numpy as np
    import rasterio
    from rasterio.coords import BoundingBox
    from rasterio.transform import from_origin
    
    logger.info("üîß ASSEMBLAGE SIMPLE DES ORTHOS DE ZONES (pas de fusion)")
    logger.info(f"üìÅ R√©pertoire des zones : {zones_output_dir}")
    
    # üîß CORRECTION : D√©tecter automatiquement la r√©solution des orthos unitaires
    if final_resolution is None:
        logger.info("üîç D√©tection automatique de la r√©solution des orthos unitaires...")
        # Lire la premi√®re ortho pour obtenir sa r√©solution
        first_ortho = None
        for file in os.listdir(zones_output_dir):
            if file.endswith('_fused_color_median_harmonized.tif'):
                first_ortho = os.path.join(zones_output_dir, file)
                break
        
        if first_ortho:
            with rasterio.open(first_ortho) as src:
                # Calculer la r√©solution √† partir de la transformation affine
                transform = src.transform
                # La r√©solution est la valeur absolue de a (largeur) et e (hauteur) de la transformation
                pixel_width = abs(transform.a)
                pixel_height = abs(transform.e)
                # Prendre la moyenne des deux r√©solutions
                final_resolution = (pixel_width + pixel_height) / 2
                logger.info(f"  ‚úÖ R√©solution d√©tect√©e : {final_resolution:.6f}m/pixel")
                logger.info(f"  üìè Largeur pixel : {pixel_width:.6f}m, Hauteur pixel : {pixel_height:.6f}m")
        else:
            logger.warning("‚ö†Ô∏è Aucune ortho trouv√©e, utilisation de la r√©solution par d√©faut 0.1m")
            final_resolution = 0.1
    
    logger.info(f"üìè R√©solution finale utilis√©e : {final_resolution}m")
    
    # √âTAPE 1 : Lire toutes les orthos de zones
    logger.info("üìñ Lecture des orthos de zones...")
    
    zone_ortho_files = []
    zone_bounds_list = []
    
    # Utiliser directement les zones originales (sans √©galisation)
    logger.info("  üîÑ Lecture des zones originales (sans √©galisation)...")
    for file in os.listdir(zones_output_dir):
        if file.endswith('_fused_color_median_harmonized.tif'):
            file_path = os.path.join(zones_output_dir, file)
            
            try:
                with rasterio.open(file_path) as src:
                    bounds = src.bounds
                    transform = src.transform
                    width = src.width
                    height = src.height
                    crs = src.crs
                    
                    # Extraire l'ID de zone du nom de fichier
                    zone_id = int(file.split('_')[1])
                    
                    zone_ortho_files.append({
                        'file_path': file_path,
                        'zone_id': zone_id,
                        'bounds': bounds,
                        'transform': transform,
                        'width': width,
                        'height': height,
                        'crs': crs
                    })
                    
                    zone_bounds_list.append(bounds)
                    
                    logger.info(f"  ‚úÖ Zone {zone_id}: {width}√ó{height} pixels, bounds: {bounds}")
                    
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Impossible de lire {file}: {e}")
                continue
    
    if not zone_ortho_files:
        logger.error("‚ùå Aucune ortho de zone trouv√©e !")
        return None
    
    logger.info(f"üìä Total : {len(zone_ortho_files)} orthos de zones trouv√©es")
    
    # √âTAPE 2 : Calculer la grille finale unifi√©e
    logger.info("üßÆ Calcul de la grille finale unifi√©e...")
    
    # Calculer l'√©tendue globale
    global_left = min(bounds.left for bounds in zone_bounds_list)
    global_bottom = min(bounds.bottom for bounds in zone_bounds_list)
    global_right = max(bounds.right for bounds in zone_bounds_list)
    global_top = max(bounds.top for bounds in zone_bounds_list)
    
    global_bounds = BoundingBox(
        left=global_left,
        bottom=global_bottom,
        right=global_right,
        top=global_top
    )
    
    logger.info(f"üåç √âtendue globale : {global_bounds}")
    logger.info(f"  üìè Largeur : {float(global_bounds.right - global_bounds.left):.3f}m")
    logger.info(f"  üìè Hauteur : {float(global_bounds.top - global_bounds.bottom):.3f}m")
    
    # Calculer les dimensions de la grille finale
    # CORRECTION : Utiliser round() au lieu de int() pour √©viter les lignes noires
    width_pixels = (global_bounds.right - global_bounds.left) / final_resolution
    height_pixels = (global_bounds.top - global_bounds.bottom) / final_resolution
    
    final_width = round(width_pixels)
    final_height = round(height_pixels)
    
    logger.info(f"üîç DEBUG Dimensions calcul√©es:")
    logger.info(f"  Largeur brute: {width_pixels:.6f} pixels -> arrondie: {final_width}")
    logger.info(f"  Hauteur brute: {height_pixels:.6f} pixels -> arrondie: {final_height}")
    logger.info(f"  Diff√©rence largeur: {abs(width_pixels - final_width):.6f} pixels")
    logger.info(f"  Diff√©rence hauteur: {abs(height_pixels - final_height):.6f} pixels")
    
    logger.info(f"üñºÔ∏è Grille finale : {final_width} √ó {final_height} pixels")
    
    # √âTAPE 3 : Cr√©er la grille finale vide
    logger.info("üé® Cr√©ation de la grille finale vide...")
    
    # Initialiser avec des valeurs nodata (noir)
    final_ortho = np.zeros((final_height, final_width, 3), dtype=np.uint8)
    
    # Cr√©er le transform pour la grille finale
    # CORRECTION : Utiliser from_bounds pour un alignement parfait
    final_transform = rasterio.transform.from_bounds(
        global_bounds.left, global_bounds.bottom, 
        global_bounds.right, global_bounds.top, 
        final_width, final_height
    )
    
    logger.info(f"üîç DEBUG Transform final:")
    logger.info(f"  from_origin: left={global_bounds.left:.6f}, top={global_bounds.top:.6f}")
    logger.info(f"  from_bounds: {final_transform}")
    
    # √âTAPE 4 : Placer chaque ortho de zone √† sa position exacte
    logger.info("üîß Placement des orthos de zones...")
    
    zones_placed = 0
    
    for zone_data in zone_ortho_files:
        try:
            logger.info(f"  üîÑ Placement Zone {zone_data['zone_id']}...")
            
            # Lire l'ortho de la zone
            with rasterio.open(zone_data['file_path']) as src:
                zone_image = src.read([1, 2, 3])  # RGB
                zone_bounds = zone_data['bounds']
            
            # Calculer la position dans la grille finale
            # CORRECTION COMPL√àTE : Utiliser round() pour √©viter les d√©calages d'un pixel
            x_pos = (zone_bounds.left - global_bounds.left) / final_resolution
            y_pos = (global_bounds.top - zone_bounds.top) / final_resolution
            
            start_x = round(x_pos)
            start_y = round(y_pos)
            end_x = start_x + zone_data['width']
            end_y = start_y + zone_data['height']
            
            # DEBUG : V√©rifier les arrondis de position
            logger.info(f"    üîç DEBUG Position calcul√©e:")
            logger.info(f"      x_pos brute: {x_pos:.6f} -> arrondie: {start_x}")
            logger.info(f"      y_pos brute: {y_pos:.6f} -> arrondie: {start_y}")
            logger.info(f"      d√©calage x: {abs(x_pos - start_x):.6f} pixels")
            logger.info(f"      d√©calage y: {abs(y_pos - start_y):.6f} pixels")
            
            # DEBUG : Afficher les calculs d√©taill√©s pour v√©rifier l'alignement
            logger.info(f"    üîç DEBUG Alignement Y:")
            logger.info(f"      global_bounds.top: {global_bounds.top:.6f}m")
            logger.info(f"      zone_bounds.top: {zone_bounds.top:.6f}m")
            logger.info(f"      diff_y: {global_bounds.top - zone_bounds.top:.6f}m")
            logger.info(f"      start_y pixels: {start_y}")
            logger.info(f"      zone height: {zone_data['height']} pixels")
            
            logger.info(f"    üìç Position dans la grille : ({start_x}, {start_y}) √† ({end_x}, {end_y})")
            
            # V√©rifier les limites
            if (start_x < 0 or start_y < 0 or 
                end_x > final_width or end_y > final_height):
                logger.warning(f"    ‚ö†Ô∏è Zone {zone_data['zone_id']} d√©passe les limites de la grille finale")
                continue
            
            # Placer l'ortho de la zone dans la grille finale
            # Note : zone_image est (3, height, width), on transpose pour (height, width, 3)
            zone_image_rgb = zone_image.transpose(1, 2, 0)
            
            final_ortho[start_y:end_y, start_x:end_x] = zone_image_rgb
            
            zones_placed += 1
            logger.info(f"    ‚úÖ Zone {zone_data['zone_id']} plac√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"    ‚ùå Erreur lors du placement de la Zone {zone_data['zone_id']}: {e}")
            continue
    
    logger.info(f"üìä Zones plac√©es : {zones_placed}/{len(zone_ortho_files)}")
    
    # √âTAPE 5 : Sauvegarder l'ortho unifi√©e finale
    logger.info("üíæ Sauvegarde de l'ortho unifi√©e finale...")
    
    output_path = os.path.join(zones_output_dir, "ortho_unified_final.tif")
    
    # R√©cup√©rer le CRS de la premi√®re zone (toutes devraient avoir le m√™me)
    reference_crs = zone_ortho_files[0]['crs']
    
    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=final_height,
        width=final_width,
        count=3,
        dtype=np.uint8,
        crs=reference_crs,
        transform=final_transform,
        photometric='rgb'
    ) as dst:
        # √âcrire les 3 bandes RGB
        dst.write(final_ortho[:, :, 0], 1)  # Rouge
        dst.write(final_ortho[:, :, 1], 2)  # Vert
        dst.write(final_ortho[:, :, 2], 3)  # Bleu
        
        # M√©tadonn√©es
        dst.update_tags(
            Software='PhotoGeoAlign Simple Ortho Assembly',
            Resolution=f'{final_resolution}m per pixel',
            Zones_Processed=str(zones_placed),
            Assembly_Method='Simple placement without fusion'
        )
    
    logger.info(f"üéâ ORTHO UNIFI√âE CR√â√âE : {output_path}")
    logger.info(f"   üìè Dimensions : {final_width} √ó {final_height} pixels")
    logger.info(f"   üìè √âtendue : {(global_bounds.right - global_bounds.left):.3f}m √ó {(global_bounds.top - global_bounds.bottom):.3f}m")
    logger.info(f"   üéØ Zones assembl√©es : {zones_placed}")
    
    return output_path

def calculate_global_histogram_and_quantiles(zones_output_dir, logger):
    """
    CALCUL GLOBAL : Histogramme et quantiles de r√©f√©rence sur TOUTES les zones
    Exclut les pixels noirs ET tr√®s sombres (seuil r√©aliste)
    
    Args:
        zones_output_dir: R√©pertoire contenant toutes les zones
        logger: Logger pour les messages
    
    Returns:
        dict: Quantiles globaux de r√©f√©rence par bande
    """
    import numpy as np
    import os
    import rasterio
    
    logger.info("üåç CALCUL GLOBAL : Histogramme et quantiles de r√©f√©rence sur toutes les zones...")
    
    # √âTAPE 1 : Trouver tous les fichiers de zones
    zone_files = []
    for file in os.listdir(zones_output_dir):
        if file.endswith('_fused_color_median_harmonized.tif'):
            zone_files.append(os.path.join(zones_output_dir, file))
    
    if not zone_files:
        logger.warning("‚ö†Ô∏è Aucune zone trouv√©e pour le calcul global")
        return None
    
    logger.info(f"  üìÅ Zones trouv√©es : {len(zone_files)}")
    
    # √âTAPE 2 : Calculer l'histogramme global de toutes les zones
    logger.info("  üìä Calcul de l'histogramme global...")
    global_histogram = np.zeros((3, 256))  # R, G, B, 0-255
    
    total_pixels_processed = 0
    total_valid_pixels = 0
    
    for zone_file in zone_files:
        try:
            with rasterio.open(zone_file) as src:
                zone_image = src.read()  # (3, H, W)
                height, width = src.height, src.width
                
                # Convertir en format (H, W, 3) pour le traitement
                zone_image_rgb = zone_image.transpose(1, 2, 0)
                
                # Cr√©er un masque pour exclure UNIQUEMENT les pixels noirs (0,0,0)
                black_pixels_mask = np.all(zone_image_rgb == [0, 0, 0], axis=2)
                valid_pixels_mask = ~black_pixels_mask
                
                valid_pixels_count = np.sum(valid_pixels_mask)
                total_pixels_processed += height * width
                total_valid_pixels += valid_pixels_count
                
                # Ajouter √† l'histogramme global (pixels valides uniquement)
                for band_idx in range(3):
                    band = zone_image_rgb[:, :, band_idx]
                    valid_band_values = band[valid_pixels_mask]
                    
                    if len(valid_band_values) > 0:
                        hist, _ = np.histogram(valid_band_values, bins=256, range=(0, 256))
                        global_histogram[band_idx] += hist
                
                logger.info(f"    ‚úÖ {os.path.basename(zone_file)}: {valid_pixels_count}/{height*width} pixels valides")
                
        except Exception as e:
            logger.warning(f"    ‚ö†Ô∏è Erreur lors du traitement de {os.path.basename(zone_file)}: {e}")
            continue
    
    logger.info(f"  üìä Total global : {total_valid_pixels}/{total_pixels_processed} pixels valides ({total_valid_pixels/total_pixels_processed*100:.1f}%)")
    
    # √âTAPE 3 : Calculer les quantiles de r√©f√©rence globaux
    logger.info("  üéØ Calcul des quantiles de r√©f√©rence globaux...")
    global_quantiles = {}
    
    for band_idx in range(3):
        cumulative_hist = np.cumsum(global_histogram[band_idx])
        total_pixels_band = cumulative_hist[-1]
        
        if total_pixels_band == 0:
            logger.warning(f"    ‚ö†Ô∏è Bande {['Rouge', 'Vert', 'Bleu'][band_idx]} : aucun pixel valide")
            global_quantiles[band_idx] = {'q25': 128, 'q50': 128, 'q75': 128}
            continue
        
        # Trouver les indices des quantiles
        q25_idx = np.searchsorted(cumulative_hist, total_pixels_band * 0.25)
        q50_idx = np.searchsorted(cumulative_hist, total_pixels_band * 0.50)
        q75_idx = np.searchsorted(cumulative_hist, total_pixels_band * 0.75)
        
        global_quantiles[band_idx] = {
            'q25': int(q25_idx),
            'q50': int(q50_idx), 
            'q75': int(q75_idx)
        }
        
        logger.info(f"    {['Rouge', 'Vert', 'Bleu'][band_idx]}: Q25={q25_idx}, Q50={q50_idx}, Q75={q75_idx}")
    
    logger.info("  ‚úÖ Quantiles globaux calcul√©s avec succ√®s")
    return global_quantiles

def equalize_zone_to_global_quantiles(zone_ortho_path, global_quantiles, logger):
    """
    √âGALISATION GLOBALE : √âgalise une zone vers les quantiles globaux de r√©f√©rence
    PR√âSERVE LA G√âOSPATIALIT√â (CRS + g√©otransformation)
    
    Args:
        zone_ortho_path: Chemin vers l'ortho de zone √† √©galiser
        global_quantiles: Quantiles globaux de r√©f√©rence
        logger: Logger pour les messages
    
    Returns:
        str: Chemin vers la zone √©galis√©e
    """
    import numpy as np
    import os
    import rasterio
    
    logger.info(f"üé® √âGALISATION GLOBALE DE LA ZONE : {os.path.basename(zone_ortho_path)}")
    
    try:
        # √âTAPE 1 : Lire l'image avec rasterio pour pr√©server la g√©ospatialit√©
        logger.info(f"  üìñ Lecture de la zone avec rasterio...")
        with rasterio.open(zone_ortho_path) as src:
            zone_image = src.read()  # (3, H, W)
            height, width = src.height, src.width
            crs = src.crs
            transform = src.transform
            bounds = src.bounds
            
            logger.info(f"  üìè Dimensions : {width} √ó {height} pixels")
            logger.info(f"  üåç CRS : {crs}")
            logger.info(f"  üìç Bounds : {bounds}")
        
        # Convertir en format (H, W, 3) pour le traitement
        zone_image_rgb = zone_image.transpose(1, 2, 0)  # (H, W, 3)
        
        # √âTAPE 2 : Cr√©er un masque pour exclure UNIQUEMENT les pixels noirs (0,0,0)
        logger.info(f"  üîç Cr√©ation du masque des pixels valides...")
        black_pixels_mask = np.all(zone_image_rgb == [0, 0, 0], axis=2)
        valid_pixels_mask = ~black_pixels_mask
        
        valid_pixels_count = np.sum(valid_pixels_mask)
        total_pixels = height * width
        
        logger.info(f"  üîç Pixels valides : {valid_pixels_count}/{total_pixels} ({valid_pixels_count/total_pixels*100:.1f}%)")
        
        if valid_pixels_count == 0:
            logger.warning(f"  ‚ö†Ô∏è Aucun pixel valide trouv√© dans la zone")
            return zone_ortho_path
        
        # √âTAPE 3 : √âgalisation vers les quantiles globaux
        logger.info(f"  üîß Application de l'√©galisation vers les quantiles globaux...")
        
        # Cr√©er une image de sortie
        equalized_image = np.zeros_like(zone_image_rgb)
        
        # √âgalisation par bande vers les quantiles globaux
        equalization_factors = []
        
        for band_idx in range(3):
            band = zone_image_rgb[:, :, band_idx]
            valid_band_values = band[valid_pixels_mask]
            
            if len(valid_band_values) == 0:
                equalized_image[:, :, band_idx] = band
                equalization_factors.append(1.0)
                continue
            
            # Calculer les quantiles actuels de la zone
            current_q25 = np.percentile(valid_band_values, 25)
            current_q50 = np.percentile(valid_band_values, 50)
            current_q75 = np.percentile(valid_band_values, 75)
            
            # Quantiles globaux de r√©f√©rence
            target_q25 = global_quantiles[band_idx]['q25']
            target_q50 = global_quantiles[band_idx]['q50']
            target_q75 = global_quantiles[band_idx]['q75']
            
            logger.info(f"    {['Rouge', 'Vert', 'Bleu'][band_idx]}:")
            logger.info(f"      Actuel: Q25={current_q25:.1f}, Q50={current_q50:.1f}, Q75={current_q75:.1f}")
            logger.info(f"      Cible: Q25={target_q25}, Q50={target_q50}, Q75={target_q75}")
            
            # √âTAPE 4 : √âgalisation par transformation de quantiles
            # Utiliser une transformation lin√©aire bas√©e sur les quantiles
            if current_q75 > current_q25:  # √âviter division par z√©ro
                # Transformation lin√©aire : (x - q25) / (q75 - q25) * (target_q75 - target_q25) + target_q25
                scale_factor = (target_q75 - target_q25) / (current_q75 - current_q25)
                offset = target_q25 - current_q25 * scale_factor
                
                # Appliquer la transformation
                equalized_band = np.clip(band * scale_factor + offset, 0, 255).astype(np.uint8)
                
                # Calculer le facteur d'√©galisation pour les logs
                factor = scale_factor
            else:
                # Si pas de variation, appliquer un d√©calage simple
                offset = target_q50 - current_q50
                equalized_band = np.clip(band + offset, 0, 255).astype(np.uint8)
                factor = 1.0
            
            equalized_image[:, :, band_idx] = equalized_band
            equalization_factors.append(factor)
        
        logger.info(f"  üìä Facteurs d'√©galisation: R={equalization_factors[0]:.3f}, G={equalization_factors[1]:.3f}, B={equalization_factors[2]:.3f}")
        
        # √âTAPE 5 : Sauvegarder avec rasterio pour pr√©server la g√©ospatialit√©
        logger.info(f"  üíæ Sauvegarde avec rasterio (g√©ospatialit√© pr√©serv√©e)...")
        
        # Cr√©er le nom de fichier de sortie
        base_name = os.path.splitext(os.path.basename(zone_ortho_path))[0]
        output_path = zone_ortho_path.replace(
            base_name, 
            f"{base_name}_equalized_global_quantiles"
        )
        
        # Sauvegarder avec rasterio en pr√©servant CRS et g√©otransformation
        with rasterio.open(
            output_path,
            'w',
            driver='GTiff',
            height=height,
            width=width,
            count=3,
            dtype=np.uint8,
            crs=crs,  # ‚úÖ PR√âSERVE LE CRS
            transform=transform,  # ‚úÖ PR√âSERVE LA G√âOTRANSFORMATION
            photometric='rgb'
        ) as dst:
            # √âcrire les 3 bandes RGB (format rasterio : (3, H, W))
            equalized_image_rasterio = equalized_image.transpose(2, 0, 1)  # (H, W, 3) ‚Üí (3, H, W)
            dst.write(equalized_image_rasterio)
            
            # M√©tadonn√©es
            dst.update_tags(
                Software='PhotoGeoAlign Global Quantile Equalization',
                Method='Global quantile transformation',
                Global_Q25=f"{[global_quantiles[i]['q25'] for i in range(3)]}",
                Global_Q50=f"{[global_quantiles[i]['q50'] for i in range(3)]}",
                Global_Q75=f"{[global_quantiles[i]['q75'] for i in range(3)]}",
                Equalization_Factors=f"{equalization_factors}",
                Pixels_Excluded=str(total_pixels - valid_pixels_count)
            )
        
        logger.info(f"  ‚úÖ Zone √©galis√©e sauvegard√©e : {os.path.basename(output_path)}")
        logger.info(f"  üìã M√©tadonn√©es d'√©galisation:")
        logger.info(f"    - M√©thode: √âgalisation vers quantiles globaux")
        logger.info(f"    - Pixels exclus: {total_pixels - valid_pixels_count} (noirs [0,0,0])")
        logger.info(f"    - Facteurs R/G/B: {equalization_factors}")
        logger.info(f"    - G√©ospatialit√©: CRS et g√©otransformation pr√©serv√©s ‚úÖ")
        
        return output_path
        
    except Exception as e:
        logger.error(f"  ‚ùå Erreur lors de l'√©galisation de la zone : {e}")
        return None

def individual_zone_equalization(zone_ortho_path, logger):
    """
    √âGALISATION GLOBALE : Interface principale pour l'√©galisation des zones
    Utilise la nouvelle strat√©gie d'√©galisation globale bas√©e sur les quantiles
    
    Args:
        zone_ortho_path: Chemin vers l'ortho de zone √† √©galiser
        logger: Logger pour les messages
    
    Returns:
        str: Chemin vers la zone √©galis√©e
    """
    # Cette fonction est maintenant une interface qui appelle l'√©galisation globale
    # Elle sera remplac√©e par l'appel direct dans le pipeline principal
    logger.warning("‚ö†Ô∏è Cette fonction est obsol√®te. Utilisez l'√©galisation globale directement.")
    return zone_ortho_path
    
    final_width = round(width_pixels)
    final_height = round(height_pixels)
    
    logger.info(f"üîç DEBUG Dimensions calcul√©es:")
    logger.info(f"  Largeur brute: {width_pixels:.6f} pixels -> arrondie: {final_width}")
    logger.info(f"  Hauteur brute: {height_pixels:.6f} pixels -> arrondie: {final_height}")
    logger.info(f"  Diff√©rence largeur: {abs(width_pixels - final_width):.6f} pixels")
    logger.info(f"  Diff√©rence hauteur: {abs(height_pixels - final_height):.6f} pixels")
    
    logger.info(f"üñºÔ∏è Grille finale : {final_width} √ó {final_height} pixels")
    
    # √âTAPE 3 : Cr√©er la grille finale vide
    logger.info("üé® Cr√©ation de la grille finale vide...")
    
    # Initialiser avec des valeurs nodata (noir)
    final_ortho = np.zeros((final_height, final_width, 3), dtype=np.uint8)
    
    # Cr√©er le transform pour la grille finale
    # CORRECTION : Utiliser from_bounds pour un alignement parfait
    final_transform = rasterio.transform.from_bounds(
        global_bounds.left, global_bounds.bottom, 
        global_bounds.right, global_bounds.top, 
        final_width, final_height
    )
    
    logger.info(f"üîç DEBUG Transform final:")
    logger.info(f"  from_origin: left={global_bounds.left:.6f}, top={global_bounds.top:.6f}")
    logger.info(f"  from_bounds: {final_transform}")
    
    # √âTAPE 4 : Placer chaque ortho de zone √† sa position exacte
    logger.info("üîß Placement des orthos de zones...")
    
    zones_placed = 0
    
    for zone_data in zone_ortho_files:
        try:
            logger.info(f"  üîÑ Placement Zone {zone_data['zone_id']}...")
            
            # Lire l'ortho de la zone
            with rasterio.open(zone_data['file_path']) as src:
                zone_image = src.read([1, 2, 3])  # RGB
                zone_bounds = zone_data['bounds']
            
            # Calculer la position dans la grille finale
            # CORRECTION COMPL√àTE : Utiliser round() pour √©viter les d√©calages d'un pixel
            x_pos = (zone_bounds.left - global_bounds.left) / final_resolution
            y_pos = (global_bounds.top - zone_bounds.top) / final_resolution
            
            start_x = round(x_pos)
            start_y = round(y_pos)
            end_x = start_x + zone_data['width']
            end_y = start_y + zone_data['height']
            
            # DEBUG : V√©rifier les arrondis de position
            logger.info(f"    üîç DEBUG Position calcul√©e:")
            logger.info(f"      x_pos brute: {x_pos:.6f} -> arrondie: {start_x}")
            logger.info(f"      y_pos brute: {y_pos:.6f} -> arrondie: {start_y}")
            logger.info(f"      d√©calage x: {abs(x_pos - start_x):.6f} pixels")
            logger.info(f"      d√©calage y: {abs(y_pos - start_y):.6f} pixels")
            
            # DEBUG : Afficher les calculs d√©taill√©s pour v√©rifier l'alignement
            logger.info(f"    üîç DEBUG Alignement Y:")
            logger.info(f"      global_bounds.top: {global_bounds.top:.6f}m")
            logger.info(f"      zone_bounds.top: {zone_bounds.top:.6f}m")
            logger.info(f"      diff_y: {global_bounds.top - zone_bounds.top:.6f}m")
            logger.info(f"      start_y pixels: {start_y}")
            logger.info(f"      zone height: {zone_data['height']} pixels")
            
            logger.info(f"    üìç Position dans la grille : ({start_x}, {start_y}) √† ({end_x}, {end_y})")
            
            # V√©rifier les limites
            if (start_x < 0 or start_y < 0 or 
                end_x > final_width or end_y > final_height):
                logger.warning(f"    ‚ö†Ô∏è Zone {zone_data['zone_id']} d√©passe les limites de la grille finale")
                continue
            
            # Placer l'ortho de la zone dans la grille finale
            # Note : zone_image est (3, height, width), on transpose pour (height, width, 3)
            zone_image_rgb = zone_image.transpose(1, 2, 0)
            
            final_ortho[start_y:end_y, start_x:end_x] = zone_image_rgb
            
            zones_placed += 1
            logger.info(f"    ‚úÖ Zone {zone_data['zone_id']} plac√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"    ‚ùå Erreur lors du placement de la Zone {zone_data['zone_id']}: {e}")
            continue
    
    logger.info(f"üìä Zones plac√©es : {zones_placed}/{len(zone_ortho_files)}")
    
    # √âTAPE 5 : Sauvegarder l'ortho unifi√©e finale
    logger.info("üíæ Sauvegarde de l'ortho unifi√©e finale...")
    
    output_path = os.path.join(zones_output_dir, "ortho_unified_final.tif")
    
    # R√©cup√©rer le CRS de la premi√®re zone (toutes devraient avoir le m√™me)
    reference_crs = zone_ortho_files[0]['crs']
    
    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=final_height,
        width=final_width,
        count=3,
        dtype=np.uint8,
        crs=reference_crs,
        transform=final_transform,
        photometric='rgb'
    ) as dst:
        # √âcrire les 3 bandes RGB
        dst.write(final_ortho[:, :, 0], 1)  # Rouge
        dst.write(final_ortho[:, :, 1], 2)  # Vert
        dst.write(final_ortho[:, :, 2], 3)  # Bleu
        
        # M√©tadonn√©es
        dst.update_tags(
            Software='PhotoGeoAlign Simple Ortho Assembly',
            Resolution=f'{final_resolution}m per pixel',
            Zones_Processed=str(zones_placed),
            Assembly_Method='Simple placement without fusion'
        )
    
    logger.info(f"üéâ ORTHO UNIFI√âE CR√â√âE : {output_path}")
    logger.info(f"   üìè Dimensions : {final_width} √ó {final_height} pixels")
    logger.info(f"   üìè √âtendue : {(global_bounds.right - global_bounds.left):.3f}m √ó {(global_bounds.top - global_bounds.bottom):.3f}m")
    logger.info(f"   üéØ Zones assembl√©es : {zones_placed}")
    
    return output_path
    final_height = round(height_pixels)
    
    logger.info(f"üîç DEBUG Dimensions calcul√©es:")
    logger.info(f"  Largeur brute: {width_pixels:.6f} pixels -> arrondie: {final_width}")
    logger.info(f"  Hauteur brute: {height_pixels:.6f} pixels -> arrondie: {final_height}")
    logger.info(f"  Diff√©rence largeur: {abs(width_pixels - final_width):.6f} pixels")
    logger.info(f"  Diff√©rence hauteur: {abs(height_pixels - final_height):.6f} pixels")
    
    logger.info(f"üñºÔ∏è Grille finale : {final_width} √ó {final_height} pixels")
    
    # √âTAPE 3 : Cr√©er la grille finale vide
    logger.info("üé® Cr√©ation de la grille finale vide...")
    
    # Initialiser avec des valeurs nodata (noir)
    final_ortho = np.zeros((final_height, final_width, 3), dtype=np.uint8)
    
    # Cr√©er le transform pour la grille finale
    # CORRECTION : Utiliser from_bounds pour un alignement parfait
    final_transform = rasterio.transform.from_bounds(
        global_bounds.left, global_bounds.bottom, 
        global_bounds.right, global_bounds.top, 
        final_width, final_height
    )
    
    logger.info(f"üîç DEBUG Transform final:")
    logger.info(f"  from_origin: left={global_bounds.left:.6f}, top={global_bounds.top:.6f}")
    logger.info(f"  from_bounds: {final_transform}")
    
    # √âTAPE 4 : Placer chaque ortho de zone √† sa position exacte
    logger.info("üîß Placement des orthos de zones...")
    
    zones_placed = 0
    
    for zone_data in zone_ortho_files:
        try:
            logger.info(f"  üîÑ Placement Zone {zone_data['zone_id']}...")
            
            # Lire l'ortho de la zone
            with rasterio.open(zone_data['file_path']) as src:
                zone_image = src.read([1, 2, 3])  # RGB
                zone_bounds = zone_data['bounds']
            
            # Calculer la position dans la grille finale
            # CORRECTION COMPL√àTE : Utiliser round() pour √©viter les d√©calages d'un pixel
            x_pos = (zone_bounds.left - global_bounds.left) / final_resolution
            y_pos = (global_bounds.top - zone_bounds.top) / final_resolution
            
            start_x = round(x_pos)
            start_y = round(y_pos)
            end_x = start_x + zone_data['width']
            end_y = start_y + zone_data['height']
            
            # DEBUG : V√©rifier les arrondis de position
            logger.info(f"    üîç DEBUG Position calcul√©e:")
            logger.info(f"      x_pos brute: {x_pos:.6f} -> arrondie: {start_x}")
            logger.info(f"      y_pos brute: {y_pos:.6f} -> arrondie: {start_y}")
            logger.info(f"      d√©calage x: {abs(x_pos - start_x):.6f} pixels")
            logger.info(f"      d√©calage y: {abs(y_pos - start_y):.6f} pixels")
            
            # DEBUG : Afficher les calculs d√©taill√©s pour v√©rifier l'alignement
            logger.info(f"    üîç DEBUG Alignement Y:")
            logger.info(f"      global_bounds.top: {global_bounds.top:.6f}m")
            logger.info(f"      zone_bounds.top: {zone_bounds.top:.6f}m")
            logger.info(f"      diff_y: {global_bounds.top - zone_bounds.top:.6f}m")
            logger.info(f"      start_y pixels: {start_y}")
            logger.info(f"      zone height: {zone_data['height']} pixels")
            
            logger.info(f"    üìç Position dans la grille : ({start_x}, {start_y}) √† ({end_x}, {end_y})")
            
            # V√©rifier les limites
            if (start_x < 0 or start_y < 0 or 
                end_x > final_width or end_y > final_height):
                logger.warning(f"    ‚ö†Ô∏è Zone {zone_data['zone_id']} d√©passe les limites de la grille finale")
                continue
            
            # Placer l'ortho de la zone dans la grille finale
            # Note : zone_image est (3, height, width), on transpose pour (height, width, 3)
            zone_image_rgb = zone_image.transpose(1, 2, 0)
            
            final_ortho[start_y:end_y, start_x:end_x] = zone_image_rgb
            
            zones_placed += 1
            logger.info(f"    ‚úÖ Zone {zone_data['zone_id']} plac√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"    ‚ùå Erreur lors du placement de la Zone {zone_data['zone_id']}: {e}")
            continue
    
    logger.info(f"üìä Zones plac√©es : {zones_placed}/{len(zone_ortho_files)}")
    
    # √âTAPE 5 : Sauvegarder l'ortho unifi√©e finale
    logger.info("üíæ Sauvegarde de l'ortho unifi√©e finale...")
    
    output_path = os.path.join(zones_output_dir, "ortho_unified_final.tif")
    
    # R√©cup√©rer le CRS de la premi√®re zone (toutes devraient avoir le m√™me)
    reference_crs = zone_ortho_files[0]['crs']
    
    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=final_height,
        width=final_width,
        count=3,
        dtype=np.uint8,
        crs=reference_crs,
        transform=final_transform,
        photometric='rgb'
    ) as dst:
        # √âcrire les 3 bandes RGB
        dst.write(final_ortho[:, :, 0], 1)  # Rouge
        dst.write(final_ortho[:, :, 1], 2)  # Vert
        dst.write(final_ortho[:, :, 2], 3)  # Bleu
        
        # M√©tadonn√©es
        dst.update_tags(
            Software='PhotoGeoAlign Simple Ortho Assembly',
            Resolution=f'{final_resolution}m per pixel',
            Zones_Processed=str(zones_placed),
            Assembly_Method='Simple placement without fusion'
        )
    
    logger.info(f"üéâ ORTHO UNIFI√âE CR√â√âE : {output_path}")
    logger.info(f"   üìè Dimensions : {final_width} √ó {final_height} pixels")
    logger.info(f"   üìè √âtendue : {(global_bounds.right - global_bounds.left):.3f}m √ó {(global_bounds.top - global_bounds.bottom):.3f}m")
    logger.info(f"   üéØ Zones assembl√©es : {zones_placed}")
    
    return output_path

def calculate_global_histogram_and_quantiles(zones_output_dir, logger):
    """
    CALCUL GLOBAL : Histogramme et quantiles de r√©f√©rence sur TOUTES les zones
    Exclut les pixels noirs ET tr√®s sombres (seuil r√©aliste)
    
    Args:
        zones_output_dir: R√©pertoire contenant toutes les zones
        logger: Logger pour les messages
    
    Returns:
        dict: Quantiles globaux de r√©f√©rence par bande
    """
    import numpy as np
    import os
    import rasterio
    
    logger.info("üåç CALCUL GLOBAL : Histogramme et quantiles de r√©f√©rence sur toutes les zones...")
    
    # √âTAPE 1 : Trouver tous les fichiers de zones
    zone_files = []
    for file in os.listdir(zones_output_dir):
        if file.endswith('_fused_color_median_harmonized.tif'):
            zone_files.append(os.path.join(zones_output_dir, file))
    
    if not zone_files:
        logger.warning("‚ö†Ô∏è Aucune zone trouv√©e pour le calcul global")
        return None
    
    logger.info(f"  üìÅ Zones trouv√©es : {len(zone_files)}")
    
    # √âTAPE 2 : Calculer l'histogramme global de toutes les zones
    logger.info("  üìä Calcul de l'histogramme global...")
    global_histogram = np.zeros((3, 256))  # R, G, B, 0-255
    
    total_pixels_processed = 0
    total_valid_pixels = 0
    
    for zone_file in zone_files:
        try:
            with rasterio.open(zone_file) as src:
                zone_image = src.read()  # (3, H, W)
                height, width = src.height, src.width
                
                # Convertir en format (H, W, 3) pour le traitement
                zone_image_rgb = zone_image.transpose(1, 2, 0)
                
                # Cr√©er un masque pour exclure UNIQUEMENT les pixels noirs (0,0,0)
                black_pixels_mask = np.all(zone_image_rgb == [0, 0, 0], axis=2)
                valid_pixels_mask = ~black_pixels_mask
                
                valid_pixels_count = np.sum(valid_pixels_mask)
                total_pixels_processed += height * width
                total_valid_pixels += valid_pixels_count
                
                # Ajouter √† l'histogramme global (pixels valides uniquement)
                for band_idx in range(3):
                    band = zone_image_rgb[:, :, band_idx]
                    valid_band_values = band[valid_pixels_mask]
                    
                    if len(valid_band_values) > 0:
                        hist, _ = np.histogram(valid_band_values, bins=256, range=(0, 256))
                        global_histogram[band_idx] += hist
                
                logger.info(f"    ‚úÖ {os.path.basename(zone_file)}: {valid_pixels_count}/{height*width} pixels valides")
                
        except Exception as e:
            logger.warning(f"    ‚ö†Ô∏è Erreur lors du traitement de {os.path.basename(zone_file)}: {e}")
            continue
    
    logger.info(f"  üìä Total global : {total_valid_pixels}/{total_pixels_processed} pixels valides ({total_valid_pixels/total_pixels_processed*100:.1f}%)")
    
    # √âTAPE 3 : Calculer les quantiles de r√©f√©rence globaux
    logger.info("  üéØ Calcul des quantiles de r√©f√©rence globaux...")
    global_quantiles = {}
    
    for band_idx in range(3):
        cumulative_hist = np.cumsum(global_histogram[band_idx])
        total_pixels_band = cumulative_hist[-1]
        
        if total_pixels_band == 0:
            logger.warning(f"    ‚ö†Ô∏è Bande {['Rouge', 'Vert', 'Bleu'][band_idx]} : aucun pixel valide")
            global_quantiles[band_idx] = {'q25': 128, 'q50': 128, 'q75': 128}
            continue
        
        # Trouver les indices des quantiles
        q25_idx = np.searchsorted(cumulative_hist, total_pixels_band * 0.25)
        q50_idx = np.searchsorted(cumulative_hist, total_pixels_band * 0.50)
        q75_idx = np.searchsorted(cumulative_hist, total_pixels_band * 0.75)
        
        global_quantiles[band_idx] = {
            'q25': int(q25_idx),
            'q50': int(q50_idx), 
            'q75': int(q75_idx)
        }
        
        logger.info(f"    {['Rouge', 'Vert', 'Bleu'][band_idx]}: Q25={q25_idx}, Q50={q50_idx}, Q75={q75_idx}")
    
    logger.info("  ‚úÖ Quantiles globaux calcul√©s avec succ√®s")
    return global_quantiles

def equalize_zone_to_global_quantiles(zone_ortho_path, global_quantiles, logger):
    """
    √âGALISATION GLOBALE : √âgalise une zone vers les quantiles globaux de r√©f√©rence
    PR√âSERVE LA G√âOSPATIALIT√â (CRS + g√©otransformation)
    
    Args:
        zone_ortho_path: Chemin vers l'ortho de zone √† √©galiser
        global_quantiles: Quantiles globaux de r√©f√©rence
        logger: Logger pour les messages
    
    Returns:
        str: Chemin vers la zone √©galis√©e
    """
    import numpy as np
    import os
    import rasterio
    
    logger.info(f"üé® √âGALISATION GLOBALE DE LA ZONE : {os.path.basename(zone_ortho_path)}")
    
    try:
        # √âTAPE 1 : Lire l'image avec rasterio pour pr√©server la g√©ospatialit√©
        logger.info(f"  üìñ Lecture de la zone avec rasterio...")
        with rasterio.open(zone_ortho_path) as src:
            zone_image = src.read()  # (3, H, W)
            height, width = src.height, src.width
            crs = src.crs
            transform = src.transform
            bounds = src.bounds
            
            logger.info(f"  üìè Dimensions : {width} √ó {height} pixels")
            logger.info(f"  üåç CRS : {crs}")
            logger.info(f"  üìç Bounds : {bounds}")
        
        # Convertir en format (H, W, 3) pour le traitement
        zone_image_rgb = zone_image.transpose(1, 2, 0)  # (H, W, 3)
        
        # √âTAPE 2 : Cr√©er un masque pour exclure UNIQUEMENT les pixels noirs (0,0,0)
        logger.info(f"  üîç Cr√©ation du masque des pixels valides...")
        black_pixels_mask = np.all(zone_image_rgb == [0, 0, 0], axis=2)
        valid_pixels_mask = ~black_pixels_mask
        
        valid_pixels_count = np.sum(valid_pixels_mask)
        total_pixels = height * width
        
        logger.info(f"  üîç Pixels valides : {valid_pixels_count}/{total_pixels} ({valid_pixels_count/total_pixels*100:.1f}%)")
        
        if valid_pixels_count == 0:
            logger.warning(f"  ‚ö†Ô∏è Aucun pixel valide trouv√© dans la zone")
            return zone_ortho_path
        
        # √âTAPE 3 : √âgalisation vers les quantiles globaux
        logger.info(f"  üîß Application de l'√©galisation vers les quantiles globaux...")
        
        # Cr√©er une image de sortie
        equalized_image = np.zeros_like(zone_image_rgb)
        
        # √âgalisation par bande vers les quantiles globaux
        equalization_factors = []
        
        for band_idx in range(3):
            band = zone_image_rgb[:, :, band_idx]
            valid_band_values = band[valid_pixels_mask]
            
            if len(valid_band_values) == 0:
                equalized_image[:, :, band_idx] = band
                equalization_factors.append(1.0)
                continue
            
            # Calculer les quantiles actuels de la zone
            current_q25 = np.percentile(valid_band_values, 25)
            current_q50 = np.percentile(valid_band_values, 50)
            current_q75 = np.percentile(valid_band_values, 75)
            
            # Quantiles globaux de r√©f√©rence
            target_q25 = global_quantiles[band_idx]['q25']
            target_q50 = global_quantiles[band_idx]['q50']
            target_q75 = global_quantiles[band_idx]['q75']
            
            logger.info(f"    {['Rouge', 'Vert', 'Bleu'][band_idx]}:")
            logger.info(f"      Actuel: Q25={current_q25:.1f}, Q50={current_q50:.1f}, Q75={current_q75:.1f}")
            logger.info(f"      Cible: Q25={target_q25}, Q50={target_q50}, Q75={target_q75}")
            
            # √âTAPE 4 : √âgalisation par transformation de quantiles
            # Utiliser une transformation lin√©aire bas√©e sur les quantiles
            if current_q75 > current_q25:  # √âviter division par z√©ro
                # Transformation lin√©aire : (x - q25) / (q75 - q25) * (target_q75 - target_q25) + target_q25
                scale_factor = (target_q75 - target_q25) / (current_q75 - current_q25)
                offset = target_q25 - current_q25 * scale_factor
                
                # Appliquer la transformation
                equalized_band = np.clip(band * scale_factor + offset, 0, 255).astype(np.uint8)
                
                # Calculer le facteur d'√©galisation pour les logs
                factor = scale_factor
            else:
                # Si pas de variation, appliquer un d√©calage simple
                offset = target_q50 - current_q50
                equalized_band = np.clip(band + offset, 0, 255).astype(np.uint8)
                factor = 1.0
            
            equalized_image[:, :, band_idx] = equalized_band
            equalization_factors.append(factor)
        
        logger.info(f"  üìä Facteurs d'√©galisation: R={equalization_factors[0]:.3f}, G={equalization_factors[1]:.3f}, B={equalization_factors[2]:.3f}")
        
        # √âTAPE 5 : Sauvegarder avec rasterio pour pr√©server la g√©ospatialit√©
        logger.info(f"  üíæ Sauvegarde avec rasterio (g√©ospatialit√© pr√©serv√©e)...")
        
        # Cr√©er le nom de fichier de sortie
        base_name = os.path.splitext(os.path.basename(zone_ortho_path))[0]
        output_path = zone_ortho_path.replace(
            base_name, 
            f"{base_name}_equalized_global_quantiles"
        )
        
        # Sauvegarder avec rasterio en pr√©servant CRS et g√©otransformation
        with rasterio.open(
            output_path,
            'w',
            driver='GTiff',
            height=height,
            width=width,
            count=3,
            dtype=np.uint8,
            crs=crs,  # ‚úÖ PR√âSERVE LE CRS
            transform=transform,  # ‚úÖ PR√âSERVE LA G√âOTRANSFORMATION
            photometric='rgb'
        ) as dst:
            # √âcrire les 3 bandes RGB (format rasterio : (3, H, W))
            equalized_image_rasterio = equalized_image.transpose(2, 0, 1)  # (H, W, 3) ‚Üí (3, H, W)
            dst.write(equalized_image_rasterio)
            
            # M√©tadonn√©es
            dst.update_tags(
                Software='PhotoGeoAlign Global Quantile Equalization',
                Method='Global quantile transformation',
                Global_Q25=f"{[global_quantiles[i]['q25'] for i in range(3)]}",
                Global_Q50=f"{[global_quantiles[i]['q50'] for i in range(3)]}",
                Global_Q75=f"{[global_quantiles[i]['q75'] for i in range(3)]}",
                Equalization_Factors=f"{equalization_factors}",
                Pixels_Excluded=str(total_pixels - valid_pixels_count)
            )
        
        logger.info(f"  ‚úÖ Zone √©galis√©e sauvegard√©e : {os.path.basename(output_path)}")
        logger.info(f"  üìã M√©tadonn√©es d'√©galisation:")
        logger.info(f"    - M√©thode: √âgalisation vers quantiles globaux")
        logger.info(f"    - Pixels exclus: {total_pixels - valid_pixels_count} (noirs [0,0,0])")
        logger.info(f"    - Facteurs R/G/B: {equalization_factors}")
        logger.info(f"    - G√©ospatialit√©: CRS et g√©otransformation pr√©serv√©s ‚úÖ")
        
        return output_path
        
    except Exception as e:
        logger.error(f"  ‚ùå Erreur lors de l'√©galisation de la zone : {e}")
        return None

def individual_zone_equalization(zone_ortho_path, logger):
    """
    √âGALISATION GLOBALE : Interface principale pour l'√©galisation des zones
    Utilise la nouvelle strat√©gie d'√©galisation globale bas√©e sur les quantiles
    
    Args:
        zone_ortho_path: Chemin vers l'ortho de zone √† √©galiser
        logger: Logger pour les messages
    
    Returns:
        str: Chemin vers la zone √©galis√©e
    """
    # Cette fonction est maintenant une interface qui appelle l'√©galisation globale
    # Elle sera remplac√©e par l'appel direct dans le pipeline principal
    logger.warning("‚ö†Ô∏è Cette fonction est obsol√®te. Utilisez l'√©galisation globale directement.")
    return zone_ortho_path

def simple_mnt_assembly(zones_output_dir, final_resolution, logger):
    """
    ASSEMBLAGE SIMPLE DES MNT : Place les MNT de zones c√¥te √† c√¥te sans fusion
    Utilise la m√™me logique de g√©or√©f√©rencement que simple_ortho_assembly
    
    Args:
        zones_output_dir: R√©pertoire contenant les zones avec leurs MNT
        final_resolution: R√©solution finale en m√®tres par pixel (si None, d√©tection automatique)
        logger: Logger pour les messages
    
    Returns:
        str: Chemin vers le MNT unifi√© final
    """
    import os
    import numpy as np
    import rasterio
    import re
    from rasterio.coords import BoundingBox
    from rasterio.transform import Affine
    
    logger.info("üîß ASSEMBLAGE SIMPLE DES MNT DE ZONES (pas de fusion)")
    logger.info(f"üìÅ R√©pertoire des zones : {zones_output_dir}")
    
    # üîß CORRECTION : D√©tecter automatiquement la r√©solution des MNT unitaires
    if final_resolution is None:
        logger.info("üîç D√©tection automatique de la r√©solution des MNT unitaires...")
        # Lire le premier MNT pour obtenir sa r√©solution
        first_mnt = None
        for file in os.listdir(zones_output_dir):
            if file.endswith('_fused_height.tif'):
                first_mnt = os.path.join(zones_output_dir, file)
                break
        
        if first_mnt:
            with rasterio.open(first_mnt) as src:
                # Calculer la r√©solution √† partir de la transformation affine
                transform = src.transform
                # La r√©solution est la valeur absolue de a (largeur) et e (hauteur) de la transformation
                pixel_width = abs(transform.a)
                pixel_height = abs(transform.e)
                # Prendre la moyenne des deux r√©solutions
                final_resolution = (pixel_width + pixel_height) / 2
                logger.info(f"  ‚úÖ R√©solution d√©tect√©e : {final_resolution:.6f}m/pixel")
                logger.info(f"  üìè Largeur pixel : {pixel_width:.6f}m, Hauteur pixel : {pixel_height:.6f}m")
        else:
            logger.warning("‚ö†Ô∏è Aucun MNT trouv√©, utilisation de la r√©solution par d√©faut 0.1m")
            final_resolution = 0.1
    
    logger.info(f"üìè R√©solution finale utilis√©e : {final_resolution}m")
    
    # √âTAPE 1 : Lire les MNT de zones
    logger.info("üìñ Lecture des MNT de zones...")
    
    zone_mnt_files = []
    for file in os.listdir(zones_output_dir):
        if file.endswith('_fused_height.tif'):
            file_path = os.path.join(zones_output_dir, file)
            try:
                with rasterio.open(file_path) as src:
                    # Extraire les informations de la zone
                    zone_bounds = src.bounds
                    zone_data = {
                        'file_path': file_path,
                        'bounds': zone_bounds,
                        'width': src.width,
                        'height': src.height,
                        'crs': src.crs,
                        'transform': src.transform
                    }
                    
                    # Extraire l'ID de zone du nom de fichier
                    zone_id_match = re.search(r'zone_(\d+)_', file)
                    if zone_id_match:
                        zone_data['zone_id'] = int(zone_id_match.group(1))
                    else:
                        zone_data['zone_id'] = len(zone_mnt_files)
                    
                    zone_mnt_files.append(zone_data)
                    logger.info(f"  ‚úÖ {file}: {src.width}√ó{src.height} pixels, CRS: {src.crs}")
                    
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Erreur lors de la lecture de {file}: {e}")
                continue
    
    if not zone_mnt_files:
        logger.error("‚ùå Aucun MNT de zone trouv√© !")
        return None
    
    logger.info(f"üìä MNT de zones trouv√©s : {len(zone_mnt_files)}")
    
    # √âTAPE 2 : Calculer les bornes globales
    logger.info("üåç Calcul des bornes globales...")
    
    # Initialiser avec la premi√®re zone
    global_bounds = zone_mnt_files[0]['bounds']
    
    for zone_data in zone_mnt_files[1:]:
        zone_bounds = zone_data['bounds']
        global_bounds = BoundingBox(
            left=min(global_bounds.left, zone_bounds.left),
            bottom=min(global_bounds.bottom, zone_bounds.bottom),
            right=max(global_bounds.right, zone_bounds.right),
            top=max(global_bounds.top, zone_bounds.top)
        )
    
    logger.info(f"üìè Bornes globales : {global_bounds}")
    
    # √âTAPE 3 : Calculer les dimensions de la grille finale
    logger.info("üìê Calcul des dimensions de la grille finale...")
    
    # Calculer la taille en pixels (arrondir pour √©viter les d√©calages)
    final_width = round((global_bounds.right - global_bounds.left) / final_resolution)
    final_height = round((global_bounds.top - global_bounds.bottom) / final_resolution)
    
    logger.info(f"üìè Grille finale : {final_width} √ó {final_height} pixels")
    logger.info(f"üìè √âtendue : {(global_bounds.right - global_bounds.left):.3f}m √ó {(global_bounds.top - global_bounds.bottom):.3f}m")
    
    # Cr√©er la transformation affine finale
    # IMPORTANT : Pour les MNT, on inverse l'axe Y car rasterio attend que Y augmente vers le bas
    # mais les coordonn√©es g√©ographiques augmentent vers le haut
    final_transform = Affine.translation(global_bounds.left, global_bounds.top) * Affine.scale(final_resolution, -final_resolution)
    
    # √âTAPE 4 : Cr√©er la grille finale et placer les MNT
    logger.info("üîß Placement des MNT de zones dans la grille finale...")
    
    # Grille finale pour les MNT (1 bande, donn√©es float32)
    final_mnt = np.full((final_height, final_width), np.nan, dtype=np.float32)
    
    zones_placed = 0
    
    for zone_data in zone_mnt_files:
        try:
            logger.info(f"  üîÑ Placement Zone {zone_data['zone_id']}...")
            
            # Lire le MNT de la zone
            with rasterio.open(zone_data['file_path']) as src:
                zone_mnt = src.read(1)  # 1 bande de hauteur
            
            zone_bounds = zone_data['bounds']
            
            # Calculer la position dans la grille finale (m√™me logique que pour les orthos)
            x_pos = (zone_bounds.left - global_bounds.left) / final_resolution
            y_pos = (global_bounds.top - zone_bounds.top) / final_resolution
            
            # Arrondir pour √©viter les d√©calages de pixels
            start_x = round(x_pos)
            start_y = round(y_pos)
            end_x = start_x + zone_data['width']
            end_y = start_y + zone_data['height']
            
            # DEBUG : V√©rifier les arrondis de position
            logger.info(f"    üîç DEBUG Position calcul√©e:")
            logger.info(f"      x_pos brute: {x_pos:.6f} -> arrondie: {start_x}")
            logger.info(f"      y_pos brute: {y_pos:.6f} -> arrondie: {start_y}")
            logger.info(f"      d√©calage x: {abs(x_pos - start_x):.6f} pixels")
            logger.info(f"      d√©calage y: {abs(y_pos - start_y):.6f} pixels")
            
            # DEBUG : Afficher les calculs d√©taill√©s pour v√©rifier l'alignement
            logger.info(f"    üîç DEBUG Alignement Y:")
            logger.info(f"      global_bounds.top: {global_bounds.top:.6f}m")
            logger.info(f"      zone_bounds.top: {zone_bounds.top:.6f}m")
            logger.info(f"      diff_y: {global_bounds.top - zone_bounds.top:.6f}m")
            logger.info(f"      start_y pixels: {start_y}")
            logger.info(f"      zone height: {zone_data['height']} pixels")
            
            logger.info(f"    üìç Position dans la grille : ({start_x}, {start_y}) √† ({end_x}, {end_y})")
            
            # V√©rifier les limites
            if (start_x < 0 or start_y < 0 or 
                end_x > final_width or end_y > final_height):
                logger.warning(f"    ‚ö†Ô∏è Zone {zone_data['zone_id']} d√©passe les limites de la grille finale")
                continue
            
            # Placer le MNT de la zone dans la grille finale
            final_mnt[start_y:end_y, start_x:end_x] = zone_mnt
            
            zones_placed += 1
            logger.info(f"    ‚úÖ Zone {zone_data['zone_id']} plac√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"    ‚ùå Erreur lors du placement de la Zone {zone_data['zone_id']}: {e}")
            continue
    
    logger.info(f"üìä Zones plac√©es : {zones_placed}/{len(zone_mnt_files)}")
    
    # √âTAPE 5 : Sauvegarder le MNT unifi√© final
    logger.info("üíæ Sauvegarde du MNT unifi√© final...")
    
    output_path = os.path.join(zones_output_dir, "mnt_unified_final.tif")
    
    # R√©cup√©rer le CRS de la premi√®re zone (toutes devraient avoir le m√™me)
    reference_crs = zone_mnt_files[0]['crs']
    
    with rasterio.open(
        output_path,
        'w',
        driver='GTiff',
        height=final_height,
        width=final_width,
        count=1,  # 1 bande pour les MNT
        dtype=np.float32,  # Donn√©es de hauteur en float32
        crs=reference_crs,
        transform=final_transform,
        nodata=np.nan  # Valeur nodata pour les pixels sans donn√©es
    ) as dst:
        # √âcrire la bande de hauteur
        dst.write(final_mnt, 1)
        
        # M√©tadonn√©es
        dst.update_tags(
            Software='PhotoGeoAlign Simple MNT Assembly',
            Resolution=f'{final_resolution}m per pixel',
            Zones_Processed=str(zones_placed),
            Assembly_Method='Simple placement without fusion',
            Data_Type='Height values in meters',
            Nodata_Value='NaN'
        )
    
    logger.info(f"üéâ MNT UNIFI√â CR√â√â : {output_path}")
    logger.info(f"   üìè Dimensions : {final_width} √ó {final_height} pixels")
    logger.info(f"   üìè √âtendue : {(global_bounds.right - global_bounds.left):.3f}m √ó {(global_bounds.top - global_bounds.bottom):.3f}m")
    logger.info(f"   üéØ Zones assembl√©es : {zones_placed}")
    
    # Statistiques sur les donn√©es
    valid_pixels = np.sum(~np.isnan(final_mnt))
    if valid_pixels > 0:
        height_min = np.nanmin(final_mnt)
        height_max = np.nanmax(final_mnt)
        logger.info(f"   üìä Hauteurs : {height_min:.3f}m √† {height_max:.3f}m")
        logger.info(f"   üìä Pixels valides : {valid_pixels}/{final_width * final_height}")
    
    return output_path

 

