def add_offset_to_clouds(input_dir, logger, coord_file=None, extra_params="", max_workers=None):
    """Ajoute l'offset aux nuages de points .ply dans le dossier fourni (et ses sous-dossiers √©ventuels)"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"Ajout de l'offset aux nuages dans {abs_input_dir} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    if not coord_file:
        logger.error("Aucun fichier de coordonn√©es fourni pour l'ajout d'offset.")
        raise RuntimeError("Aucun fichier de coordonn√©es fourni pour l'ajout d'offset.")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    output_dir = os.path.join(os.path.dirname(abs_input_dir), "offset_step")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # Lecture du fichier de coordonn√©es pour extraire l'offset
    offset = None
    try:
        with open(coord_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line.startswith('#Offset to add :'):
                    # Format: #Offset to add : X Y Z
                    parts = line.split(':')[1].strip().split()
                    if len(parts) == 3:
                        offset = [float(parts[0]), float(parts[1]), float(parts[2])]
                        break
        
        if offset is None:
            logger.error("Offset non trouv√© dans le fichier de coordonn√©es.")
            raise RuntimeError("Offset non trouv√© dans le fichier de coordonn√©es.")
        
        logger.info(f"Offset extrait : {offset}")
        
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        logger.info("Open3D import√© avec succ√®s")
    except ImportError:
        logger.error("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
        raise RuntimeError("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
    
    total_files_processed = 0
    
    # Recherche des fichiers .ply dans le dossier fourni (et sous-dossiers)
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))  # Maximum 10 processus par d√©faut
    else:
        # Limiter par CPU disponibles ET fichiers pour √©viter surcharge cluster
        max_workers = min(max_workers, cpu_count(), len(ply_files))
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, output_dir, coord_file, extra_params))
    
    # Traitement parall√®le
    total_files_processed = 0
    failed_files = []
    
    try:
        with Pool(processes=max_workers) as pool:
            # Lancement du traitement parall√®le
            results = pool.map(process_single_cloud_add_offset, process_args)
            
            # Analyse des r√©sultats
            for i, (success, message) in enumerate(results):
                if success:
                    logger.info(f"‚úÖ {message}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {message}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel en cas d'erreur
        logger.info("Tentative de traitement s√©quentiel...")
        total_files_processed = 0
        for ply_file in ply_files:
            try:
                result = process_single_cloud_add_offset((ply_file, output_dir, coord_file, extra_params))
                if result[0]:
                    logger.info(f"‚úÖ {result[1]}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    # R√©sum√© final
    if failed_files:
        logger.warning(f"‚ö†Ô∏è {len(failed_files)} fichiers n'ont pas pu √™tre trait√©s")
    logger.info(f"Ajout d'offset termin√©. {total_files_processed} fichiers trait√©s dans {output_dir}.")

def convert_itrf_to_enu(input_dir, logger, coord_file=None, extra_params="", ref_point_name=None, max_workers=None, global_ref_point=None, force_global_ref=False):
    # DEBUG : V√©rification des param√®tres re√ßus
    logger.info(f"üîç DEBUG - Param√®tres re√ßus dans convert_itrf_to_enu:")
    logger.info(f"   global_ref_point: {global_ref_point}")
    logger.info(f"   force_global_ref: {force_global_ref}")
    logger.info(f"   ref_point_name: {ref_point_name}")
    """Convertit les nuages de points d'ITRF vers ENU"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"Conversion ITRF vers ENU dans {abs_input_dir} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    if not coord_file:
        logger.error("Aucun fichier de coordonn√©es fourni pour la conversion ITRF‚ÜíENU.")
        raise RuntimeError("Aucun fichier de coordonn√©es fourni pour la conversion ITRF‚ÜíENU.")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    output_dir = os.path.join(os.path.dirname(abs_input_dir), "itrf_to_enu_step")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # √âTAPE 1 : Lecture du fichier de coordonn√©es pour obtenir le point de r√©f√©rence
    logger.info(f"Lecture du fichier de coordonn√©es : {coord_file}")
    try:
        with open(coord_file, 'r') as f:
            lines = f.readlines()
        
        # Recherche du point de r√©f√©rence
        ref_point = None
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split()
                if len(parts) >= 4:  # Format: NOM X Y Z
                    try:
                        point_name = parts[0]
                        x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                        
                        # Si un nom de point de r√©f√©rence est sp√©cifi√©, on cherche ce point
                        if ref_point_name and point_name == ref_point_name:
                            ref_point = np.array([x, y, z])
                            logger.info(f"Point de r√©f√©rence sp√©cifi√© trouv√© : {point_name} ({x:.6f}, {y:.6f}, {z:.6f})")
                            break
                        # Sinon, on prend le premier point valide
                        elif ref_point is None:
                            ref_point = np.array([x, y, z])
                            logger.info(f"Point de r√©f√©rence trouv√© : {point_name} ({x:.6f}, {y:.6f}, {z:.6f})")
                            break
                    except ValueError:
                        continue
        
        if ref_point is None:
            if ref_point_name:
                raise RuntimeError(f"Point de r√©f√©rence '{ref_point_name}' non trouv√© dans le fichier de coordonn√©es")
            else:
                raise RuntimeError("Aucun point de r√©f√©rence valide trouv√© dans le fichier de coordonn√©es")
        
        # √âTAPE 1.5 : Priorisation du point de r√©f√©rence global si forc√©
        if force_global_ref and global_ref_point is not None:
            logger.info("üéØ UTILISATION DU POINT DE R√âF√âRENCE GLOBAL FORC√â")
            logger.info(f"Point global : ({global_ref_point[0]:.6f}, {global_ref_point[1]:.6f}, {global_ref_point[2]:.6f})")
            ref_point = np.array(global_ref_point)
            logger.info("Le point global remplace le point local pour unifier le rep√®re ENU")
            logger.info("‚ö†Ô∏è  L'offset ne s'applique PAS au point global (coordonn√©es absolues pr√©serv√©es)")
        else:
            logger.info("üìç UTILISATION DU POINT DE R√âF√âRENCE LOCAL")
            logger.info(f"Point local : ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
            
            # √âTAPE 1.6 : Lecture de l'offset et application au point local UNIQUEMENT
            logger.info("Lecture de l'offset depuis le fichier de coordonn√©es...")
            offset = None
            try:
                with open(coord_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line.startswith('#Offset to add :'):
                            # Format: #Offset to add : X Y Z
                            parts = line.split(':')[1].strip().split()
                            if len(parts) == 3:
                                offset = [float(parts[0]), float(parts[1]), float(parts[2])]
                                break
                
                if offset is None:
                    logger.warning("Offset non trouv√© dans le fichier de coordonn√©es. Utilisation du point de r√©f√©rence sans offset.")
                else:
                    logger.info(f"Offset trouv√© : {offset}")
                    # Application de l'offset au point local UNIQUEMENT
                    ref_point = ref_point + np.array(offset)
                    logger.info(f"Point local avec offset appliqu√© : ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
                    
            except Exception as e:
                logger.warning(f"Erreur lors de la lecture de l'offset : {e}. Utilisation du point de r√©f√©rence sans offset.")
            
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
    
    # √âTAPE 2 : Configuration de la transformation topocentrique
    logger.info("Configuration de la transformation topocentrique ITRF2020‚ÜíENU...")
    
    try:
        import pyproj
        
        # Le point de r√©f√©rence (ref_point) est le centre de la transformation topocentrique
        # Il d√©finit l'origine du syst√®me ENU local
        tr_center = ref_point  # [X, Y, Z] en ITRF2020
        tr_ellps = "GRS80"     # Ellipso√Øde de r√©f√©rence (standard pour ITRF)
        
        logger.info(f"Centre de transformation topocentrique : ({tr_center[0]:.3f}, {tr_center[1]:.3f}, {tr_center[2]:.3f})")
        logger.info(f"Ellipso√Øde de r√©f√©rence : {tr_ellps}")
        
        # Cr√©ation du pipeline de transformation topocentrique
        # +proj=topocentric : projection topocentrique
        # +X_0, +Y_0, +Z_0 : coordonn√©es du centre de transformation en ITRF
        # +ellps : ellipso√Øde de r√©f√©rence
        pipeline = "+proj=topocentric +X_0={0} +Y_0={1} +Z_0={2} +ellps={3}".format(
            tr_center[0], tr_center[1], tr_center[2], tr_ellps
        )
        
        transformer = pyproj.Transformer.from_pipeline(pipeline)
        logger.info(f"Pipeline de transformation cr√©√© : {pipeline}")
        
    except ImportError:
        logger.error("pyproj n'est pas install√©. La transformation topocentrique n√©cessite pyproj.")
        raise RuntimeError("pyproj n'est pas install√©. Veuillez l'installer avec: pip install pyproj")
    
    logger.info("Transformation topocentrique configur√©e avec succ√®s")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        logger.info("Open3D import√© avec succ√®s")
    except ImportError:
        logger.error("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
        raise RuntimeError("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
    
    # √âTAPE 4 : Traitement des nuages de points
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.lower().endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))  # Maximum 10 processus par d√©faut
    else:
        # Limiter par CPU disponibles ET fichiers pour √©viter surcharge cluster
        max_workers = min(max_workers, cpu_count(), len(ply_files))
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, output_dir, coord_file, extra_params, ref_point_name))
    
    # Traitement parall√®le
    total_files_processed = 0
    failed_files = []
    
    try:
        # DEBUG : V√©rification avant l'appel parall√®le
        logger.info(f"üîç DEBUG - Pr√©paration de l'appel √† process_single_cloud_itrf_to_enu:")
        logger.info(f"   Nombre de fichiers PLY: {len(ply_files)}")
        logger.info(f"   Max workers: {max_workers}")
        logger.info(f"   Centre de transformation: ({tr_center[0]:.6f}, {tr_center[1]:.6f}, {tr_center[2]:.6f})")
        logger.info(f"   Premier fichier: {os.path.basename(ply_files[0]) if ply_files else 'Aucun'}")
        
        # Traitement parall√®le avec Pool
        with Pool(processes=max_workers) as pool:
            # Lancement du traitement parall√®le
            # Ajout des param√®tres du point global aux arguments
            extended_process_args = [args + (global_ref_point, force_global_ref) for args in process_args]
            results = pool.map(process_single_cloud_itrf_to_enu, extended_process_args)
            
            # Analyse des r√©sultats
            for i, (success, message) in enumerate(results):
                if success:
                    logger.info(f"‚úÖ {message}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {message}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel en cas d'erreur
        logger.info("Tentative de traitement s√©quentiel...")
        total_files_processed = 0
        for ply_file in ply_files:
            try:
                result = process_single_cloud_itrf_to_enu((ply_file, output_dir, coord_file, extra_params, ref_point_name))
                if result[0]:
                    logger.info(f"‚úÖ {result[1]}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    # R√©sum√© final
    if failed_files:
        logger.warning(f"‚ö†Ô∏è {len(failed_files)} fichiers n'ont pas pu √™tre trait√©s")
    logger.info(f"Conversion ITRF vers ENU termin√©e. {total_files_processed} fichiers trait√©s dans {output_dir}.")

def deform_clouds(input_dir, logger, deformation_type="lineaire", deformation_params="", extra_params="", bascule_xml_file=None, coord_file=None, max_workers=None, global_ref_point=None, force_global_ref=False):
    """Applique une d√©formation aux nuages de points bas√©e sur les r√©sidus GCPBascule"""
    abs_input_dir = os.path.abspath(input_dir)
    logger.info(f"D√©formation des nuages dans {abs_input_dir} avec le type {deformation_type} ...")
    
    # V√©rification de l'existence du dossier d'entr√©e
    if not os.path.exists(abs_input_dir):
        logger.error(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
        raise RuntimeError(f"Le dossier d'entr√©e n'existe pas : {abs_input_dir}")
    
    if not os.path.isdir(abs_input_dir):
        logger.error(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
        raise RuntimeError(f"Le chemin sp√©cifi√© n'est pas un dossier : {abs_input_dir}")
    
    # Import d'open3d pour g√©rer les fichiers PLY
    try:
        import open3d as o3d
        logger.info("Open3D import√© avec succ√®s")
    except ImportError:
        logger.error("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
        raise RuntimeError("Open3D n'est pas install√©. Veuillez l'installer avec: pip install open3d")
    
    # Cr√©ation du dossier de sortie pour cette √©tape
    output_dir = os.path.join(os.path.dirname(abs_input_dir), f"deform_{deformation_type}_step")
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Dossier de sortie cr√©√© : {output_dir}")
    
    # √âTAPE 1 : Lecture des r√©sidus depuis le fichier XML GCPBascule
    logger.info("Lecture des r√©sidus GCPBascule...")
    
    if not bascule_xml_file:
        logger.error("Aucun fichier XML GCPBascule sp√©cifi√©")
        raise RuntimeError("Aucun fichier XML GCPBascule sp√©cifi√©. Utilisez le param√®tre bascule_xml_file.")
    
    if not os.path.exists(bascule_xml_file):
        logger.error(f"Fichier XML GCPBascule introuvable : {bascule_xml_file}")
        raise RuntimeError(f"Fichier XML GCPBascule introuvable : {bascule_xml_file}")
    
    xml_file = bascule_xml_file
    logger.info(f"Fichier XML GCPBascule : {xml_file}")
    
    # Lecture et parsing du XML
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        # Extraction des r√©sidus
        residues = {}
        for residue in root.findall('.//Residus'):
            name = residue.find('Name').text
            offset_elem = residue.find('Offset')
            offset_text = offset_elem.text.strip().split()
            offset = [float(x) for x in offset_text]
            dist = float(residue.find('Dist').text)
            
            residues[name] = {
                'offset': np.array(offset),  # R√©sidu en ITRF
                'distance': dist
            }
            logger.info(f"R√©sidu {name}: offset={offset}, distance={dist:.3f}m")
        
        if not residues:
            logger.error("Aucun r√©sidu trouv√© dans le fichier XML")
            raise RuntimeError("Aucun r√©sidu trouv√© dans le fichier XML GCPBascule")
            
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier XML : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier XML : {e}")
    
    logger.info(f"Lecture termin√©e : {len(residues)} r√©sidus trouv√©s")
    
    # √âTAPE 1.5 : Conversion des r√©sidus ITRF vers ENU
    logger.info("Conversion des r√©sidus ITRF vers ENU...")
    
    # Nous avons besoin du point de r√©f√©rence pour la transformation topocentrique
    # Nous devons le lire depuis le fichier de coordonn√©es
    if not coord_file:
        logger.error("Fichier de coordonn√©es requis pour la conversion des r√©sidus ITRF‚ÜíENU")
        raise RuntimeError("Fichier de coordonn√©es requis pour la conversion des r√©sidus ITRF‚ÜíENU")
    
    if not os.path.exists(coord_file):
        logger.error(f"Fichier de coordonn√©es introuvable : {coord_file}")
        raise RuntimeError(f"Fichier de coordonn√©es introuvable : {coord_file}")
    
    # Lecture du point de r√©f√©rence depuis le fichier de coordonn√©es
    try:
        with open(coord_file, 'r') as f:
            lines = f.readlines()
        
        # √âTAPE 1.5 : Priorisation du point de r√©f√©rence global si forc√©
        if force_global_ref and global_ref_point is not None:
            logger.info("üéØ UTILISATION DU POINT DE R√âF√âRENCE GLOBAL FORC√â (d√©formation)")
            logger.info(f"Point global : ({global_ref_point[0]:.6f}, {global_ref_point[1]:.6f}, {global_ref_point[2]:.6f})")
            ref_point = np.array(global_ref_point)
            logger.info("Le point global remplace le point local pour la d√©formation")
            logger.info("‚ö†Ô∏è  L'offset ne s'applique PAS au point global (coordonn√©es absolues pr√©serv√©es)")
        else:
            logger.info("üìç UTILISATION DU POINT DE R√âF√âRENCE LOCAL (d√©formation)")
            
            # Recherche du point de r√©f√©rence (premier point par d√©faut)
            ref_point = None
            offset = None
            
            for line in lines:
                line = line.strip()
                if line.startswith('#F=') or line.startswith('#'):
                    continue
                
                # Format attendu : NOM X Y Z
                parts = line.split()
                if len(parts) >= 4:
                    try:
                        x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                        ref_point = np.array([x, y, z])
                        logger.info(f"Point local trouv√© : {parts[0]} ({x:.6f}, {y:.6f}, {z:.6f})")
                        break
                    except ValueError:
                        continue
            
            if ref_point is None:
                logger.error("Aucun point de r√©f√©rence valide trouv√© dans le fichier de coordonn√©es")
                raise RuntimeError("Aucun point de r√©f√©rence valide trouv√© dans le fichier de coordonn√©es")
            
            # Lecture de l'offset et application au point local UNIQUEMENT
            try:
                for line in lines:
                    if line.startswith('#Offset to add :'):
                        offset_text = line.replace('#Offset to add :', '').strip()
                        offset = [float(x) for x in offset_text.split()]
                        logger.info(f"Offset trouv√© : {offset}")
                        break
            except Exception as e:
                logger.warning(f"Erreur lors de la lecture de l'offset : {e}. Utilisation du point de r√©f√©rence sans offset.")
                offset = [0.0, 0.0, 0.0]
            
            # Application de l'offset au point local UNIQUEMENT
            if offset:
                ref_point = ref_point + np.array(offset)
                logger.info(f"Point local avec offset appliqu√© : ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
        
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
        raise RuntimeError(f"Erreur lors de la lecture du fichier de coordonn√©es : {e}")
    
    # Configuration de la transformation topocentrique pour les r√©sidus
    try:
        import pyproj
        
        # Le point de r√©f√©rence (ref_point) est le centre de la transformation topocentrique
        tr_center = ref_point  # [X, Y, Z] en ITRF2020
        tr_ellps = "GRS80"     # Ellipso√Øde de r√©f√©rence (standard pour ITRF)
        
        logger.info(f"Centre de transformation pour r√©sidus : ({tr_center[0]:.3f}, {tr_center[1]:.3f}, {tr_center[2]:.3f})")
        
        # Cr√©ation du pipeline de transformation topocentrique
        pipeline = "+proj=topocentric +X_0={0} +Y_0={1} +Z_0={2} +ellps={3}".format(
            tr_center[0], tr_center[1], tr_center[2], tr_ellps
        )
        
        transformer = pyproj.Transformer.from_pipeline(pipeline)
        logger.info(f"Pipeline de transformation pour r√©sidus cr√©√© : {pipeline}")
        
    except ImportError:
        logger.error("pyproj n'est pas install√©. Veuillez l'installer avec: pip install pyproj")
        raise RuntimeError("pyproj n'est pas install√©. Veuillez l'installer avec: pip install pyproj")
    except Exception as e:
        logger.error(f"Erreur lors de la configuration de la transformation : {e}")
        raise RuntimeError(f"Erreur lors de la configuration de la transformation : {e}")
    
    # Conversion des r√©sidus ITRF vers ENU
    residues_enu = {}
    for name, residue_data in residues.items():
        # Le r√©sidu est un vecteur de d√©placement en ITRF
        itrf_offset = residue_data['offset']
        
        # Conversion du vecteur de d√©placement ITRF vers ENU
        # Pour un vecteur de d√©placement, nous utilisons la matrice de rotation
        # qui est la d√©riv√©e de la transformation topocentrique
        
        # M√©thode 1 : Utilisation de la matrice de rotation
        # La transformation topocentrique a une matrice de rotation R
        # Pour un vecteur de d√©placement : ENU_vector = R * ITRF_vector
        
        # Calcul de la matrice de rotation (approximation)
        # Pour un point proche du centre de transformation, la rotation est approximativement constante
        
        # M√©thode 2 : Transformation directe (plus pr√©cise)
        # Nous transformons le point de r√©f√©rence + le vecteur de d√©placement
        point_with_offset = ref_point + itrf_offset
        enu_point = transformer.transform(point_with_offset[0], point_with_offset[1], point_with_offset[2])
        enu_ref = transformer.transform(ref_point[0], ref_point[1], ref_point[2])
        
        # Le vecteur de d√©placement ENU est la diff√©rence
        enu_offset = np.array([enu_point[0] - enu_ref[0], 
                              enu_point[1] - enu_ref[1], 
                              enu_point[2] - enu_ref[2]])
        
        # DEBUG : V√©rification des coordonn√©es de transformation
        logger.info(f"üîç DEBUG - Transformation r√©sidu {name}:")
        logger.info(f"   Point de r√©f√©rence ITRF: ({ref_point[0]:.6f}, {ref_point[1]:.6f}, {ref_point[2]:.6f})")
        logger.info(f"   Point avec offset ITRF: ({point_with_offset[0]:.6f}, {point_with_offset[1]:.6f}, {point_with_offset[2]:.6f})")
        logger.info(f"   Point de r√©f√©rence ENU: ({enu_ref[0]:.6f}, {enu_ref[1]:.6f}, {enu_ref[2]:.6f})")
        logger.info(f"   Point avec offset ENU: ({enu_point[0]:.6f}, {enu_point[1]:.6f}, {enu_point[2]:.6f})")
        logger.info(f"   Vecteur d√©placement ENU: ({enu_offset[0]:.6f}, {enu_offset[1]:.6f}, {enu_offset[2]:.6f})")
        
        residues_enu[name] = {
            'offset': enu_offset,
            'distance': residue_data['distance']
        }
        
        logger.info(f"R√©sidu {name} ENU: offset={enu_offset.tolist()}, distance={residue_data['distance']:.3f}m")
    
    logger.info(f"Conversion termin√©e : {len(residues_enu)} r√©sidus convertis en ENU")
    
    # √âTAPE 1.6 : Pr√©paration des donn√©es pour l'interpolation TPS
    logger.info("Pr√©paration des donn√©es pour l'interpolation TPS...")
    
    # Nous avons besoin des positions des GCPs dans les nuages pour l'interpolation
    # Pour l'instant, nous utilisons une approximation bas√©e sur les coordonn√©es nominales
    # TODO: Impl√©menter la d√©tection automatique des GCPs dans les nuages
    
    # Extraction des positions des GCPs depuis le fichier de coordonn√©es
    gcp_positions = {}
    try:
        with open(coord_file, 'r') as f:
            lines = f.readlines()
        
        # IMPORTANT : Relecture de l'offset du fichier de coordonn√©es pour les GCPs
        coord_offset = None
        try:
            for line in lines:
                if line.startswith('#Offset to add :'):
                    offset_text = line.replace('#Offset to add :', '').strip()
                    coord_offset = [float(x) for x in offset_text.split()]
                    logger.info(f"Offset du fichier de coordonn√©es pour GCPs : {coord_offset}")
                    break
        except Exception as e:
            logger.warning(f"Erreur lors de la lecture de l'offset pour GCPs : {e}")
            coord_offset = [0.0, 0.0, 0.0]
        
        for line in lines:
            line = line.strip()
            if line.startswith('#F=') or line.startswith('#'):
                continue
            
            parts = line.split()
            if len(parts) >= 4:
                try:
                    name = parts[0]
                    x, y, z = float(parts[1]), float(parts[2]), float(parts[3])
                    
                    # IMPORTANT : L'offset doit TOUJOURS √™tre appliqu√© pour obtenir les vraies coordonn√©es ITRF
                    # Le point global sert seulement de centre de transformation ENU
                    if coord_offset is not None:
                        x += coord_offset[0]
                        y += coord_offset[1]
                        z += coord_offset[2]
                        logger.info(f"GCP {name} : offset appliqu√© ({coord_offset[0]:.6f}, {coord_offset[1]:.6f}, {coord_offset[2]:.6f})")
                    else:
                        logger.warning(f"GCP {name} : pas d'offset disponible, coordonn√©es relatives utilis√©es")
                    
                    point_itrf = np.array([x, y, z])
                    
                    if force_global_ref and global_ref_point is not None:
                        logger.info(f"GCP {name} : transformation ENU avec le point global comme centre")
                    else:
                        logger.info(f"GCP {name} : transformation ENU avec le point local comme centre")
                    
                    # Conversion en ENU avec le M√äME transformer
                    enu_pos = transformer.transform(point_itrf[0], point_itrf[1], point_itrf[2])
                    gcp_positions[name] = np.array([enu_pos[0], enu_pos[1], enu_pos[2]])
                    
                    logger.info(f"GCP {name}: ITRF({point_itrf[0]:.6f}, {point_itrf[1]:.6f}, {point_itrf[2]:.6f}) ‚Üí ENU({enu_pos[0]:.6f}, {enu_pos[1]:.6f}, {enu_pos[2]:.6f})")
                    
                except ValueError:
                    continue
        
        logger.info(f"Positions des GCPs pr√©par√©es : {len(gcp_positions)} points")
        
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©paration des positions GCPs : {e}")
        raise RuntimeError(f"Erreur lors de la pr√©paration des positions GCPs : {e}")
    
    # √âTAPE 2 : Fonctions d'interpolation TPS
    def thin_plate_spline_interpolation(points, control_points, control_values):
        """
        Interpolation par Thin Plate Splines
        
        Args:
            points: Points √† interpoler (N, 3)
            control_points: Points de contr√¥le (M, 3)
            control_values: Valeurs aux points de contr√¥le (M, 3)
        
        Returns:
            Valeurs interpol√©es aux points (N, 3)
        """
        try:
            from scipy.spatial.distance import cdist
            from scipy.linalg import solve
            
            M = len(control_points)
            N = len(points)
            
            # Calcul des distances entre points de contr√¥le
            K = cdist(control_points, control_points, metric='euclidean')
            # Fonction de base radiale (RBF)
            K = K * np.log(K + 1e-10)  # √âviter log(0)
            
            # Construction du syst√®me lin√©aire
            # [K  P] [w] = [v]
            # [P' 0] [a]   [0]
            # o√π P = [1, x, y, z] pour chaque point de contr√¥le
            
            P = np.column_stack([np.ones(M), control_points])
            A = np.block([[K, P], [P.T, np.zeros((4, 4))]])
            
            # R√©solution pour chaque composante (x, y, z)
            interpolated_values = np.zeros((N, 3))
            
            for dim in range(3):
                b = np.concatenate([control_values[:, dim], np.zeros(4)])
                solution = solve(A, b)
                w = solution[:M]
                a = solution[M:]
                
                # Calcul des distances entre points d'interpolation et points de contr√¥le
                K_interp = cdist(points, control_points, metric='euclidean')
                K_interp = K_interp * np.log(K_interp + 1e-10)
                
                # Interpolation
                P_interp = np.column_stack([np.ones(N), points])
                interpolated_values[:, dim] = K_interp @ w + P_interp @ a
            
            return interpolated_values
            
        except ImportError:
            logger.warning("scipy non disponible, utilisation de l'interpolation lin√©aire")
            return linear_interpolation(points, control_points, control_values)
    
    def linear_interpolation(points, control_points, control_values):
        """
        Interpolation lin√©aire simple par distance inverse pond√©r√©e
        """
        from scipy.spatial.distance import cdist
        
        # Calcul des distances
        distances = cdist(points, control_points, metric='euclidean')
        
        # Pond√©ration par distance inverse
        weights = 1.0 / (distances + 1e-10)
        weights = weights / np.sum(weights, axis=1, keepdims=True)
        
        # Interpolation
        interpolated_values = weights @ control_values
        
        return interpolated_values
    
    # √âTAPE 3 : Traitement des nuages de points
    ply_files = []
    for root, dirs, files in os.walk(abs_input_dir):
        for file in files:
            if file.lower().endswith('.ply'):
                ply_files.append(os.path.join(root, file))
    
    logger.info(f"Trouv√© {len(ply_files)} fichiers .ply dans {abs_input_dir}")
    
    if len(ply_files) == 0:
        logger.warning(f"Aucun fichier .ply trouv√© dans {abs_input_dir}")
        logger.info("Aucun fichier √† traiter.")
        return
    
    # Configuration de la parall√©lisation
    if max_workers is None:
        max_workers = min(10, cpu_count(), len(ply_files))  # Maximum 10 processus par d√©faut
    else:
        # Limiter par CPU disponibles ET fichiers pour √©viter surcharge cluster
        max_workers = min(max_workers, cpu_count(), len(ply_files))
    logger.info(f"Traitement parall√®le avec {max_workers} processus...")
    
    # Pr√©paration des arguments pour le multiprocessing
    process_args = []
    for ply_file in ply_files:
        process_args.append((ply_file, output_dir, residues_enu, gcp_positions, deformation_type))
    
    # Traitement parall√®le
    total_files_processed = 0
    failed_files = []
    
    try:
        with Pool(processes=max_workers) as pool:
            # Lancement du traitement parall√®le
            results = pool.map(process_single_cloud_deform, process_args)
            
            # Analyse des r√©sultats
            for i, (success, message) in enumerate(results):
                if success:
                    logger.info(f"‚úÖ {message}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {message}")
                    failed_files.append(ply_files[i])
                    
    except Exception as e:
        logger.error(f"Erreur lors du traitement parall√®le : {e}")
        # Fallback vers le traitement s√©quentiel en cas d'erreur
        logger.info("Tentative de traitement s√©quentiel...")
        total_files_processed = 0
        for ply_file in ply_files:
            try:
                result = process_single_cloud_deform((ply_file, output_dir, residues_enu, gcp_positions, deformation_type))
                if result[0]:
                    logger.info(f"‚úÖ {result[1]}")
                    total_files_processed += 1
                else:
                    logger.error(f"‚ùå {result[1]}")
            except Exception as e:
                logger.error(f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}")
    
    # R√©sum√© final
    if failed_files:
        logger.warning(f"‚ö†Ô∏è {len(failed_files)} fichiers n'ont pas pu √™tre trait√©s")
    logger.info(f"D√©formation {deformation_type} termin√©e. {total_files_processed} fichiers trait√©s dans {output_dir}.")

def process_single_cloud_for_unified(args):
    """Fonction de traitement d'un seul nuage pour la cr√©ation d'orthoimage/MNT unifi√©s (pour multiprocessing)"""
    ply_file, global_min_coords, global_max_coords, resolution, grid_width, grid_height = args
    
    # Cr√©ation d'un logger pour ce processus
    logger = logging.getLogger(f"Unified_{os.getpid()}")
    logger.setLevel(logging.INFO)
    
    try:
        import open3d as o3d
        import numpy as np
        
        # Lecture du nuage
        cloud = o3d.io.read_point_cloud(ply_file)
        if not cloud.has_points():
            return False, f"Nuage vide dans {os.path.basename(ply_file)}"
        
        points = np.asarray(cloud.points)
        logger.info(f"  {len(points)} points charg√©s")
        
        # Cr√©ation des matrices pour ce nuage
        height_raster = np.full((grid_height, grid_width), np.nan)
        color_raster = np.zeros((grid_height, grid_width, 3), dtype=np.uint8)
        
        # Rasterisation des points
        points_processed = 0
        for i, point in enumerate(points):
            # Conversion des coordonn√©es en indices de grille globale
            x_idx = int((point[0] - global_min_coords[0]) / resolution)
            y_idx = int((point[1] - global_min_coords[1]) / resolution)
            
            # V√©rification des limites
            if 0 <= x_idx < grid_width and 0 <= y_idx < grid_height:
                points_processed += 1
                # Mise √† jour de la hauteur (prendre la plus haute si plusieurs points)
                if np.isnan(height_raster[y_idx, x_idx]) or point[2] > height_raster[y_idx, x_idx]:
                    height_raster[y_idx, x_idx] = point[2]
                
                # Mise √† jour de la couleur
                if cloud.has_colors():
                    colors = np.asarray(cloud.colors)
                    color = colors[i]
                    # Conversion de [0,1] vers [0,255]
                    color_255 = (color * 255).astype(np.uint8)
                    color_raster[y_idx, x_idx] = color_255
        
        logger.info(f"  Points trait√©s : {points_processed}/{len(points)}")
        
        return True, (height_raster, color_raster)
        
    except Exception as e:
        return False, f"Erreur lors du traitement de {os.path.basename(ply_file)} : {e}"

def individual_zone_equalization(zone_ortho_path, logger):
    """
    √âGALISATION GLOBALE : Interface principale pour l'√©galisation des zones
    Utilise la nouvelle strat√©gie d'√©galisation globale bas√©e sur les quantiles
    
    Args:
        zone_ortho_path: Chemin vers l'ortho de zone √† √©galiser
        logger: Logger pour les messages
    
    Returns:
        str: Chemin vers la zone √©galis√©e
    """
    # Cette fonction est maintenant une interface qui appelle l'√©galisation globale
    # Elle sera remplac√©e par l'appel direct dans le pipeline principal
    logger.warning("‚ö†Ô∏è Cette fonction est obsol√®te. Utilisez l'√©galisation globale directement.")
    return zone_ortho_path
